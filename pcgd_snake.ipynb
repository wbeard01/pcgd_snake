{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "9c9iiuEGkGKi",
        "outputId": "6cd540db-f1e0-41a5-d97e-3df3a3db141a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kc-ml2/marlenv.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8bdVlGgvzxW",
        "outputId": "d7d0d120-3463-4090-8e3a-c049ca6d7cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marlenv'...\n",
            "remote: Enumerating objects: 770, done.\u001b[K\n",
            "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 770 (delta 156), reused 183 (delta 100), pack-reused 485\u001b[K\n",
            "Receiving objects: 100% (770/770), 4.52 MiB | 15.69 MiB/s, done.\n",
            "Resolving deltas: 100% (433/433), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/marlenv/"
      ],
      "metadata": {
        "id": "1fYAHaQfs7en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bef0005-dc25-4a29-8ab6-5a1a01425b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/marlenv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kel1f9RSwJPK",
        "outputId": "8ad55d54-f4ff-4435-b78e-3615da88f932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/marlenv\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym==0.24.1\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym==0.24.1->marlenv===1.0.0test1) (3.11.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793152 sha256=502e3b1b24bf2dce5f7aff6d2ea52f5fac7b74e2bd0e5e50c174a47eae4ba91c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/e9/0b/5536e77ed2edbbf067ecff287ec039633d40daee4d8dac7716\n",
            "Successfully built gym\n",
            "Installing collected packages: gym, marlenv\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py develop for marlenv\n",
            "Successfully installed gym-0.24.1 marlenv-1.0.0test1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import marlenv\n",
        "import marlenv.envs\n",
        "from google.colab import files\n",
        "\n",
        "custom_reward_func = {\n",
        "    'fruit': 1.0,\n",
        "    'kill': 20.0,\n",
        "    'lose': -1.0,\n",
        "    'time': 0.0,\n",
        "    'win': 1.0\n",
        "}\n",
        "env = gym.make(\n",
        "    'Snake-v1',\n",
        "    # height=20,       # Height of the grid map\n",
        "    # width=20,        # Width of the grid map\n",
        "    # num_snakes=4,    # Number of snakes to spawn on grid\n",
        "    # snake_length=3,  # Initial length of the snake at spawn time\n",
        "    # vision_range=10,  # Vision range (both width height), map returned if None\n",
        "    # frame_stack=1,   # Number of observations to stack on return\n",
        "    # num_envs=2,\n",
        "    height=20,\n",
        "    width=20,\n",
        "    num_snakes=4,\n",
        "    num_fruits = 8,\n",
        "    snake_length=3,\n",
        "    vision_range=None,\n",
        "    frame_stack=1,\n",
        "    observer='snake',\n",
        "    # full_observation=False,\n",
        "    reward_func=custom_reward_func\n",
        ")\n",
        "\n",
        "# env = gym.make('Snake-v1', reward_func=custom_reward_func)\n"
      ],
      "metadata": {
        "id": "9Gcab50zva4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076e2c6d-a118-4498-ffb5-5142a32589e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()"
      ],
      "metadata": {
        "id": "pmJUrM2QrHWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43833312-3c9e-49fa-cc2f-5fdd5fe574e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. \u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbW9OYr5rm-d",
        "outputId": "9d6a85f9-e7bc-4395-e07f-8ba89d3de0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 20, 20, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(obs[0, :, :, 0]) #locations of the walls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Fm7efF5DpvE8",
        "outputId": "db8e5d88-2d31-4550-ace4-d6b2940a236e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed2d52e0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(obs[0, :, :, 1]) #locations of the fruits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0wVZM5uXuFso",
        "outputId": "e0b9b15f-661b-489d-ccdd-859faea295a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed2d573490>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(obs[0, :, :, 2]+obs[0, :, :, 3]+obs[0, :, :, 4]) #locations of oppenents?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0TbAU-fluNao",
        "outputId": "538e7984-7fa6-4afd-a2a3-7919fc78f2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed2d0b7070>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(obs[0, :, :, 5]+obs[0, :, :, 6]+obs[0, :, :, 7]) #locations of the snake itself？"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vo2wVBwLwSsZ",
        "outputId": "859650f7-35c9-4efb-f6b2-b46dff11bc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed2d094130>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class policy(nn.Module):\n",
        "    \"\"\"General policy model for calculating action policy from state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(policy, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Conv2d(8, 4, 2),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Conv2d(4, 4, 2),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Flatten(),\n",
        "                                   nn.Linear(1296, 64),\n",
        "                                   nn.ReLU(),\n",
        "                                  #  nn.Linear(256, 64),\n",
        "                                  #  nn.ReLU(),\n",
        "                                  #  nn.Linear(64, 64),\n",
        "                                  #  nn.ReLU(),\n",
        "                                   nn.Linear(64, 3),\n",
        "                                   nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        mu = self.actor(state)\n",
        "        return Categorical(mu)\n",
        "\n",
        "class critic(nn.Module):\n",
        "    \"\"\"Critic model for estimating value from state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(critic, self).__init__()\n",
        "\n",
        "        self.critic = nn.Sequential(nn.Conv2d(8, 8, 2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(8, 8, 2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Flatten(),\n",
        "                                    nn.Linear(2592, 256),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(256, 64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(64, 64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value"
      ],
      "metadata": {
        "id": "jRjOBpbHomPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import pdb\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import pdb\n",
        "\n",
        "# TODO(yyshi): Casework for this?\n",
        "DEFAULT_DTYPE = torch.float32\n",
        "torch.set_default_dtype(DEFAULT_DTYPE)\n",
        "\n",
        "def zero_grad(params):\n",
        "    \"\"\"Given some list of Tensors, zero and reset gradients.\"\"\"\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            p.grad.detach()\n",
        "            p.grad.zero_()\n",
        "\n",
        "def flatten_filter_none(grad_list, param_list,\n",
        "                        detach=False,\n",
        "                        neg=False,\n",
        "                        device=torch.device('cpu')):\n",
        "    \"\"\"\n",
        "    Given a list of Tensors with possible None values, returns single Tensor\n",
        "    with None removed and flattened.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for grad, param in zip(grad_list, param_list):\n",
        "        if grad is None:\n",
        "            filtered.append(torch.zeros(param.numel(), device=device, requires_grad=True))\n",
        "        else:\n",
        "            filtered.append(grad.contiguous().view(-1))\n",
        "\n",
        "    result = torch.cat(filtered) if not neg else -torch.cat(filtered)\n",
        "\n",
        "    # Use this only if higher order derivatives are not needed.\n",
        "    if detach:\n",
        "        result.detach()\n",
        "\n",
        "    return result\n",
        "\n",
        "# TODO(anonymous): make this user interface cleaner.\n",
        "class SGD(object):\n",
        "    \"\"\"Optimizer class for simultaneous SGD\"\"\"\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 device=torch.device('cpu')\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param lr_list: list of learning rates per player optimizer.\n",
        "        \"\"\"\n",
        "        # Store optimizer state.\n",
        "        player_list = [list(elem) for elem in player_list]\n",
        "        self.state = {'step': 0,\n",
        "                      'player_list': player_list,\n",
        "                      'lr_list': lr_list}\n",
        "        # TODO(anonymous): set this device in CMD algorithm.\n",
        "        self.device = device\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for player in self.state['player_list']:\n",
        "            zero_grad(player)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.state\n",
        "\n",
        "    def step(self, loss_list):\n",
        "        #print('loss function', loss_list)\n",
        "        grad_list = [\n",
        "            autograd.grad(loss, player, retain_graph=True, allow_unused=True)\n",
        "            for loss, player in zip(loss_list, self.state['player_list'])\n",
        "        ]\n",
        "\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[0]))\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[1]))\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[2]))\n",
        "\n",
        "\n",
        "\n",
        "        for grad, player, lr in zip(grad_list, self.state['player_list'], self.state['lr_list']):\n",
        "            for player_elem, grad_elem in zip(player, grad):\n",
        "                if grad_elem is not None:\n",
        "                    player_elem.data -= grad_elem * lr\n",
        "\n",
        "def critic_update(mat_states, mat_action_mask, returns, q, optim_q, batch_size, trj_len, unflatten_state, device):\n",
        "    counter = 0\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            mat_state_unflattened_t0 = unflatten_state(mat_states[j, t, :].reshape(1, -1))\n",
        "            mat_state_unflattened_t0 = mat_state_unflattened_t0.to(device)\n",
        "            if (j==0 and t == 0):\n",
        "                #mat_state_list = mat_state_unflattened_t0\n",
        "                value_pred = q(mat_state_unflattened_t0)\n",
        "            else:\n",
        "                if(mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                #mat_state_list = torch.cat((mat_state_list, mat_state_unflattened_t0), dim = 0)\n",
        "                value_pred_current = q(mat_state_unflattened_t0)\n",
        "                value_pred = torch.cat((value_pred, value_pred_current), dim=0)\n",
        "\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            return_current = returns[j, t].reshape(1, -1)\n",
        "            return_current = return_current.to(device)\n",
        "            if (j == 0 and t == 0):\n",
        "                return_list = return_current\n",
        "            else:\n",
        "                if (mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                return_list = torch.cat((return_list, return_current), dim=0)\n",
        "\n",
        "    #value_pred = q(c)\n",
        "    critic_loss = (return_list - value_pred).pow(2).mean()\n",
        "    optim_q.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    optim_q.step()\n",
        "    critic_loss = critic_loss.detach().cpu()\n",
        "\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    return critic_loss\n",
        "\n",
        "def get_advantage(\n",
        "        next_value, reward_mat, value_mat, mat_done,\n",
        "        gamma=0.99, tau=0.95, device=torch.device('cpu')\n",
        "):\n",
        "    insert_tensor = torch.tensor([[float(next_value)]], device=device)\n",
        "    value_mat = torch.cat([value_mat, insert_tensor])\n",
        "    gae = 0\n",
        "    returns = []\n",
        "\n",
        "    for i in reversed(range(len(reward_mat))):\n",
        "        delta = reward_mat[i] + gamma * value_mat[i + 1] * mat_done[i] - value_mat[i]\n",
        "        gae = delta + gamma * tau * mat_done[i] * gae\n",
        "        returns.append(gae + value_mat[i])\n",
        "\n",
        "    # Reverse ordering.\n",
        "    returns.reverse()\n",
        "\n",
        "    vals = torch.cat(returns).reshape(-1, 1)\n",
        "    return vals\n",
        "\n",
        "# def critic_update(state_mat, return_mat, q, optim_q):\n",
        "#     val_loc = q(state_mat)\n",
        "\n",
        "#     critic_loss = (return_mat - val_loc).pow(2).mean()\n",
        "\n",
        "#     optim_q.zero_grad()\n",
        "#     critic_loss.backward()\n",
        "#     optim_q.step()\n",
        "\n",
        "#     critic_loss = critic_loss.detach().cpu()\n",
        "\n",
        "#     return critic_loss\n",
        "\n",
        "# TODO(anonymous): Revisit this?\n",
        "# def get_advantage(\n",
        "#         next_value, reward_mat, value_mat, masks,\n",
        "#         gamma=0.99, tau=0.95, device=torch.device('cpu')\n",
        "# ):\n",
        "#     insert_tensor = torch.tensor([[float(next_value)]], device=device)\n",
        "#     value_mat = torch.cat([value_mat, insert_tensor])\n",
        "#     gae = 0\n",
        "#     returns = []\n",
        "\n",
        "#     for i in reversed(range(len(reward_mat))):\n",
        "#         delta = reward_mat[i] + gamma * value_mat[i + 1] * masks[i] - value_mat[i]\n",
        "#         gae = delta + gamma * tau * masks[i] * gae\n",
        "#         returns.append(gae + value_mat[i])\n",
        "\n",
        "#     # Reverse ordering.\n",
        "#     returns.reverse()\n",
        "\n",
        "#     vals = torch.cat(returns).reshape(-1, 1)\n",
        "#     return vals"
      ],
      "metadata": {
        "id": "KkBlB1lWpDzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingWrapper:\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-3,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            self_play=False,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "            critic_optim_kwargs={},\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store game environment.\n",
        "        self.env = env\n",
        "        # Training parameters.\n",
        "        self.policies = policies\n",
        "        self.critics = critics\n",
        "\n",
        "        # Sampling parameters.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Device to be used.\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # TODO(anonymous): Implement self play in a cleaner way.\n",
        "        self.self_play = self_play\n",
        "        if self.self_play:\n",
        "            assert (len(self.critics) == 1)\n",
        "            self.critic_optim = [\n",
        "                torch.optim.Adam(self.critics[0].parameters(), lr=critic_lr)\n",
        "            ]\n",
        "        else:\n",
        "            self.critic_optim = [\n",
        "                torch.optim.Adam(c.parameters(), lr=critic_lr, **critic_optim_kwargs) for c in self.critics\n",
        "            ]\n",
        "\n",
        "            # GAE estimation work.\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "    ## start from here on 2023/01/05 - change the data storage type to tensor\n",
        "    def sample(self, verbose=True):\n",
        "        \"\"\"\n",
        "        :param verbose: Print debugging information if requested.\n",
        "        Sample observations actions and states. Returns trajectory observations\n",
        "        actions rewards in single list format (which seperates trajectories\n",
        "        using done mask).\n",
        "        \"\"\"\n",
        "        # We collect trajectories all into one list (using a done mask) for simplicity.\n",
        "        num_agents = len(self.policies)\n",
        "        max_traj_length = 1000\n",
        "        mat_all_states_t = []\n",
        "        mat_all_actions_t = []\n",
        "        mat_all_action_mask_t = []\n",
        "        mat_all_rewards_t = []\n",
        "        mat_all_done_t = []\n",
        "\n",
        "        # If verbose, show how long sampling takes.\n",
        "        if verbose:\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "        for j in range(self.batch_size):\n",
        "            # Reset environment for each trajectory in batch.\n",
        "            mat_states_t = []\n",
        "            mat_actions_t = []\n",
        "            mat_action_mask_t = []\n",
        "            mat_rewards_t = []\n",
        "            mat_done_t = []\n",
        "\n",
        "            obs = self.env.reset()\n",
        "            dones = [False for _ in range(num_agents)]\n",
        "\n",
        "            for t in range(max_traj_length):\n",
        "                # Record state...\n",
        "                mat_states_t.append(obs)\n",
        "                # action_mask = 1 if actively taking action; otherwise, action_mask = 0\n",
        "                mat_action_mask_t.append([1. - int(elem) for elem in dones])\n",
        "\n",
        "                # Since env is usually on CPU, send observation to GPU,\n",
        "                # sample, then collect back to CPU.\n",
        "                actions = []\n",
        "                for i in range(num_agents):\n",
        "                    policy = self.policies[i]\n",
        "                    obs_gpu = torch.tensor([obs[i]], device=self.device, dtype=self.dtype).permute(0,3,1,2)\n",
        "                    dist = policy(obs_gpu)\n",
        "\n",
        "                    action = dist.sample().cpu().numpy()\n",
        "\n",
        "                    # TODO(anonymous): Pytorch doesn't handle 0-dim tensors (a.k.a scalars well)\n",
        "                    if action.ndim == 1 and action.size == 1:\n",
        "                        action = action[0]\n",
        "                    else:\n",
        "                        # action = np.squeeze(dist.sample().cpu().numpy(), axis=1)\n",
        "                        action = np.squeeze(action, axis=1)\n",
        "\n",
        "                    actions.append(action)\n",
        "\n",
        "                # Advance environment one step forwards.\n",
        "                obs, rewards, dones, _ = self.env.step(actions)\n",
        "\n",
        "                # Record actions, rewards, and inverse done mask.\n",
        "                mat_actions_t.append(actions)\n",
        "\n",
        "                # 2023/01/05 YYSHI TODO(Comment out the random probablity for breaking out)\n",
        "                # if(t > 10):\n",
        "                #     p = torch.rand(1)\n",
        "                #     if(p>0.96):\n",
        "                #         # find the alive snake\n",
        "                #         index = [i for i, x in enumerate(dones) if not (x)]\n",
        "                #         for j in index:\n",
        "                #             rewards[j] += t*0.3\n",
        "                #             #print('Snake', j, 'Alive, when game finished!!', dones[j], p)\n",
        "                #             dones[j] = True\n",
        "\n",
        "                mat_rewards_t.append(rewards)\n",
        "                mat_done_t.append([1. - int(elem) for elem in dones])\n",
        "\n",
        "                # Break once all players are done.\n",
        "                if all(dones):\n",
        "                    break\n",
        "\n",
        "                # elif sum(dones) == 3: #only 1 snake alive\n",
        "                #     # find the alive snake\n",
        "                #     index = [i for i, x in enumerate(dones) if not (x)]\n",
        "                #     rewards[index] += 20\n",
        "                #     dones[index] = True\n",
        "                #     print('Snake', index, 'Alive!!')\n",
        "                #     break\n",
        "                # else:\n",
        "                #     pass\n",
        "\n",
        "            mat_all_states_t.append(mat_states_t)\n",
        "            mat_all_actions_t.append(mat_actions_t)\n",
        "            mat_all_action_mask_t.append(mat_action_mask_t)\n",
        "            mat_all_rewards_t.append(mat_rewards_t)\n",
        "            mat_all_done_t.append(mat_done_t)\n",
        "\n",
        "        # Create data on GPU for later update step\n",
        "        # TODO(yyshi): list agent, each element of the list contains # of trajectories,\n",
        "        # TODO(yyshi): and each trajectories contain # of samples\n",
        "        # (agent, num_traj, traj_length)\n",
        "        # have trajectory with splits same to the player with the longest trajectory.\n",
        "        trj_len = 0\n",
        "        for j in range(self.batch_size):\n",
        "            trj_len = np.maximum(trj_len, len(mat_all_states_t[j]))\n",
        "\n",
        "        state_dim = int(obs.size/obs.shape[0])\n",
        "\n",
        "        state_shape = obs_gpu.shape[1:]\n",
        "        mat_states = torch.zeros((num_agents, self.batch_size, trj_len, state_dim))\n",
        "        mat_actions = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_action_mask = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_rewards = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_done = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "\n",
        "        for j in range(self.batch_size):\n",
        "            current_trj_len = len(mat_all_states_t[j])\n",
        "            for i in range(current_trj_len):\n",
        "                for k in range(num_agents):\n",
        "                    mat_states[k, j, i] = torch.flatten(torch.tensor(mat_all_states_t[j][i][k], dtype=self.dtype, device=self.device).permute(2,0,1))\n",
        "                    mat_actions[k, j, i] = torch.tensor(mat_all_actions_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    mat_rewards[k, j, i] = torch.tensor(mat_all_rewards_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    # if trajectory is not active, mask = 0;\n",
        "                    mat_action_mask[k, j, i] = torch.tensor(mat_all_action_mask_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    mat_done[k, j, i] = torch.tensor(mat_all_done_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "\n",
        "        unflatten_state = torch.nn.Unflatten(1, state_shape)\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            print('sample took:', time.time() - batch_start_time)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # for t in range(trj_len):\n",
        "        #     state_unflatten = unflatten(mat_states[0, 0, t, :].reshape(1, -1))\n",
        "        #     plt.imshow(state_unflatten[0, 0, :, :] + 2*state_unflatten[0, 1, :, :] + 3*state_unflatten[0, 2, :, :] + 4*state_unflatten[0, 3, :, :])\n",
        "        #     plt.savefig(\"Instant{}.png\".format(t))\n",
        "\n",
        "        # mat_states, [num_agent, num_traj, traj_len, obs_dim] observations of all agents\n",
        "        # mat_actions, [num_agent, num_traj, traj_len] actions of all agents\n",
        "        # mat_action_mask, [num_agent, num_traj, traj_len] mat_action_mask[i, j, t] = 0 if agent i in trajectory j has died at time t-1 or earlier\n",
        "        # mat_rewards, [num_agent, num_traj, traj_len], rewards of all agents\n",
        "        # mat_done, [num_agent, num_traj, traj_len], mat_done[i, j, t] = 0 if agent i in trajectory j has died at time t or earlier\n",
        "        return mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state"
      ],
      "metadata": {
        "id": "DHymE_QL3NgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(yyshi): Refine this class structure based on feedback.\n",
        "class MultiSimGD(TrainingWrapper):\n",
        "    # TODO(yyshi): Horizon, gamma tuning?\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-3,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            self_play=False,\n",
        "            policy_lr=0.002,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "            critc_optim_kwargs={}\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "        super(MultiSimGD, self).__init__(\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=batch_size,\n",
        "            critic_lr=critic_lr,\n",
        "            tol=tol,\n",
        "            device=device,\n",
        "            dtype=dtype,\n",
        "            self_play=self_play,\n",
        "            gamma=gamma,\n",
        "            tau=tau,\n",
        "            critic_optim_kwargs=critc_optim_kwargs\n",
        "        )\n",
        "\n",
        "        # Optimizers for policies and critics.\n",
        "        self.policy_optim = SGD(\n",
        "            [p.parameters() for p in self.policies],\n",
        "            [policy_lr for _ in self.policies],\n",
        "            device=device\n",
        "        )\n",
        "    \n",
        "    def step(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute update step for policies and critics.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            step_start_time = time.time()\n",
        "\n",
        "        # TODO(anonymous): Fix this when making self-play more robust.\n",
        "        critics = self.critics\n",
        "        if self.self_play:\n",
        "            critics = [self.critics[0] for _ in range(len(self.policies))]\n",
        "\n",
        "        # Use critic function to get advantage.\n",
        "        # values, returns, advantages = [], [], []\n",
        "\n",
        "        # Compute generalized advantage estimation (GAE).\n",
        "        num_agent = len(critics)\n",
        "        batch_size = mat_states.shape[1]\n",
        "        trj_len = mat_states.shape[2]\n",
        "\n",
        "        values = torch.zeros((num_agent, batch_size, trj_len))\n",
        "        advantages = torch.zeros((num_agent, batch_size, trj_len))\n",
        "        returns = torch.zeros((num_agent, batch_size, trj_len))\n",
        "\n",
        "        for i, q in enumerate(critics):\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                ## compute value & advantage tensor\n",
        "                for t in range(trj_len):\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        values[i, j, t] = q(mat_state_unflattened_t0).detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * values[i, j, t + 1] - values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*values[i, j, t+1] - values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] - values[i, j, t])*mat_done[i, j, t]\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*returns[i, j, tt+1]\n",
        "\n",
        "        # Use sampled values to fit critic model.\n",
        "        if self.self_play:\n",
        "            cat_states = torch.cat([mat_states[i] for i in range(len(mat_states))])\n",
        "            cat_states_mask = torch.cat([mat_action_mask[i] for i in range(len(mat_action_mask))])\n",
        "            cat_returns = torch.cat([returns[i] for i in range(len(returns))])\n",
        "\n",
        "            #critic_update(cat_states, cat_returns, self.critics[0], self.critic_optim[0])\n",
        "            critic_update(cat_states, cat_states_mask, cat_returns, self.critics[0],\n",
        "                          self.critic_optim[0], batch_size, trj_len, unflatten_state, self.device)\n",
        "        else:\n",
        "            for i, q in enumerate(self.critics):\n",
        "                critic_update(mat_states[i], mat_action_mask[i], returns[i], q,\n",
        "                              self.critic_optim[i], batch_size, trj_len, unflatten_state, self.device)\n",
        "\n",
        "        # Include last index to compute pairs.\n",
        "        # traj_indices.append(mat_done[0].size(0))\n",
        "\n",
        "        log_probs = []\n",
        "        gradient_losses = []\n",
        "        for i, p in enumerate(self.policies):\n",
        "            # Our training wrapper assumes that the policy returns a distribution.\n",
        "            # desired: -1, state_dim (4*20*20)\n",
        "            # mat_states: number_trj, trj_len, state_dim\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    ## potential code optimization: break the for-loop when done\n",
        "                    mat_state_cur_unflattened = unflatten_state(mat_states[i, j, t].reshape(1, -1))\n",
        "                    mat_state_cur_unflattened = mat_state_cur_unflattened.to(self.device)\n",
        "                    log_pi = p(mat_state_cur_unflattened).log_prob(mat_actions[i, j, t].to(self.device))\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    if(j==0 and t==0):\n",
        "                        policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            gradient_losses.append(-policyloss/batch_size)\n",
        "\n",
        "\n",
        "        # Update the policy parameters.\n",
        "        self.policy_optim.zero_grad()\n",
        "        self.policy_optim.step(gradient_losses)\n",
        "        gradient_losses.clear()\n",
        "\n",
        "\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            print('MultiSimGD step took:', time.time() - step_start_time)"
      ],
      "metadata": {
        "id": "ffAzsb2b14Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Dx(x_dual, alpha = 1):\n",
        "        return x_dual / alpha\n",
        "\n",
        "def Dx_inv(x_dual, alpha = 1):\n",
        "    return x_dual * alpha\n",
        "\n",
        "def Dxx_vp(x_primal, vec, alpha = 1):\n",
        "    # Does not need to be in-place.\n",
        "    return vec * alpha\n",
        "\n",
        "def Dxx_inv_vp(x_primal, vec, alpha = 1):\n",
        "    return vec / alpha\n",
        "\n",
        "def antivp(\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened,\n",
        "        transpose=False,\n",
        "        device=torch.device('cpu'),\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param hessian_loss_list: list of objective functions for hessian computation\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: list of flattened vectors for each player\n",
        "    :param bregman: dictionary representing bregman potential to use\n",
        "    :param transpose: compute product against transpose if set\n",
        "\n",
        "    Computes right product of antisymmetric metamatrix with a vector of player vectors.\n",
        "    \"\"\"\n",
        "    # TODO(yyshi): add error handling and assertions\n",
        "    # assert(len(hessian_loss_list) == len(player_list))\n",
        "    # assert(len(hessian_loss_list) == len(vector_list))\n",
        "    prod_list = [torch.zeros_like(v, device=device) for v in vector_list_flattened]\n",
        "\n",
        "    for i, row_params in enumerate(player_list):\n",
        "        for j, (col_params, vector_elem) in enumerate(zip(player_list, vector_list_flattened)):\n",
        "            if i == j:\n",
        "                # Diagonal element is the Bregman term.\n",
        "                prod_list[i] += Dxx_vp(player_list_flattened[i], vector_elem)\n",
        "                continue\n",
        "\n",
        "            # Otherwise, we construct our Hessian vector products. Variable\n",
        "            # retain_graph must be set to true, or we cant compute multiple\n",
        "            # subsequent Hessians any more.\n",
        "            left_loss, right_loss = hessian_loss_list[i], hessian_loss_list[j]\n",
        "            \n",
        "            if transpose:\n",
        "                left_loss, right_loss = right_loss, left_loss\n",
        "\n",
        "            # Anti-symmetric decomposition (1/2)(A - A^T)...\n",
        "            left_grad_raw = autograd.grad(left_loss, col_params,\n",
        "                                          create_graph=True,\n",
        "                                          retain_graph=True,\n",
        "                                          allow_unused=True)\n",
        "            left_grad_flattened = flatten_filter_none(left_grad_raw, col_params,\n",
        "                                                      device=device)\n",
        "\n",
        "            left_hvp_raw = autograd.grad(left_grad_flattened, row_params,\n",
        "                                         grad_outputs=vector_elem,\n",
        "                                         create_graph=False,\n",
        "                                         retain_graph=True,\n",
        "                                         allow_unused=True)\n",
        "            left_hvp_flattened = flatten_filter_none(left_hvp_raw, row_params,\n",
        "                                                     device=device)\n",
        "\n",
        "            right_grad_raw = autograd.grad(right_loss, col_params,\n",
        "                                           create_graph=True,\n",
        "                                           retain_graph=True,\n",
        "                                           allow_unused=True)\n",
        "            right_grad_flattened = flatten_filter_none(right_grad_raw, col_params,\n",
        "                                                       device=device)\n",
        "\n",
        "            right_hvp_raw = autograd.grad(right_grad_flattened, row_params,\n",
        "                                          grad_outputs=vector_elem,\n",
        "                                          create_graph=False,\n",
        "                                          retain_graph=True,\n",
        "                                          allow_unused=True)\n",
        "            right_hvp_flattened = flatten_filter_none(right_hvp_raw, row_params,\n",
        "                                                      device=device)\n",
        "\n",
        "            prod_list[i] += 0.5 * (left_hvp_flattened - right_hvp_flattened)\n",
        "\n",
        "    # Detach to get memory back.\n",
        "    prod_list = [elem.detach() for elem in prod_list]\n",
        "\n",
        "    return prod_list\n",
        "\n",
        "def avp(\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened,\n",
        "        transpose=False,\n",
        "        device=torch.device('cpu'),\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param hessian_loss_list: list of objective functions for hessian computation\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: list of flattened vectors for each player\n",
        "    :param bregman: dictionary representing bregman potential to use\n",
        "    :param transpose: compute product against transpose if set\n",
        "\n",
        "    Computes right product of metamatrix with a vector of player vectors.\n",
        "    \"\"\"\n",
        "    # TODO(anonymous): add error handling and assertions\n",
        "    # assert(len(hessian_loss_list) == len(player_list))\n",
        "    # assert(len(hessian_loss_list) == len(vector_list))\n",
        "    # TODO(yyshi): add eta to the function input\n",
        "    \n",
        "    eta = 0.8\n",
        "    \n",
        "    prod_list = [torch.zeros_like(v, device=device) for v in vector_list_flattened]\n",
        "\n",
        "    for i, row_params in enumerate(player_list):\n",
        "        for j, (col_params, vector_elem) in enumerate(zip(player_list, vector_list_flattened)):\n",
        "            if i == j:\n",
        "                # Diagonal element is the Bregman term, i.e., I * vector_elem\n",
        "                prod_list[i] += Dxx_vp(player_list_flattened[i], vector_elem, alpha = 1)\n",
        "                continue\n",
        "\n",
        "            # Otherwise, we construct our Hessian vector products. Variable\n",
        "            # retain_graph must be set to true, or we cant compute multiple\n",
        "            # subsequent Hessians any more.\n",
        "\n",
        "            loss = hessian_loss_list[i, j] if not transpose else hessian_loss_list[j, i]\n",
        "\n",
        "            grad_raw = autograd.grad(loss, col_params,\n",
        "                                     create_graph=True,\n",
        "                                     retain_graph=True,\n",
        "                                     allow_unused=True)\n",
        "\n",
        "            grad_flattened = flatten_filter_none(grad_raw, col_params,\n",
        "                                                 device=device)\n",
        "\n",
        "            # Don't need any higher order derivatives, so create_graph = False.\n",
        "            hvp_raw = autograd.grad(grad_flattened, row_params,\n",
        "                                    grad_outputs=vector_elem,\n",
        "                                    create_graph=False,\n",
        "                                    retain_graph=True,\n",
        "                                    allow_unused=True)\n",
        "            \n",
        "            # hvp_flattened = hvp_raw[0]\n",
        "            hvp_flattened = flatten_filter_none(hvp_raw, row_params,\n",
        "                                                device=device)\n",
        "\n",
        "            prod_list[i] += eta*hvp_flattened\n",
        "\n",
        "    # Detach to get memory back.\n",
        "    prod_list = [elem.detach() for elem in prod_list]\n",
        "    \n",
        "\n",
        "    return prod_list\n",
        "\n",
        "def metamatrix_conjugate_gradient(\n",
        "        grad_loss_list,\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened=None,\n",
        "        mvp=avp,\n",
        "        n_steps=20,\n",
        "        tol=1e-3,\n",
        "        atol=1e-3,\n",
        "        device=torch.device('cpu')\n",
        "):\n",
        "    \"\"\"\n",
        "    :param grad_loss_list: list of loss tensors for each player to compute gradient\n",
        "    :param hessian_loss_list: list of loss tensors for each player to compute hessian\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: initial guess for update solution\n",
        "    :param bregman: dict representing a Bregman potential to be used\n",
        "    :param n_steps: number of iteration steps for conjugate gradient\n",
        "    :param tol: relative residual tolerance threshold from initial vector guess\n",
        "    :param atol: absolute residual tolerance threshold\n",
        "\n",
        "    Compute solution to meta-matrix game form using preconditioned conjugate\n",
        "    gradient method. Since the metamatrix A is not p.s.d, we multiply both sides\n",
        "    by the transpose to ensure p.s.d.\n",
        "\n",
        "    In other words, note that solving Ax = b (where A is meta matrix, x is\n",
        "    vector of update vectors and b is learning rate times vector of gradients\n",
        "    is the same as solving A'x = b' (where A' = (A^T)A and b' = (A^T)b.\n",
        "    \"\"\"\n",
        "\n",
        "    b = []\n",
        "    for loss, param_tensors in zip(grad_loss_list, player_list):\n",
        "        # Get vector list of negative gradients.\n",
        "        grad_raw = autograd.grad(loss, param_tensors,\n",
        "                                 retain_graph=True,\n",
        "                                 allow_unused=True)\n",
        "        \n",
        "        grad_flattened = flatten_filter_none(grad_raw, param_tensors,\n",
        "                                             neg=True, detach=True, device=device)\n",
        "        \n",
        "        b.append(grad_flattened.detach())\n",
        "        # b.append(grad_raw[0].detach())\n",
        "\n",
        "    # Multiplying both sides by transpose to ensure p.s.d.\n",
        "    # r = A^t * b (before we subtract)\n",
        "    r = mvp(hessian_loss_list, player_list, player_list_flattened, b, \n",
        "            transpose=True, device=device)\n",
        "\n",
        "    # Set relative residual threshold based on norm of b.\n",
        "    # norm_At_b = sum(r_elem*r_elem for r_elem in r)\n",
        "    \n",
        "    \n",
        "    norm_At_b = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "    residual_tol = tol * norm_At_b\n",
        "    print('residual_tolerance', residual_tol)\n",
        "\n",
        "    # If no guess provided, start from zero vector.\n",
        "    if vector_list_flattened is None:\n",
        "        vector_list_flattened = [torch.zeros_like(p, device=device)\n",
        "                                 for p in player_list_flattened]\n",
        "    else:\n",
        "        # Compute initial residual if a guess is given.\n",
        "        A_x = mvp(hessian_loss_list, player_list, player_list_flattened, vector_list_flattened,\n",
        "                  transpose=False, device=device)\n",
        "        At_A_x = mvp(hessian_loss_list, player_list, player_list_flattened, A_x,\n",
        "                     transpose=True, device=device)\n",
        "\n",
        "        # torch._foreach_sub_(r, At_A_x)\n",
        "        for r_elem, At_A_x_elem in zip(r, At_A_x):\n",
        "            r_elem -= At_A_x_elem\n",
        "\n",
        "    ## up until here, we have computed r_0 = A^tb - A^t A x0\n",
        "    ## Use preconditioner if available...\n",
        "    # z = r\n",
        "    # if 'Dxx_inv_vp' in bregman:\n",
        "    #     z = [bregman['Dxx_inv_vp'](params, r_elems)\n",
        "    #          for params, r_elems in zip(player_list_flattened, r)]\n",
        "\n",
        "    # Early exit if solution already found.\n",
        "    # rdotr = sum(r_elem*r_elem for r_elem in r)\n",
        "    rdotr = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "\n",
        "    # rdotz = sum(torch.dot(r_elem, z_elem) for r_elem, z_elem in zip(r, z))\n",
        "    if rdotr < residual_tol or rdotr < atol:\n",
        "        return vector_list_flattened, 0, rdotr\n",
        "\n",
        "    # Define p and measure current candidate vector.\n",
        "    p = [r_elem.clone().detach() for r_elem in r]\n",
        "\n",
        "    # Use conjugate gradient to find vector solution.\n",
        "    for i in range(n_steps):\n",
        "        print('conjugate number of step,', i, 'rdotr', rdotr)\n",
        "        step_3 = time.time()\n",
        "        A_p = mvp(hessian_loss_list, player_list, player_list_flattened, p,\n",
        "                  transpose=False, device=device)\n",
        "        At_A_p = mvp(hessian_loss_list, player_list, player_list_flattened, A_p,\n",
        "                     transpose=True, device=device)\n",
        "        #print('One conjugate gradient step took:', time.time() - step_3)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            alpha = torch.div(rdotr, sum(torch.dot(e1, e2) for e1, e2 in zip(p, At_A_p)))\n",
        "            #alpha = torch.div(rdotr, sum(e1*e2 for e1, e2 in zip(p, At_A_p)))\n",
        "\n",
        "            # Update candidate solution and residual, where:\n",
        "            # (1) x_new = x + alpha * p\n",
        "            # (2) r_new = r - alpha * A^t A * p\n",
        "\n",
        "            # torch._foreach_add_(vector_list_flattened, p, alpha=alpha)\n",
        "            # torch._foreach_sub_(r, At_A_p, alpha=alpha)\n",
        "\n",
        "            for vlf_elem, p_elem in zip(vector_list_flattened, p):\n",
        "                vlf_elem += p_elem * alpha\n",
        "            for r_elem, At_A_P_elem in zip(r, At_A_p):\n",
        "                r_elem -= At_A_P_elem * alpha\n",
        "\n",
        "            # Calculate new residual metric\n",
        "            #new_rdotr = sum(r_elem*r_elem for r_elem in r)\n",
        "            new_rdotr = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "\n",
        "            # Break if solution is within threshold\n",
        "            if new_rdotr < atol or new_rdotr < residual_tol:\n",
        "                break\n",
        "\n",
        "            # If preconditioner provided, use it...\n",
        "            # z = r\n",
        "            # if 'Dxx_inv_vp' in bregman:\n",
        "            #     z = [bregman['Dxx_inv_vp'](params, r_elems)\n",
        "            #          for params, r_elems in zip(player_list_flattened, r)]\n",
        "            # new_rdotz = sum(torch.dot(r_elem, z_elem) for r_elem, z_elem in zip(r, z))\n",
        "\n",
        "            # Otherwise, update and continue.\n",
        "            # (3) p_new = r_new + beta * p\n",
        "\n",
        "            # torch._foreach_add(z, p, alpha=beta)\n",
        "            beta = torch.div(new_rdotr, rdotr)\n",
        "            p = [r_elem + p_elem * beta for r_elem, p_elem in zip(r, p)]\n",
        "\n",
        "            rdotr = new_rdotr\n",
        "            # rdotz = new_rdotz\n",
        "\n",
        "    return vector_list_flattened, i + 1, rdotr\n"
      ],
      "metadata": {
        "id": "MzYSB21T56sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CMD(object):\n",
        "    \"\"\"Optimizer class for the CMD algorithm with differentiable player objectives.\"\"\"\n",
        "\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 antisymetric=False,\n",
        "                 tol=None,\n",
        "                 atol=None,\n",
        "                 n_steps=None,\n",
        "                 device=torch.device('cpu')\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param bregman: dict representing Bregman potential to be used\n",
        "        \"\"\"\n",
        "\n",
        "        # In case, parameter generators are provided.\n",
        "        player_list = [list(elem) for elem in player_list]\n",
        "\n",
        "        # Conjugate gradient will provably converge in number of params steps.\n",
        "        if n_steps is None:\n",
        "            n_steps = sum([sum([elem.numel() for elem in param_list])\n",
        "                           for param_list in player_list])\n",
        "\n",
        "        # Store optimizer state.\n",
        "        self.state = {'step': 0,\n",
        "                      'player_list': player_list,\n",
        "                      'lr_list': lr_list,\n",
        "                      'tol': tol,\n",
        "                      'atol': atol,\n",
        "                      'n_steps': n_steps,\n",
        "                      'last_dual_soln': None,\n",
        "                      'last_dual_soln_n_iter': 0,\n",
        "                      'last_dual_residual': 0.,\n",
        "                      'antisymetric': antisymetric}\n",
        "        # TODO(yyshi): set this device in CMD algorithm.\n",
        "        self.device = device\n",
        "        self.mvp = avp if not antisymetric else antivp\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for player in self.state['player_list']:\n",
        "            zero_grad(player)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.state\n",
        "\n",
        "    def player_list(self):\n",
        "        return self.state['player_list']\n",
        "\n",
        "    def step(self, loss_list):\n",
        "        # Compute flattened player list for some small optimization.\n",
        "        player_list = self.state['player_list']\n",
        "        player_list_flattened = [flatten_filter_none(player, player,\n",
        "                                                     detach=True, device=self.device)\n",
        "                                 for player in player_list]\n",
        "\n",
        "        # Compute dual solution first, before mapping back to primal.\n",
        "        # Use dual solution as initial guess for numerical speed.\n",
        "        nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n",
        "            loss_list,\n",
        "            loss_list,\n",
        "            player_list,\n",
        "            player_list_flattened,\n",
        "            vector_list_flattened=self.state['last_dual_soln'],\n",
        "            tol=self.state['tol'],\n",
        "            atol=self.state['atol'],\n",
        "            n_steps=self.state['n_steps'],\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Store state for use in next nash computation..\n",
        "        self.state['step'] += 1\n",
        "        self.state['last_dual_soln'] = nash_list_flattened\n",
        "        self.state['last_dual_soln_n_iter'] = n_iter\n",
        "        self.state['last_dual_residual'] = res\n",
        "\n",
        "        # Map dual solution back into primal space.\n",
        "        # mapped_list_flattened = exp_map(player_list_flattened,\n",
        "        #                                 nash_list_flattened,\n",
        "        #                                 bregman=self.bregman)\n",
        "\n",
        "        # Update parameters in place to update players as optimizer.\n",
        "        for player, mapped_flattened in zip(self.state['player_list'], mapped_list_flattened):\n",
        "            idx = 0\n",
        "            for p in player:\n",
        "                p.data = mapped_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                idx += p.numel()\n",
        "\n",
        "\n",
        "# TODO(anonymous): May need to fix this optimizer for self-play, specifically update\n",
        "# method, since we need to update data and not just replace it. If this were\n",
        "# self play, only one of the updates calculated from nash would carry through.\n",
        "class CMD_RL(CMD):\n",
        "    \"\"\"RL optimizer using CMD algorithm, using derivation from CoPG paper.\"\"\"\n",
        "\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 antisymetric=False,\n",
        "                 tol=None,\n",
        "                 atol=None,\n",
        "                 n_steps=None,\n",
        "                 device=torch.device('cpu')\n",
        "                 ):\n",
        "        ## TODO(yyshi): change this hyperparameter to program input\n",
        "        # self.policy_lr = 0.001\n",
        "        self.used_ = \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param bregman: dict representing Bregman potential to be used\n",
        "        \"\"\"\n",
        "        super(CMD_RL, self).__init__(player_list,\n",
        "                                     lr_list,\n",
        "                                     antisymetric=antisymetric,\n",
        "                                     tol=tol, atol=atol,\n",
        "                                     n_steps=n_steps,\n",
        "                                     device=device)\n",
        "\n",
        "    def step(self, grad_loss_list, hessian_loss_list, cgd=False):\n",
        "        \"\"\"\n",
        "        CMD algorithm using derivation for gradient and hessian term from CoPG.\n",
        "        \"\"\"\n",
        "        # Compute flattened player list for some small optimization.\n",
        "        player_list = self.state['player_list']\n",
        "        player_list_flattened = [flatten_filter_none(player, player, detach=True, device=self.device)\n",
        "                                 for player in player_list]\n",
        "\n",
        "\n",
        "        \"\"\" Test Method I\"\"\"\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n",
        "            grad_loss_list,\n",
        "            hessian_loss_list,\n",
        "            player_list,\n",
        "            player_list_flattened,\n",
        "            vector_list_flattened=self.state['last_dual_soln'],\n",
        "            tol=self.state['tol'],\n",
        "            atol=self.state['atol'],\n",
        "            n_steps=self.state['n_steps'],\n",
        "            device=self.device\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        print('metamatrix_conjugate_gradient time', end_time-start_time)\n",
        "\n",
        "        # Store state for use in next nash computation..\n",
        "        self.state['step'] += 1\n",
        "        self.state['last_dual_soln'] = nash_list_flattened\n",
        "        self.state['last_dual_soln_n_iter'] = n_iter\n",
        "        self.state['last_dual_residual'] = res\n",
        "\n",
        "        # Edge case to enable self play in CGD case (since we can compute\n",
        "        # element-wise in place operations in CGD).\n",
        "        for player, nash_flattened, lr in zip(self.state['player_list'], nash_list_flattened, self.state['lr_list']):\n",
        "            idx = 0\n",
        "            for p in player:\n",
        "                ## TODO(shi): double-check this line, to see whether adding or substracting gradient\n",
        "                #p.data += lr*nash_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                p.data -= lr*nash_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                idx += p.numel()\n",
        "        return\n",
        "\n",
        "\n",
        "        # Map dual solution back into primal space.\n",
        "        # mapped_list_flattened = exp_map(player_list_flattened,\n",
        "        #                                 nash_list_flattened,\n",
        "        #                                 bregman=self.bregman)\n",
        "\n",
        "        # # Update parameters in place to update players as optimizer.\n",
        "        # for player, mapped_flattened in zip(self.state['player_list'], mapped_list_flattened):\n",
        "        #     idx = 0\n",
        "        #     for p in player:\n",
        "        #         p.data = mapped_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "        #         idx += p.numel()"
      ],
      "metadata": {
        "id": "FS_FMqEZAjya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiCoPG(TrainingWrapper):\n",
        "    # TODO(yyshi): Horizon, gamma tuning?\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-6,\n",
        "            atol=1e-6,\n",
        "            antisymetric=False,\n",
        "            n_steps = 100,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            policy_lr=0.002,\n",
        "            self_play=False,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "        super(MultiCoPG, self).__init__(\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=batch_size,\n",
        "            critic_lr=critic_lr,\n",
        "            tol=tol,\n",
        "            device=device,\n",
        "            dtype=dtype,\n",
        "            self_play=self_play,\n",
        "            gamma=gamma,\n",
        "            tau=tau,\n",
        "        )\n",
        "\n",
        "        # Optimizers for policies and critics.\n",
        "        self.policy_optim = CMD_RL(\n",
        "            [p.parameters() for p in self.policies],\n",
        "            [policy_lr for _ in self.policies],\n",
        "            antisymetric=antisymetric,\n",
        "            tol=tol,\n",
        "            atol=atol,\n",
        "            n_steps = n_steps,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    def step(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute update step for policies and critics.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            step_start_time = time.time()\n",
        "\n",
        "        # # Use critic function to get advantage.\n",
        "        # values, returns, advantages = [], [], []\n",
        "\n",
        "        # TODO(anonymous): Fix this when making self-play more robust.\n",
        "        critics = self.critics\n",
        "        if self.self_play:\n",
        "            critics = [self.critics[0] for _ in range(len(self.policies))]\n",
        "\n",
        "        num_agent = len(critics)\n",
        "        batch_size = mat_states.shape[1]\n",
        "        trj_len = mat_states.shape[2]\n",
        "        values = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        advantages = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        returns = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "\n",
        "        for i, q in enumerate(critics):\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                ## compute value & advantage tensor\n",
        "                for t in range(trj_len):\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        values[i, j, t] = q(mat_state_unflattened_t0).detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * values[i, j, t + 1] - values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*values[i, j, t+1] - values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] - values[i, j, t])*mat_done[i, j, t]\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*returns[i, j, tt+1]\n",
        "\n",
        "        if self.self_play:\n",
        "            # TODO(anonymous): Currently only supports all symmetric players.\n",
        "            cat_states = torch.cat([mat_states[i] for i in range(len(mat_states))])\n",
        "            cat_states_mask = torch.cat([mat_action_mask[i] for i in range(len(mat_action_mask))])\n",
        "            cat_returns = torch.cat([returns[i] for i in range(len(returns))])\n",
        "\n",
        "            #critic_update(cat_states, cat_returns, self.critics[0], self.critic_optim[0])\n",
        "            critic_update(cat_states, cat_states_mask, cat_returns, self.critics[0],\n",
        "                          self.critic_optim[0], batch_size, trj_len, unflatten_state, self.device)\n",
        "        else:\n",
        "            for i, q in enumerate(self.critics):\n",
        "                critic_update(mat_states[i], mat_action_mask[i], returns[i], q,\n",
        "                              self.critic_optim[i], batch_size, trj_len, unflatten_state, self.device)\n",
        "\n",
        "        ## This part compute the simultaneous gradient descent\n",
        "        gradient_losses = []\n",
        "        log_probs = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        gamma_tensor = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        \n",
        "        ## The following part might be the main cause that the code is slow\n",
        "        for i, p in enumerate(self.policies):\n",
        "            # Our training wrapper assumes that the policy returns a distribution.\n",
        "            # desired: -1, state_dim (4*20*20)\n",
        "            # mat_states: number_trj, trj_len, state_dim\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    ## potential code optimization: break the for-loop when done\n",
        "                    mat_state_cur_unflattened = unflatten_state(mat_states[i, j, t].reshape(1, -1))\n",
        "                    mat_state_cur_unflattened = mat_state_cur_unflattened.to(self.device)\n",
        "                    # instead of calling the network multiple times; \n",
        "                    log_pi = p(mat_state_cur_unflattened).log_prob(mat_actions[i, j, t].to(self.device))\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    # store the log probality of action given state in tensor; set log_prob = 0 if action_mask = 0\n",
        "                    log_probs[i, j, t] = log_pi*mat_action_mask[i, j, t]\n",
        "                    gamma_tensor[i, j, t] = factor\n",
        "\n",
        "                    if(j==0 and t==0):\n",
        "                        policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            gradient_losses.append(-policyloss/batch_size)\n",
        "\n",
        "        cumsum_log_probs = torch.cumsum(log_probs, dim=2)\n",
        "        #cumsum_log_probs = torch.zeros_like(log_probs, device=self.device)\n",
        "        cumsum_log_probs[:,:,1:]=cumsum_log_probs[:,:,0:trj_len-1]\n",
        "        cumsum_log_probs[:,:,0]=0\n",
        "\n",
        "        # Compute Hessian objectives.\n",
        "        hessian_losses = torch.zeros((num_agent, num_agent))\n",
        "        for i in range(num_agent):\n",
        "            for j in range(num_agent):\n",
        "                if (i != j):\n",
        "                    term0 = (gamma_tensor[i]*log_probs[i] * log_probs[j] * advantages[i]).sum()/batch_size\n",
        "                    term1 = (gamma_tensor[i]*cumsum_log_probs[i] * log_probs[j] * advantages[i]).sum()/batch_size\n",
        "                    term2 = (gamma_tensor[i]*cumsum_log_probs[j] * log_probs[i] * advantages[i]).sum()/batch_size\n",
        "                    hessian_losses[i, j] = -term0-term1-term2\n",
        "\n",
        "\n",
        "        ## The above part might be the main cause that the code is slow\n",
        "\n",
        "        # TODO(yyshi): REVISIT this part - the Hessian matrix is not symmetric; we use (H_o+H_o.T)/2\n",
        "        # hessian_losses = (hessian_losses+hessian_losses.T)/2\n",
        "\n",
        "        # Update the policy parameters.\n",
        "        self.policy_optim.zero_grad()\n",
        "        self.policy_optim.step(gradient_losses, hessian_losses, cgd=True)\n",
        "\n",
        "        gradient_losses.clear()\n",
        "        # hessian_losses.clear()\n",
        "\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            print('MultiCoPG step took:', time.time() - step_start_time)"
      ],
      "metadata": {
        "id": "83TOMehN6EQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiCoPG Algorithm"
      ],
      "metadata": {
        "id": "Jwi3Fdov7TCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings (CHECK THESE BEFORE RUNNING).\n",
        "device = torch.device('cuda:0')\n",
        "# device = torch.device('cpu') # Uncomment to use CPU.\n",
        "batch_size = 16\n",
        "#policy training iteration\n",
        "n_steps = 1000\n",
        "last_teps = None\n",
        "verbose = True\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "tySN812IGaNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a policy and critic; we will use self play and a symmetric critic for this game.\n",
        "p1 = policy().to(device).type(dtype)\n",
        "q = critic().to(device).type(dtype)\n",
        "## copy the policy and critic 4 times\n",
        "policies = [p1 for _ in range(4)]\n",
        "critics = [q for _ in range(4)]"
      ],
      "metadata": {
        "id": "EIr1D4kdGaNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training environment with env provided.\n",
        "train_wrap = MultiCoPG(\n",
        "    env,\n",
        "    policies,\n",
        "    critics,\n",
        "    batch_size=batch_size,\n",
        "    antisymetric=True,\n",
        "    self_play=False,\n",
        "    policy_lr=0.001,\n",
        "    tol=1e-2,\n",
        "    atol=5e-2,\n",
        "    n_steps = 20,\n",
        "    critic_lr=1e-3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print('device:', device)\n",
        "print('batch_size:', batch_size)\n",
        "print('n_steps:', n_steps)\n",
        "\n",
        "run_id = \"copg_try1_lr1e-3_smallnetwork\"\n",
        "run_location = os.path.join('/content/PCGD/', run_id)\n",
        "if not os.path.exists(run_location):\n",
        "    os.makedirs(run_location)\n",
        "\n",
        "last_teps = None\n",
        "if last_teps is None:\n",
        "    last_teps = 0\n",
        "    writer_tps = []\n",
        "    writer_p1 = []\n",
        "    writer_p2 = []\n",
        "    writer_p3 = []\n",
        "    writer_p4 = []\n",
        "    writer_eplen = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRyQ69dT38SG",
        "outputId": "b87fde32-87ea-4a28-f352-00df2c28d6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "batch_size: 16\n",
            "n_steps: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t_eps in range(last_teps, n_steps):\n",
        "    # Sample and compute update.\n",
        "    print('t_eps', t_eps)\n",
        "    states, actions, action_mask, rewards, done, unflatten_state = train_wrap.sample(verbose=verbose)\n",
        "    train_wrap.step(states, actions, action_mask, rewards, done, unflatten_state, verbose=verbose)\n",
        "\n",
        "    if ((t_eps + 1) % 5) == 0:\n",
        "        print(\"logging progress:\", t_eps + 1)\n",
        "\n",
        "        # Calculating discounted average reward for each agent.\n",
        "        disc_avg_reward = []\n",
        "        # pdb.set_trace()\n",
        "        for i in range(4):\n",
        "            total_sum = 0.\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                cumsum = 0.\n",
        "                for t in range(rewards.shape[2]):\n",
        "                    cumsum *= 0.99\n",
        "                    cumsum += rewards[i, j, t].cpu().item()\n",
        "                    if (done[i, j, t] == 0):\n",
        "                        total_sum += cumsum\n",
        "                        break\n",
        "\n",
        "            disc_avg_reward.append(total_sum/batch_size)\n",
        "\n",
        "        print('discounted sum of reward', disc_avg_reward, 'steps', t_eps)\n",
        "        # Log values to Tensorboard.\n",
        "\n",
        "        writer_tps.append(t_eps)\n",
        "        writer_p1.append(disc_avg_reward[0])\n",
        "        writer_p2.append(disc_avg_reward[1])\n",
        "        writer_p3.append(disc_avg_reward[2])\n",
        "        writer_p4.append(disc_avg_reward[3])\n",
        "        writer_eplen.append(rewards.shape[2])\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if ((t_eps + 1) % 20) == 0:\n",
        "        print('saving checkpoint:', t_eps + 1)\n",
        "\n",
        "        actor_path = os.path.join(run_location, 'actor1_' + str(t_eps + 1) + '.pth')\n",
        "        critic_path = os.path.join(run_location, 'critic1_' + str(t_eps + 1) + '.pth')\n",
        "        torch.save(p1.state_dict(), actor_path)\n",
        "        torch.save(q.state_dict(), critic_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uvJ-PJZ7f_P",
        "outputId": "e57b4de5-e0bd-44e1-bed9-f676c1f0bb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t_eps 0\n",
            "sample took: 7.401956081390381\n",
            "residual_tolerance tensor(0.4500, device='cuda:0')\n",
            "conjugate number of step, 0 rdotr tensor(44.9956, device='cuda:0')\n",
            "conjugate number of step, 1 rdotr tensor(4.0819, device='cuda:0')\n",
            "conjugate number of step, 2 rdotr tensor(2.7116, device='cuda:0')\n",
            "conjugate number of step, 3 rdotr tensor(2.5697, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(actor_path) \n",
        "files.download(critic_path) "
      ],
      "metadata": {
        "id": "RbXvthRG0zCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p2 = policy().to(device).type(dtype)\n",
        "p2.load_state_dict(torch.load(actor_path))"
      ],
      "metadata": {
        "id": "AtmXnOBV1kfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiSimGD Algorithm"
      ],
      "metadata": {
        "id": "7nqvR7tT5wrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings (CHECK THESE BEFORE RUNNING).\n",
        "device = torch.device('cuda:0')\n",
        "# device = torch.device('cpu') # Uncomment to use CPU.\n",
        "batch_size = 16\n",
        "#policy training iteration\n",
        "n_steps = 2000\n",
        "last_teps = None\n",
        "verbose = False\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "A2T7BN8doYXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a policy and critic; we will use self play and a symmetric critic for this game.\n",
        "p1 = policy().to(device).type(dtype)\n",
        "q = critic().to(device).type(dtype)\n",
        "## copy the policy and critic 4 times\n",
        "policies = [p1 for _ in range(4)]\n",
        "critics = [q for _ in range(4)]"
      ],
      "metadata": {
        "id": "dj7jVRt-o3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training environment with env provided.\n",
        "train_wrap = MultiSimGD(\n",
        "    env,\n",
        "    policies,\n",
        "    critics,\n",
        "    batch_size=batch_size,\n",
        "    self_play=False,\n",
        "    critic_lr=1e-3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print('device:', device)\n",
        "print('batch_size:', batch_size)\n",
        "print('n_steps:', n_steps)"
      ],
      "metadata": {
        "id": "VLdtH_JXplP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if last_teps is None:\n",
        "    last_teps = 0\n",
        "\n",
        "writer_tps = []\n",
        "writer_p1 = []\n",
        "writer_p2 = []\n",
        "writer_p3 = []\n",
        "writer_p4 = []\n",
        "writer_eplen = []\n",
        "for t_eps in range(last_teps, n_steps):\n",
        "    # Sample and compute update.\n",
        "    print('t_eps', t_eps)\n",
        "    states, actions, action_mask, rewards, done, unflatten_state = train_wrap.sample(verbose=verbose)\n",
        "    train_wrap.step(states, actions, action_mask, rewards, done, unflatten_state, verbose=verbose)\n",
        "\n",
        "    if ((t_eps + 1) % 20) == 0:\n",
        "        print(\"logging progress:\", t_eps + 1)\n",
        "\n",
        "        # Calculating discounted average reward for each agent.\n",
        "        disc_avg_reward = []\n",
        "        # pdb.set_trace()\n",
        "        for i in range(4):\n",
        "            total_sum = 0.\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                cumsum = 0.\n",
        "                for t in range(rewards.shape[2]):\n",
        "                    cumsum *= 0.99\n",
        "                    cumsum += rewards[i, j, t].cpu().item()\n",
        "                    if (done[i, j, t] == 0):\n",
        "                        total_sum += cumsum\n",
        "                        break\n",
        "\n",
        "            disc_avg_reward.append(total_sum/batch_size)\n",
        "\n",
        "        print('discounted sum of reward', disc_avg_reward, 'steps', t_eps)\n",
        "        # Log values to Tensorboard.\n",
        "\n",
        "        writer_tps.append(t_eps)\n",
        "        writer_p1.append(disc_avg_reward[0])\n",
        "        writer_p2.append(disc_avg_reward[1])\n",
        "        writer_p3.append(disc_avg_reward[2])\n",
        "        writer_p4.append(disc_avg_reward[3])\n",
        "        writer_eplen.append(rewards.shape[2])\n",
        "\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FbYdBUQ3o1Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}