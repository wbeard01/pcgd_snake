{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "9c9iiuEGkGKi",
        "outputId": "65539356-666c-4838-c981-421a37cf4a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 782,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kc-ml2/marlenv.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8bdVlGgvzxW",
        "outputId": "0cc46a47-10b2-4b06-f852-7d1285d574cd"
      },
      "execution_count": 783,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marlenv' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/marlenv/"
      ],
      "metadata": {
        "id": "1fYAHaQfs7en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3125447a-ce10-4c08-a229-5bc6a5c68466"
      },
      "execution_count": 784,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/marlenv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kel1f9RSwJPK",
        "outputId": "62c5601a-2849-449e-f7f9-6f0061b60c66"
      },
      "execution_count": 785,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/marlenv\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym==0.24.1 in /usr/local/lib/python3.8/dist-packages (from marlenv===1.0.0test1) (0.24.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym==0.24.1->marlenv===1.0.0test1) (3.12.0)\n",
            "Installing collected packages: marlenv\n",
            "  Attempting uninstall: marlenv\n",
            "    Found existing installation: marlenv 1.0.0test1\n",
            "    Can't uninstall 'marlenv'. No files were found to uninstall.\n",
            "  Running setup.py develop for marlenv\n",
            "Successfully installed marlenv-1.0.0test1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import marlenv\n",
        "import marlenv.envs\n",
        "from google.colab import files\n",
        "\n",
        "custom_reward_func = {\n",
        "    'fruit': 1.0,\n",
        "    'kill': 20.0,\n",
        "    'lose': -1.0,\n",
        "    'time': 0.0,\n",
        "    'win': 1.0\n",
        "}\n",
        "env = gym.make(\n",
        "    'Snake-v1',\n",
        "    # height=20,       # Height of the grid map\n",
        "    # width=20,        # Width of the grid map\n",
        "    # num_snakes=4,    # Number of snakes to spawn on grid\n",
        "    # snake_length=3,  # Initial length of the snake at spawn time\n",
        "    # vision_range=10,  # Vision range (both width height), map returned if None\n",
        "    # frame_stack=1,   # Number of observations to stack on return\n",
        "    # num_envs=2,\n",
        "    height=20,\n",
        "    width=20,\n",
        "    num_snakes=4,\n",
        "    num_fruits = 8,\n",
        "    snake_length=3,\n",
        "    vision_range=None,\n",
        "    frame_stack=1,\n",
        "    observer='snake',\n",
        "    # full_observation=False,\n",
        "    reward_func=custom_reward_func\n",
        ")\n",
        "\n",
        "# env = gym.make('Snake-v1', reward_func=custom_reward_func)\n"
      ],
      "metadata": {
        "id": "9Gcab50zva4S"
      },
      "execution_count": 786,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()"
      ],
      "metadata": {
        "id": "pmJUrM2QrHWk"
      },
      "execution_count": 787,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbW9OYr5rm-d",
        "outputId": "9c84a9fb-55e6-4de0-8a33-34281569b45b"
      },
      "execution_count": 788,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 20, 20, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 788
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(obs[0, :, :, 0]) #locations of the walls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Fm7efF5DpvE8",
        "outputId": "43bccca3-c369-430f-feb7-8316a0b62897"
      },
      "execution_count": 789,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff9d96dc280>"
            ]
          },
          "metadata": {},
          "execution_count": 789
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(obs[0, :, :, 1]) #locations of the fruits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0wVZM5uXuFso",
        "outputId": "27b9d33a-b879-4670-8d36-bea4102ce4bd"
      },
      "execution_count": 790,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff9d94d8910>"
            ]
          },
          "metadata": {},
          "execution_count": 790
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(obs[0, :, :, 2]+obs[0, :, :, 3]+obs[0, :, :, 4]) #locations of oppenents?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0TbAU-fluNao",
        "outputId": "077f5019-0f35-4570-a180-8dbebaf170e0"
      },
      "execution_count": 791,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff9d948fac0>"
            ]
          },
          "metadata": {},
          "execution_count": 791
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(obs[0, :, :, 5]+obs[0, :, :, 6]+obs[0, :, :, 7]) #locations of the snake itself？"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vo2wVBwLwSsZ",
        "outputId": "4e80678c-89f6-4709-f6cd-01fedebab724"
      },
      "execution_count": 792,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff9d978c730>"
            ]
          },
          "metadata": {},
          "execution_count": 792
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class policy(nn.Module):\n",
        "    \"\"\"General policy model for calculating action policy from state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(policy, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Conv2d(8, 4, 2),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Conv2d(4, 4, 2),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Flatten(),\n",
        "                                   nn.Linear(1296, 64),\n",
        "                                   nn.ReLU(),\n",
        "                                  #  nn.Linear(256, 64),\n",
        "                                  #  nn.ReLU(),\n",
        "                                  #  nn.Linear(64, 64),\n",
        "                                  #  nn.ReLU(),\n",
        "                                   nn.Linear(64, 3),\n",
        "                                   nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        mu = self.actor(state)\n",
        "        return Categorical(mu)\n",
        "\n",
        "class critic(nn.Module):\n",
        "    \"\"\"Critic model for estimating value from state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(critic, self).__init__()\n",
        "\n",
        "        self.critic = nn.Sequential(nn.Conv2d(8, 8, 2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(8, 8, 2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Flatten(),\n",
        "                                    nn.Linear(2592, 256),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(256, 64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(64, 64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value"
      ],
      "metadata": {
        "id": "jRjOBpbHomPH"
      },
      "execution_count": 793,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import pdb\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import pdb\n",
        "\n",
        "# TODO(yyshi): Casework for this?\n",
        "DEFAULT_DTYPE = torch.float32\n",
        "torch.set_default_dtype(DEFAULT_DTYPE)\n",
        "\n",
        "def zero_grad(params):\n",
        "    \"\"\"Given some list of Tensors, zero and reset gradients.\"\"\"\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            p.grad.detach()\n",
        "            p.grad.zero_()\n",
        "\n",
        "def flatten_filter_none(grad_list, param_list,\n",
        "                        detach=False,\n",
        "                        neg=False,\n",
        "                        device=torch.device('cpu')):\n",
        "    \"\"\"\n",
        "    Given a list of Tensors with possible None values, returns single Tensor\n",
        "    with None removed and flattened.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for grad, param in zip(grad_list, param_list):\n",
        "        if grad is None:\n",
        "            filtered.append(torch.zeros(param.numel(), device=device, requires_grad=True))\n",
        "        else:\n",
        "            filtered.append(grad.contiguous().view(-1))\n",
        "\n",
        "    result = torch.cat(filtered) if not neg else -torch.cat(filtered)\n",
        "\n",
        "    # Use this only if higher order derivatives are not needed.\n",
        "    if detach:\n",
        "        result.detach()\n",
        "\n",
        "    return result\n",
        "\n",
        "# TODO(anonymous): make this user interface cleaner.\n",
        "class SGD(object):\n",
        "    \"\"\"Optimizer class for simultaneous SGD\"\"\"\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 device=torch.device('cpu')\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param lr_list: list of learning rates per player optimizer.\n",
        "        \"\"\"\n",
        "        # Store optimizer state.\n",
        "        player_list = [list(elem) for elem in player_list]\n",
        "        self.state = {'step': 0,\n",
        "                      'player_list': player_list,\n",
        "                      'lr_list': lr_list}\n",
        "        # TODO(anonymous): set this device in CMD algorithm.\n",
        "        self.device = device\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for player in self.state['player_list']:\n",
        "            zero_grad(player)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.state\n",
        "\n",
        "    def step(self, loss_list):\n",
        "        #print('loss function', loss_list)\n",
        "        grad_list = [\n",
        "            autograd.grad(loss, player, retain_graph=True, allow_unused=True)\n",
        "            for loss, player in zip(loss_list, self.state['player_list'])\n",
        "        ]\n",
        "\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[0]))\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[1]))\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[2]))\n",
        "\n",
        "\n",
        "\n",
        "        for grad, player, lr in zip(grad_list, self.state['player_list'], self.state['lr_list']):\n",
        "            for player_elem, grad_elem in zip(player, grad):\n",
        "                if grad_elem is not None:\n",
        "                    player_elem.data -= grad_elem * lr\n",
        "\n",
        "def critic_update(mat_states, mat_action_mask, returns, q, optim_q, batch_size, trj_len, unflatten_state, device):\n",
        "    counter = 0\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            mat_state_unflattened_t0 = unflatten_state(mat_states[j, t, :].reshape(1, -1))\n",
        "            mat_state_unflattened_t0 = mat_state_unflattened_t0.to(device)\n",
        "            if (j==0 and t == 0):\n",
        "                #mat_state_list = mat_state_unflattened_t0\n",
        "                value_pred = q(mat_state_unflattened_t0)\n",
        "            else:\n",
        "                if(mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                #mat_state_list = torch.cat((mat_state_list, mat_state_unflattened_t0), dim = 0)\n",
        "                value_pred_current = q(mat_state_unflattened_t0)\n",
        "                value_pred = torch.cat((value_pred, value_pred_current), dim=0)\n",
        "\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            return_current = returns[j, t].reshape(1, -1)\n",
        "            return_current = return_current.to(device)\n",
        "            if (j == 0 and t == 0):\n",
        "                return_list = return_current\n",
        "            else:\n",
        "                if (mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                return_list = torch.cat((return_list, return_current), dim=0)\n",
        "\n",
        "    #value_pred = q(c)\n",
        "    critic_loss = (return_list - value_pred).pow(2).mean()\n",
        "    optim_q.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    optim_q.step()\n",
        "    critic_loss = critic_loss.detach().cpu()\n",
        "\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    return critic_loss\n",
        "\n",
        "def get_advantage(\n",
        "        next_value, reward_mat, value_mat, mat_done,\n",
        "        gamma=0.99, tau=0.95, device=torch.device('cpu')\n",
        "):\n",
        "    insert_tensor = torch.tensor([[float(next_value)]], device=device)\n",
        "    value_mat = torch.cat([value_mat, insert_tensor])\n",
        "    gae = 0\n",
        "    returns = []\n",
        "\n",
        "    for i in reversed(range(len(reward_mat))):\n",
        "        delta = reward_mat[i] + gamma * value_mat[i + 1] * mat_done[i] - value_mat[i]\n",
        "        gae = delta + gamma * tau * mat_done[i] * gae\n",
        "        returns.append(gae + value_mat[i])\n",
        "\n",
        "    # Reverse ordering.\n",
        "    returns.reverse()\n",
        "\n",
        "    vals = torch.cat(returns).reshape(-1, 1)\n",
        "    return vals\n",
        "\n",
        "# def critic_update(state_mat, return_mat, q, optim_q):\n",
        "#     val_loc = q(state_mat)\n",
        "\n",
        "#     critic_loss = (return_mat - val_loc).pow(2).mean()\n",
        "\n",
        "#     optim_q.zero_grad()\n",
        "#     critic_loss.backward()\n",
        "#     optim_q.step()\n",
        "\n",
        "#     critic_loss = critic_loss.detach().cpu()\n",
        "\n",
        "#     return critic_loss\n",
        "\n",
        "# TODO(anonymous): Revisit this?\n",
        "# def get_advantage(\n",
        "#         next_value, reward_mat, value_mat, masks,\n",
        "#         gamma=0.99, tau=0.95, device=torch.device('cpu')\n",
        "# ):\n",
        "#     insert_tensor = torch.tensor([[float(next_value)]], device=device)\n",
        "#     value_mat = torch.cat([value_mat, insert_tensor])\n",
        "#     gae = 0\n",
        "#     returns = []\n",
        "\n",
        "#     for i in reversed(range(len(reward_mat))):\n",
        "#         delta = reward_mat[i] + gamma * value_mat[i + 1] * masks[i] - value_mat[i]\n",
        "#         gae = delta + gamma * tau * masks[i] * gae\n",
        "#         returns.append(gae + value_mat[i])\n",
        "\n",
        "#     # Reverse ordering.\n",
        "#     returns.reverse()\n",
        "\n",
        "#     vals = torch.cat(returns).reshape(-1, 1)\n",
        "#     return vals"
      ],
      "metadata": {
        "id": "KkBlB1lWpDzU"
      },
      "execution_count": 794,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingWrapper:\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-3,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            self_play=False,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "            critic_optim_kwargs={},\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store game environment.\n",
        "        self.env = env\n",
        "        # Training parameters.\n",
        "        self.policies = policies\n",
        "        self.critics = critics\n",
        "\n",
        "        # Sampling parameters.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Device to be used.\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # TODO(anonymous): Implement self play in a cleaner way.\n",
        "        self.self_play = self_play\n",
        "        if self.self_play:\n",
        "            assert (len(self.critics) == 1)\n",
        "            self.critic_optim = [\n",
        "                torch.optim.Adam(self.critics[0].parameters(), lr=critic_lr)\n",
        "            ]\n",
        "        else:\n",
        "            self.critic_optim = [\n",
        "                torch.optim.Adam(c.parameters(), lr=critic_lr, **critic_optim_kwargs) for c in self.critics\n",
        "            ]\n",
        "\n",
        "            # GAE estimation work.\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "    ## start from here on 2023/01/05 - change the data storage type to tensor\n",
        "    def sample(self, verbose=True):\n",
        "        \"\"\"\n",
        "        :param verbose: Print debugging information if requested.\n",
        "        Sample observations actions and states. Returns trajectory observations\n",
        "        actions rewards in single list format (which seperates trajectories\n",
        "        using done mask).\n",
        "        \"\"\"\n",
        "        # We collect trajectories all into one list (using a done mask) for simplicity.\n",
        "        num_agents = len(self.policies)\n",
        "        max_traj_length = 1000\n",
        "        mat_all_states_t = []\n",
        "        mat_all_actions_t = []\n",
        "        mat_all_action_mask_t = []\n",
        "        mat_all_rewards_t = []\n",
        "        mat_all_done_t = []\n",
        "\n",
        "        # If verbose, show how long sampling takes.\n",
        "        if verbose:\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "        for j in range(self.batch_size):\n",
        "            # Reset environment for each trajectory in batch.\n",
        "            mat_states_t = []\n",
        "            mat_actions_t = []\n",
        "            mat_action_mask_t = []\n",
        "            mat_rewards_t = []\n",
        "            mat_done_t = []\n",
        "\n",
        "            obs = self.env.reset()\n",
        "            dones = [False for _ in range(num_agents)]\n",
        "\n",
        "            for t in range(max_traj_length):\n",
        "                # Record state...\n",
        "                mat_states_t.append(obs)\n",
        "                # action_mask = 1 if actively taking action; otherwise, action_mask = 0\n",
        "                mat_action_mask_t.append([1. - int(elem) for elem in dones])\n",
        "\n",
        "                # Since env is usually on CPU, send observation to GPU,\n",
        "                # sample, then collect back to CPU.\n",
        "                actions = []\n",
        "                for i in range(num_agents):\n",
        "                    policy = self.policies[i]\n",
        "                    obs_gpu = torch.tensor([obs[i]], device=self.device, dtype=self.dtype).permute(0,3,1,2)\n",
        "                    dist = policy(obs_gpu)\n",
        "\n",
        "                    action = dist.sample().cpu().numpy()\n",
        "\n",
        "                    # TODO(anonymous): Pytorch doesn't handle 0-dim tensors (a.k.a scalars well)\n",
        "                    if action.ndim == 1 and action.size == 1:\n",
        "                        action = action[0]\n",
        "                    else:\n",
        "                        # action = np.squeeze(dist.sample().cpu().numpy(), axis=1)\n",
        "                        action = np.squeeze(action, axis=1)\n",
        "\n",
        "                    actions.append(action)\n",
        "\n",
        "                # Advance environment one step forwards.\n",
        "                obs, rewards, dones, _ = self.env.step(actions)\n",
        "\n",
        "                # Record actions, rewards, and inverse done mask.\n",
        "                mat_actions_t.append(actions)\n",
        "\n",
        "                # 2023/01/05 YYSHI TODO(Comment out the random probablity for breaking out)\n",
        "                # if(t > 10):\n",
        "                #     p = torch.rand(1)\n",
        "                #     if(p>0.96):\n",
        "                #         # find the alive snake\n",
        "                #         index = [i for i, x in enumerate(dones) if not (x)]\n",
        "                #         for j in index:\n",
        "                #             rewards[j] += t*0.3\n",
        "                #             #print('Snake', j, 'Alive, when game finished!!', dones[j], p)\n",
        "                #             dones[j] = True\n",
        "\n",
        "                mat_rewards_t.append(rewards)\n",
        "                mat_done_t.append([1. - int(elem) for elem in dones])\n",
        "\n",
        "                # Break once all players are done.\n",
        "                if all(dones):\n",
        "                    break\n",
        "\n",
        "                # elif sum(dones) == 3: #only 1 snake alive\n",
        "                #     # find the alive snake\n",
        "                #     index = [i for i, x in enumerate(dones) if not (x)]\n",
        "                #     rewards[index] += 20\n",
        "                #     dones[index] = True\n",
        "                #     print('Snake', index, 'Alive!!')\n",
        "                #     break\n",
        "                # else:\n",
        "                #     pass\n",
        "\n",
        "            mat_all_states_t.append(mat_states_t)\n",
        "            mat_all_actions_t.append(mat_actions_t)\n",
        "            mat_all_action_mask_t.append(mat_action_mask_t)\n",
        "            mat_all_rewards_t.append(mat_rewards_t)\n",
        "            mat_all_done_t.append(mat_done_t)\n",
        "\n",
        "        # Create data on GPU for later update step\n",
        "        # TODO(yyshi): list agent, each element of the list contains # of trajectories,\n",
        "        # TODO(yyshi): and each trajectories contain # of samples\n",
        "        # (agent, num_traj, traj_length)\n",
        "        # have trajectory with splits same to the player with the longest trajectory.\n",
        "        trj_len = 0\n",
        "        for j in range(self.batch_size):\n",
        "            trj_len = np.maximum(trj_len, len(mat_all_states_t[j]))\n",
        "\n",
        "        state_dim = int(obs.size/obs.shape[0])\n",
        "\n",
        "        state_shape = obs_gpu.shape[1:]\n",
        "        mat_states = torch.zeros((num_agents, self.batch_size, trj_len, state_dim))\n",
        "        mat_actions = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_action_mask = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_rewards = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_done = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "\n",
        "        for j in range(self.batch_size):\n",
        "            current_trj_len = len(mat_all_states_t[j])\n",
        "            for i in range(current_trj_len):\n",
        "                for k in range(num_agents):\n",
        "                    mat_states[k, j, i] = torch.flatten(torch.tensor(mat_all_states_t[j][i][k], dtype=self.dtype, device=self.device).permute(2,0,1))\n",
        "                    mat_actions[k, j, i] = torch.tensor(mat_all_actions_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    mat_rewards[k, j, i] = torch.tensor(mat_all_rewards_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    # if trajectory is not active, mask = 0;\n",
        "                    mat_action_mask[k, j, i] = torch.tensor(mat_all_action_mask_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    mat_done[k, j, i] = torch.tensor(mat_all_done_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "\n",
        "        unflatten_state = torch.nn.Unflatten(1, state_shape)\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            print('sample took:', time.time() - batch_start_time)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # for t in range(trj_len):\n",
        "        #     state_unflatten = unflatten(mat_states[0, 0, t, :].reshape(1, -1))\n",
        "        #     plt.imshow(state_unflatten[0, 0, :, :] + 2*state_unflatten[0, 1, :, :] + 3*state_unflatten[0, 2, :, :] + 4*state_unflatten[0, 3, :, :])\n",
        "        #     plt.savefig(\"Instant{}.png\".format(t))\n",
        "\n",
        "        # mat_states, [num_agent, num_traj, traj_len, obs_dim] observations of all agents\n",
        "        # mat_actions, [num_agent, num_traj, traj_len] actions of all agents\n",
        "        # mat_action_mask, [num_agent, num_traj, traj_len] mat_action_mask[i, j, t] = 0 if agent i in trajectory j has died at time t-1 or earlier\n",
        "        # mat_rewards, [num_agent, num_traj, traj_len], rewards of all agents\n",
        "        # mat_done, [num_agent, num_traj, traj_len], mat_done[i, j, t] = 0 if agent i in trajectory j has died at time t or earlier\n",
        "        return mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state"
      ],
      "metadata": {
        "id": "DHymE_QL3NgS"
      },
      "execution_count": 795,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(yyshi): Refine this class structure based on feedback.\n",
        "class MultiSimGD(TrainingWrapper):\n",
        "    # TODO(yyshi): Horizon, gamma tuning?\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-3,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            self_play=False,\n",
        "            policy_lr=0.002,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "            critc_optim_kwargs={}\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "        super(MultiSimGD, self).__init__(\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=batch_size,\n",
        "            critic_lr=critic_lr,\n",
        "            tol=tol,\n",
        "            device=device,\n",
        "            dtype=dtype,\n",
        "            self_play=self_play,\n",
        "            gamma=gamma,\n",
        "            tau=tau,\n",
        "            critic_optim_kwargs=critc_optim_kwargs\n",
        "        )\n",
        "\n",
        "        # Optimizers for policies and critics.\n",
        "        self.policy_optim = SGD(\n",
        "            [p.parameters() for p in self.policies],\n",
        "            [policy_lr for _ in self.policies],\n",
        "            device=device\n",
        "        )\n",
        "    \n",
        "    def step(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute update step for policies and critics.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            step_start_time = time.time()\n",
        "\n",
        "        # TODO(anonymous): Fix this when making self-play more robust.\n",
        "        critics = self.critics\n",
        "        if self.self_play:\n",
        "            critics = [self.critics[0] for _ in range(len(self.policies))]\n",
        "\n",
        "        # Use critic function to get advantage.\n",
        "        # values, returns, advantages = [], [], []\n",
        "\n",
        "        # Compute generalized advantage estimation (GAE).\n",
        "        num_agent = len(critics)\n",
        "        batch_size = mat_states.shape[1]\n",
        "        trj_len = mat_states.shape[2]\n",
        "\n",
        "        values = torch.zeros((num_agent, batch_size, trj_len))\n",
        "        advantages = torch.zeros((num_agent, batch_size, trj_len))\n",
        "        returns = torch.zeros((num_agent, batch_size, trj_len))\n",
        "\n",
        "        for i, q in enumerate(critics):\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                ## compute value & advantage tensor\n",
        "                for t in range(trj_len):\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        values[i, j, t] = q(mat_state_unflattened_t0).detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * values[i, j, t + 1] - values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*values[i, j, t+1] - values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] - values[i, j, t])*mat_done[i, j, t]\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*returns[i, j, tt+1]\n",
        "\n",
        "        # Use sampled values to fit critic model.\n",
        "        if self.self_play:\n",
        "            cat_states = torch.cat([mat_states[i] for i in range(len(mat_states))])\n",
        "            cat_states_mask = torch.cat([mat_action_mask[i] for i in range(len(mat_action_mask))])\n",
        "            cat_returns = torch.cat([returns[i] for i in range(len(returns))])\n",
        "\n",
        "            #critic_update(cat_states, cat_returns, self.critics[0], self.critic_optim[0])\n",
        "            critic_update(cat_states, cat_states_mask, cat_returns, self.critics[0],\n",
        "                          self.critic_optim[0], batch_size, trj_len, unflatten_state, self.device)\n",
        "        else:\n",
        "            for i, q in enumerate(self.critics):\n",
        "                critic_update(mat_states[i], mat_action_mask[i], returns[i], q,\n",
        "                              self.critic_optim[i], batch_size, trj_len, unflatten_state, self.device)\n",
        "\n",
        "        # Include last index to compute pairs.\n",
        "        # traj_indices.append(mat_done[0].size(0))\n",
        "\n",
        "        log_probs = []\n",
        "        gradient_losses = []\n",
        "        for i, p in enumerate(self.policies):\n",
        "            # Our training wrapper assumes that the policy returns a distribution.\n",
        "            # desired: -1, state_dim (4*20*20)\n",
        "            # mat_states: number_trj, trj_len, state_dim\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    ## potential code optimization: break the for-loop when done\n",
        "                    mat_state_cur_unflattened = unflatten_state(mat_states[i, j, t].reshape(1, -1))\n",
        "                    mat_state_cur_unflattened = mat_state_cur_unflattened.to(self.device)\n",
        "                    log_pi = p(mat_state_cur_unflattened).log_prob(mat_actions[i, j, t].to(self.device))\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    if(j==0 and t==0):\n",
        "                        policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            gradient_losses.append(-policyloss/batch_size)\n",
        "\n",
        "\n",
        "        # Update the policy parameters.\n",
        "        self.policy_optim.zero_grad()\n",
        "        self.policy_optim.step(gradient_losses)\n",
        "        gradient_losses.clear()\n",
        "\n",
        "\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            print('MultiSimGD step took:', time.time() - step_start_time)"
      ],
      "metadata": {
        "id": "ffAzsb2b14Og"
      },
      "execution_count": 796,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Dx(x_dual, alpha = 1):\n",
        "        return x_dual / alpha\n",
        "\n",
        "def Dx_inv(x_dual, alpha = 1):\n",
        "    return x_dual * alpha\n",
        "\n",
        "def Dxx_vp(x_primal, vec, alpha = 1):\n",
        "    # Does not need to be in-place.\n",
        "    return vec * alpha\n",
        "\n",
        "def Dxx_inv_vp(x_primal, vec, alpha = 1):\n",
        "    return vec / alpha\n",
        "\n",
        "def antivp(\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened,\n",
        "        transpose=False,\n",
        "        device=torch.device('cpu'),\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param hessian_loss_list: list of objective functions for hessian computation\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: list of flattened vectors for each player\n",
        "    :param bregman: dictionary representing bregman potential to use\n",
        "    :param transpose: compute product against transpose if set\n",
        "\n",
        "    Computes right product of antisymmetric metamatrix with a vector of player vectors.\n",
        "    \"\"\"\n",
        "    # TODO(yyshi): add error handling and assertions\n",
        "    # assert(len(hessian_loss_list) == len(player_list))\n",
        "    # assert(len(hessian_loss_list) == len(vector_list))\n",
        "    prod_list = [torch.zeros_like(v, device=device) for v in vector_list_flattened]\n",
        "\n",
        "    for i, row_params in enumerate(player_list):\n",
        "        for j, (col_params, vector_elem) in enumerate(zip(player_list, vector_list_flattened)):\n",
        "            if i == j:\n",
        "                # Diagonal element is the Bregman term.\n",
        "                prod_list[i] += Dxx_vp(player_list_flattened[i], vector_elem)\n",
        "                continue\n",
        "\n",
        "            # Otherwise, we construct our Hessian vector products. Variable\n",
        "            # retain_graph must be set to true, or we cant compute multiple\n",
        "            # subsequent Hessians any more.\n",
        "            left_loss, right_loss = hessian_loss_list[i], hessian_loss_list[j]\n",
        "            \n",
        "            if transpose:\n",
        "                left_loss, right_loss = right_loss, left_loss\n",
        "\n",
        "            # Anti-symmetric decomposition (1/2)(A - A^T)...\n",
        "            left_grad_raw = autograd.grad(left_loss, col_params,\n",
        "                                          create_graph=True,\n",
        "                                          retain_graph=True,\n",
        "                                          allow_unused=True)\n",
        "            left_grad_flattened = flatten_filter_none(left_grad_raw, col_params,\n",
        "                                                      device=device)\n",
        "\n",
        "            left_hvp_raw = autograd.grad(left_grad_flattened, row_params,\n",
        "                                         grad_outputs=vector_elem,\n",
        "                                         create_graph=False,\n",
        "                                         retain_graph=True,\n",
        "                                         allow_unused=True)\n",
        "            left_hvp_flattened = flatten_filter_none(left_hvp_raw, row_params,\n",
        "                                                     device=device)\n",
        "\n",
        "            right_grad_raw = autograd.grad(right_loss, col_params,\n",
        "                                           create_graph=True,\n",
        "                                           retain_graph=True,\n",
        "                                           allow_unused=True)\n",
        "            right_grad_flattened = flatten_filter_none(right_grad_raw, col_params,\n",
        "                                                       device=device)\n",
        "\n",
        "            right_hvp_raw = autograd.grad(right_grad_flattened, row_params,\n",
        "                                          grad_outputs=vector_elem,\n",
        "                                          create_graph=False,\n",
        "                                          retain_graph=True,\n",
        "                                          allow_unused=True)\n",
        "            right_hvp_flattened = flatten_filter_none(right_hvp_raw, row_params,\n",
        "                                                      device=device)\n",
        "\n",
        "            prod_list[i] += 0.5 * (left_hvp_flattened - right_hvp_flattened)\n",
        "\n",
        "    # Detach to get memory back.\n",
        "    prod_list = [elem.detach() for elem in prod_list]\n",
        "\n",
        "    return prod_list\n",
        "\n",
        "def avp(\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened,\n",
        "        transpose=False,\n",
        "        device=torch.device('cpu'),\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param hessian_loss_list: list of objective functions for hessian computation\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: list of flattened vectors for each player\n",
        "    :param bregman: dictionary representing bregman potential to use\n",
        "    :param transpose: compute product against transpose if set\n",
        "\n",
        "    Computes right product of metamatrix with a vector of player vectors.\n",
        "    \"\"\"\n",
        "    # TODO(anonymous): add error handling and assertions\n",
        "    # assert(len(hessian_loss_list) == len(player_list))\n",
        "    # assert(len(hessian_loss_list) == len(vector_list))\n",
        "    # TODO(yyshi): add eta to the function input\n",
        "    \n",
        "    eta = 0.8\n",
        "    \n",
        "    prod_list = [torch.zeros_like(v, device=device) for v in vector_list_flattened]\n",
        "\n",
        "    for i, row_params in enumerate(player_list):\n",
        "        for j, (col_params, vector_elem) in enumerate(zip(player_list, vector_list_flattened)):\n",
        "            if i == j:\n",
        "                # Diagonal element is the Bregman term, i.e., I * vector_elem\n",
        "                prod_list[i] += Dxx_vp(player_list_flattened[i], vector_elem, alpha = 1)\n",
        "                continue\n",
        "\n",
        "            # Otherwise, we construct our Hessian vector products. Variable\n",
        "            # retain_graph must be set to true, or we cant compute multiple\n",
        "            # subsequent Hessians any more.\n",
        "\n",
        "            loss = hessian_loss_list[i, j] if not transpose else hessian_loss_list[j, i]\n",
        "\n",
        "            grad_raw = autograd.grad(loss, col_params,\n",
        "                                     create_graph=True,\n",
        "                                     retain_graph=True,\n",
        "                                     allow_unused=True)\n",
        "\n",
        "            grad_flattened = flatten_filter_none(grad_raw, col_params,\n",
        "                                                 device=device)\n",
        "\n",
        "            # Don't need any higher order derivatives, so create_graph = False.\n",
        "            hvp_raw = autograd.grad(grad_flattened, row_params,\n",
        "                                    grad_outputs=vector_elem,\n",
        "                                    create_graph=False,\n",
        "                                    retain_graph=True,\n",
        "                                    allow_unused=True)\n",
        "            \n",
        "            # hvp_flattened = hvp_raw[0]\n",
        "            hvp_flattened = flatten_filter_none(hvp_raw, row_params,\n",
        "                                                device=device)\n",
        "\n",
        "            prod_list[i] += eta*hvp_flattened\n",
        "\n",
        "    # Detach to get memory back.\n",
        "    prod_list = [elem.detach() for elem in prod_list]\n",
        "    \n",
        "\n",
        "    return prod_list\n",
        "\n",
        "def metamatrix_conjugate_gradient(\n",
        "        grad_loss_list,\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened=None,\n",
        "        mvp=avp,\n",
        "        n_steps=20,\n",
        "        tol=1e-3,\n",
        "        atol=1e-3,\n",
        "        device=torch.device('cpu')\n",
        "):\n",
        "    \"\"\"\n",
        "    :param grad_loss_list: list of loss tensors for each player to compute gradient\n",
        "    :param hessian_loss_list: list of loss tensors for each player to compute hessian\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: initial guess for update solution\n",
        "    :param bregman: dict representing a Bregman potential to be used\n",
        "    :param n_steps: number of iteration steps for conjugate gradient\n",
        "    :param tol: relative residual tolerance threshold from initial vector guess\n",
        "    :param atol: absolute residual tolerance threshold\n",
        "\n",
        "    Compute solution to meta-matrix game form using preconditioned conjugate\n",
        "    gradient method. Since the metamatrix A is not p.s.d, we multiply both sides\n",
        "    by the transpose to ensure p.s.d.\n",
        "\n",
        "    In other words, note that solving Ax = b (where A is meta matrix, x is\n",
        "    vector of update vectors and b is learning rate times vector of gradients\n",
        "    is the same as solving A'x = b' (where A' = (A^T)A and b' = (A^T)b.\n",
        "    \"\"\"\n",
        "\n",
        "    b = []\n",
        "    for loss, param_tensors in zip(grad_loss_list, player_list):\n",
        "        # Get vector list of negative gradients.\n",
        "        grad_raw = autograd.grad(loss, param_tensors,\n",
        "                                 retain_graph=True,\n",
        "                                 allow_unused=True)\n",
        "        \n",
        "        grad_flattened = flatten_filter_none(grad_raw, param_tensors,\n",
        "                                             neg=True, detach=True, device=device)\n",
        "        \n",
        "        b.append(grad_flattened.detach())\n",
        "        # b.append(grad_raw[0].detach())\n",
        "\n",
        "    # Multiplying both sides by transpose to ensure p.s.d.\n",
        "    # r = A^t * b (before we subtract)\n",
        "    r = mvp(hessian_loss_list, player_list, player_list_flattened, b, \n",
        "            transpose=True, device=device)\n",
        "\n",
        "    # Set relative residual threshold based on norm of b.\n",
        "    # norm_At_b = sum(r_elem*r_elem for r_elem in r)\n",
        "    \n",
        "    \n",
        "    norm_At_b = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "    residual_tol = tol * norm_At_b\n",
        "    print('residual_tolerance', residual_tol)\n",
        "\n",
        "    # If no guess provided, start from zero vector.\n",
        "    if vector_list_flattened is None:\n",
        "        vector_list_flattened = [torch.zeros_like(p, device=device)\n",
        "                                 for p in player_list_flattened]\n",
        "    else:\n",
        "        # Compute initial residual if a guess is given.\n",
        "        A_x = mvp(hessian_loss_list, player_list, player_list_flattened, vector_list_flattened,\n",
        "                  transpose=False, device=device)\n",
        "        At_A_x = mvp(hessian_loss_list, player_list, player_list_flattened, A_x,\n",
        "                     transpose=True, device=device)\n",
        "\n",
        "        # torch._foreach_sub_(r, At_A_x)\n",
        "        for r_elem, At_A_x_elem in zip(r, At_A_x):\n",
        "            r_elem -= At_A_x_elem\n",
        "\n",
        "    ## up until here, we have computed r_0 = A^tb - A^t A x0\n",
        "    ## Use preconditioner if available...\n",
        "    # z = r\n",
        "    # if 'Dxx_inv_vp' in bregman:\n",
        "    #     z = [bregman['Dxx_inv_vp'](params, r_elems)\n",
        "    #          for params, r_elems in zip(player_list_flattened, r)]\n",
        "\n",
        "    # Early exit if solution already found.\n",
        "    # rdotr = sum(r_elem*r_elem for r_elem in r)\n",
        "    rdotr = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "\n",
        "    # rdotz = sum(torch.dot(r_elem, z_elem) for r_elem, z_elem in zip(r, z))\n",
        "    if rdotr < residual_tol or rdotr < atol:\n",
        "        return vector_list_flattened, 0, rdotr\n",
        "\n",
        "    # Define p and measure current candidate vector.\n",
        "    p = [r_elem.clone().detach() for r_elem in r]\n",
        "\n",
        "    # Use conjugate gradient to find vector solution.\n",
        "    for i in range(n_steps):\n",
        "        print('conjugate number of step,', i, 'rdotr', rdotr)\n",
        "        step_3 = time.time()\n",
        "        A_p = mvp(hessian_loss_list, player_list, player_list_flattened, p,\n",
        "                  transpose=False, device=device)\n",
        "        At_A_p = mvp(hessian_loss_list, player_list, player_list_flattened, A_p,\n",
        "                     transpose=True, device=device)\n",
        "        #print('One conjugate gradient step took:', time.time() - step_3)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            alpha = torch.div(rdotr, sum(torch.dot(e1, e2) for e1, e2 in zip(p, At_A_p)))\n",
        "            #alpha = torch.div(rdotr, sum(e1*e2 for e1, e2 in zip(p, At_A_p)))\n",
        "\n",
        "            # Update candidate solution and residual, where:\n",
        "            # (1) x_new = x + alpha * p\n",
        "            # (2) r_new = r - alpha * A^t A * p\n",
        "\n",
        "            # torch._foreach_add_(vector_list_flattened, p, alpha=alpha)\n",
        "            # torch._foreach_sub_(r, At_A_p, alpha=alpha)\n",
        "\n",
        "            for vlf_elem, p_elem in zip(vector_list_flattened, p):\n",
        "                vlf_elem += p_elem * alpha\n",
        "            for r_elem, At_A_P_elem in zip(r, At_A_p):\n",
        "                r_elem -= At_A_P_elem * alpha\n",
        "\n",
        "            # Calculate new residual metric\n",
        "            #new_rdotr = sum(r_elem*r_elem for r_elem in r)\n",
        "            new_rdotr = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "\n",
        "            # Break if solution is within threshold\n",
        "            if new_rdotr < atol or new_rdotr < residual_tol:\n",
        "                break\n",
        "\n",
        "            # If preconditioner provided, use it...\n",
        "            # z = r\n",
        "            # if 'Dxx_inv_vp' in bregman:\n",
        "            #     z = [bregman['Dxx_inv_vp'](params, r_elems)\n",
        "            #          for params, r_elems in zip(player_list_flattened, r)]\n",
        "            # new_rdotz = sum(torch.dot(r_elem, z_elem) for r_elem, z_elem in zip(r, z))\n",
        "\n",
        "            # Otherwise, update and continue.\n",
        "            # (3) p_new = r_new + beta * p\n",
        "\n",
        "            # torch._foreach_add(z, p, alpha=beta)\n",
        "            beta = torch.div(new_rdotr, rdotr)\n",
        "            p = [r_elem + p_elem * beta for r_elem, p_elem in zip(r, p)]\n",
        "\n",
        "            rdotr = new_rdotr\n",
        "            # rdotz = new_rdotz\n",
        "\n",
        "    return vector_list_flattened, i + 1, rdotr\n"
      ],
      "metadata": {
        "id": "MzYSB21T56sS"
      },
      "execution_count": 797,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CMD(object):\n",
        "    \"\"\"Optimizer class for the CMD algorithm with differentiable player objectives.\"\"\"\n",
        "\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 antisymetric=False,\n",
        "                 tol=None,\n",
        "                 atol=None,\n",
        "                 n_steps=None,\n",
        "                 device=torch.device('cpu')\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param bregman: dict representing Bregman potential to be used\n",
        "        \"\"\"\n",
        "\n",
        "        # In case, parameter generators are provided.\n",
        "        player_list = [list(elem) for elem in player_list]\n",
        "\n",
        "        # Conjugate gradient will provably converge in number of params steps.\n",
        "        if n_steps is None:\n",
        "            n_steps = sum([sum([elem.numel() for elem in param_list])\n",
        "                           for param_list in player_list])\n",
        "\n",
        "        # Store optimizer state.\n",
        "        self.state = {'step': 0,\n",
        "                      'player_list': player_list,\n",
        "                      'lr_list': lr_list,\n",
        "                      'tol': tol,\n",
        "                      'atol': atol,\n",
        "                      'n_steps': n_steps,\n",
        "                      'last_dual_soln': None,\n",
        "                      'last_dual_soln_n_iter': 0,\n",
        "                      'last_dual_residual': 0.,\n",
        "                      'antisymetric': antisymetric}\n",
        "        # TODO(yyshi): set this device in CMD algorithm.\n",
        "        self.device = device\n",
        "        self.mvp = avp if not antisymetric else antivp\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for player in self.state['player_list']:\n",
        "            zero_grad(player)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.state\n",
        "\n",
        "    def player_list(self):\n",
        "        return self.state['player_list']\n",
        "\n",
        "    def step(self, loss_list):\n",
        "        # Compute flattened player list for some small optimization.\n",
        "        player_list = self.state['player_list']\n",
        "        player_list_flattened = [flatten_filter_none(player, player,\n",
        "                                                     detach=True, device=self.device)\n",
        "                                 for player in player_list]\n",
        "\n",
        "        # Compute dual solution first, before mapping back to primal.\n",
        "        # Use dual solution as initial guess for numerical speed.\n",
        "        nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n",
        "            loss_list,\n",
        "            loss_list,\n",
        "            player_list,\n",
        "            player_list_flattened,\n",
        "            vector_list_flattened=self.state['last_dual_soln'],\n",
        "            tol=self.state['tol'],\n",
        "            atol=self.state['atol'],\n",
        "            n_steps=self.state['n_steps'],\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Store state for use in next nash computation..\n",
        "        self.state['step'] += 1\n",
        "        self.state['last_dual_soln'] = nash_list_flattened\n",
        "        self.state['last_dual_soln_n_iter'] = n_iter\n",
        "        self.state['last_dual_residual'] = res\n",
        "\n",
        "        # Map dual solution back into primal space.\n",
        "        # mapped_list_flattened = exp_map(player_list_flattened,\n",
        "        #                                 nash_list_flattened,\n",
        "        #                                 bregman=self.bregman)\n",
        "\n",
        "        # Update parameters in place to update players as optimizer.\n",
        "        for player, mapped_flattened in zip(self.state['player_list'], mapped_list_flattened):\n",
        "            idx = 0\n",
        "            for p in player:\n",
        "                p.data = mapped_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                idx += p.numel()\n",
        "\n",
        "\n",
        "# TODO(anonymous): May need to fix this optimizer for self-play, specifically update\n",
        "# method, since we need to update data and not just replace it. If this were\n",
        "# self play, only one of the updates calculated from nash would carry through.\n",
        "class CMD_RL(CMD):\n",
        "    \"\"\"RL optimizer using CMD algorithm, using derivation from CoPG paper.\"\"\"\n",
        "\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 antisymetric=False,\n",
        "                 tol=None,\n",
        "                 atol=None,\n",
        "                 n_steps=None,\n",
        "                 device=torch.device('cpu')\n",
        "                 ):\n",
        "        ## TODO(yyshi): change this hyperparameter to program input\n",
        "        # self.policy_lr = 0.001\n",
        "        self.used_ = \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param bregman: dict representing Bregman potential to be used\n",
        "        \"\"\"\n",
        "        super(CMD_RL, self).__init__(player_list,\n",
        "                                     lr_list,\n",
        "                                     antisymetric=antisymetric,\n",
        "                                     tol=tol, atol=atol,\n",
        "                                     n_steps=n_steps,\n",
        "                                     device=device)\n",
        "\n",
        "    def step(self, grad_loss_list, hessian_loss_list, cgd=False):\n",
        "        \"\"\"\n",
        "        CMD algorithm using derivation for gradient and hessian term from CoPG.\n",
        "        \"\"\"\n",
        "        # Compute flattened player list for some small optimization.\n",
        "        player_list = self.state['player_list']\n",
        "        player_list_flattened = [flatten_filter_none(player, player, detach=True, device=self.device)\n",
        "                                 for player in player_list]\n",
        "\n",
        "\n",
        "        \"\"\" Test Method I\"\"\"\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n",
        "            grad_loss_list,\n",
        "            hessian_loss_list,\n",
        "            player_list,\n",
        "            player_list_flattened,\n",
        "            vector_list_flattened=self.state['last_dual_soln'],\n",
        "            tol=self.state['tol'],\n",
        "            atol=self.state['atol'],\n",
        "            n_steps=self.state['n_steps'],\n",
        "            device=self.device\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        print('metamatrix_conjugate_gradient time', end_time-start_time)\n",
        "\n",
        "        # Store state for use in next nash computation..\n",
        "        self.state['step'] += 1\n",
        "        self.state['last_dual_soln'] = nash_list_flattened\n",
        "        self.state['last_dual_soln_n_iter'] = n_iter\n",
        "        self.state['last_dual_residual'] = res\n",
        "\n",
        "        # Edge case to enable self play in CGD case (since we can compute\n",
        "        # element-wise in place operations in CGD).\n",
        "        for player, nash_flattened, lr in zip(self.state['player_list'], nash_list_flattened, self.state['lr_list']):\n",
        "            idx = 0\n",
        "            for p in player:\n",
        "                ## TODO(shi): double-check this line, to see whether adding or substracting gradient\n",
        "                #p.data += lr*nash_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                p.data -= lr*nash_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                idx += p.numel()\n",
        "        return\n",
        "\n",
        "\n",
        "        # Map dual solution back into primal space.\n",
        "        # mapped_list_flattened = exp_map(player_list_flattened,\n",
        "        #                                 nash_list_flattened,\n",
        "        #                                 bregman=self.bregman)\n",
        "\n",
        "        # # Update parameters in place to update players as optimizer.\n",
        "        # for player, mapped_flattened in zip(self.state['player_list'], mapped_list_flattened):\n",
        "        #     idx = 0\n",
        "        #     for p in player:\n",
        "        #         p.data = mapped_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "        #         idx += p.numel()"
      ],
      "metadata": {
        "id": "FS_FMqEZAjya"
      },
      "execution_count": 798,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiCoPG(TrainingWrapper):\n",
        "    # TODO(yyshi): Horizon, gamma tuning?\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-6,\n",
        "            atol=1e-6,\n",
        "            antisymetric=False,\n",
        "            n_steps = 100,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            policy_lr=0.002,\n",
        "            self_play=False,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "        super(MultiCoPG, self).__init__(\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=batch_size,\n",
        "            critic_lr=critic_lr,\n",
        "            tol=tol,\n",
        "            device=device,\n",
        "            dtype=dtype,\n",
        "            self_play=self_play,\n",
        "            gamma=gamma,\n",
        "            tau=tau,\n",
        "        )\n",
        "\n",
        "        # Optimizers for policies and critics.\n",
        "        self.policy_optim = CMD_RL(\n",
        "            [p.parameters() for p in self.policies],\n",
        "            [policy_lr for _ in self.policies],\n",
        "            antisymetric=antisymetric,\n",
        "            tol=tol,\n",
        "            atol=atol,\n",
        "            n_steps = n_steps,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    def step(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute update step for policies and critics.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            step_start_time = time.time()\n",
        "\n",
        "        # # Use critic function to get advantage.\n",
        "        # values, returns, advantages = [], [], []\n",
        "\n",
        "        # TODO(anonymous): Fix this when making self-play more robust.\n",
        "        critics = self.critics\n",
        "        if self.self_play:\n",
        "            critics = [self.critics[0] for _ in range(len(self.policies))]\n",
        "\n",
        "        num_agent = len(critics)\n",
        "        batch_size = mat_states.shape[1]\n",
        "        trj_len = mat_states.shape[2]\n",
        "        values = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        advantages = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        returns = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "\n",
        "        ''' - TEST BATCHING VALUES AND ADVANTAGES 1/3\n",
        "        unbatched_values = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        unbatched_advantages = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        unbatched_returns = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        '''\n",
        "\n",
        "        for i, q in enumerate(critics):\n",
        "\n",
        "            # run batched critics\n",
        "            batch_unflattened_q = torch.nn.Unflatten(dim=1, unflattened_size=torch.Size([8, 20, 20]))(mat_states[i, :, :, :].reshape(-1, 3200))\n",
        "            batch_unflattened_q = batch_unflattened_q.to(self.device)\n",
        "            evaluated_policies_q = q(batch_unflattened_q)\n",
        "\n",
        "            current_index = 0\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                ## compute value & advantage tensor\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        values[i, j, t] = evaluated_policies_q[current_index].detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = evaluated_policies_q[current_index + 1].detach()\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * values[i, j, t + 1] - values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = evaluated_policies_q[current_index + 1].detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*values[i, j, t+1] - values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] - values[i, j, t])*mat_done[i, j, t]\n",
        "                    current_index += 1\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*returns[i, j, tt+1]\n",
        "\n",
        "            ''' - TEST BATCHING VALUES AND ADVANTAGES 2/3\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                ## compute value & advantage tensor\n",
        "                for t in range(trj_len):\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        unbatched_values[i, j, t] = q(mat_state_unflattened_t0).detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        unbatched_values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        unbatched_advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * unbatched_values[i, j, t + 1] - unbatched_values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        unbatched_values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        unbatched_advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*unbatched_values[i, j, t+1] - unbatched_values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        unbatched_advantages[i, j, t] = (mat_rewards[i, j, t] - unbatched_values[i, j, t])*mat_done[i, j, t]\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        unbatched_returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            unbatched_returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            unbatched_returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            unbatched_returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*unbatched_returns[i, j, tt+1]\n",
        "            '''\n",
        "        \n",
        "        ''' - TEST BATCHING VALUES AND ADVANTAGES 3/3\n",
        "        # This segment is to justify the change of atol for (b) since the values are small\n",
        "        # print(\"RANGES\", len(critics), batch_size, trj_len)\n",
        "        # for i, q in enumerate(critics):\n",
        "        #     for j in range(batch_size):\n",
        "        #         for t in range(trj_len):\n",
        "        #           if not torch.isclose(unbatched_advantages[i, j, t], advantages[i, j, t]):\n",
        "        #             print(\"MISMATCH\", i, j, t, unbatched_advantages[i, j, t], advantages[i, j, t])\n",
        "        print(\"TEST BATCHING VALUES AND ADVANTAGES (a):\", torch.allclose(unbatched_values, values))\n",
        "        print(\"TEST BATCHING VALUES AND ADVANTAGES (b):\", torch.allclose(unbatched_advantages, advantages, atol=1e-4))\n",
        "        print(\"TEST BATCHING VALUES AND ADVANTAGES (c):\", torch.allclose(unbatched_returns, returns))\n",
        "        '''\n",
        "\n",
        "        if self.self_play:\n",
        "            # TODO(anonymous): Currently only supports all symmetric players.\n",
        "            cat_states = torch.cat([mat_states[i] for i in range(len(mat_states))])\n",
        "            cat_states_mask = torch.cat([mat_action_mask[i] for i in range(len(mat_action_mask))])\n",
        "            cat_returns = torch.cat([returns[i] for i in range(len(returns))])\n",
        "\n",
        "            #critic_update(cat_states, cat_returns, self.critics[0], self.critic_optim[0])\n",
        "            critic_update(cat_states, cat_states_mask, cat_returns, self.critics[0],\n",
        "                          self.critic_optim[0], batch_size, trj_len, unflatten_state, self.device)\n",
        "        else:\n",
        "            for i, q in enumerate(self.critics):\n",
        "                critic_update(mat_states[i], mat_action_mask[i], returns[i], q,\n",
        "                              self.critic_optim[i], batch_size, trj_len, unflatten_state, self.device)\n",
        "\n",
        "        ## This part compute the simultaneous gradient descent\n",
        "        gradient_losses = []\n",
        "        log_probs = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        gamma_tensor = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "\n",
        "        ''' - TEST BATCHING GRADIENT LOSSES 1/3\n",
        "        unbatched_gradient_losses = []\n",
        "        unbatched_log_probs = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        unbatched_gamma_tensor = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        '''\n",
        "        \n",
        "        ## The following part might be the main cause that the code is slow\n",
        "        for i, p in enumerate(self.policies):\n",
        "\n",
        "            # run batched policies\n",
        "            batch_unflattened = torch.nn.Unflatten(dim=1, unflattened_size=torch.Size([8, 20, 20]))(mat_states[i, :, :].reshape(-1, 3200))\n",
        "            batch_unflattened = batch_unflattened.to(self.device)\n",
        "            evaluated_policies = p(batch_unflattened)\n",
        "            flattened_actions = mat_actions[i, :, :].reshape(-1, 1)\n",
        "            flattened_actions = flattened_actions.to(self.device)\n",
        "            batch_log_probs = evaluated_policies.log_prob(flattened_actions)\n",
        "\n",
        "            current_index = 0\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    log_pi = batch_log_probs[current_index, current_index]\n",
        "                    current_index += 1\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    log_probs[i, j, t] = log_pi*mat_action_mask[i, j, t]\n",
        "                    gamma_tensor[i, j, t] = factor\n",
        "\n",
        "                    if(j==0 and t==0):\n",
        "                        policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            gradient_losses.append(-policyloss/batch_size)\n",
        "\n",
        "            ''' - TEST BATCHING GRADIENT LOSSES 2/3\n",
        "            # Our training wrapper assumes that the policy returns a distribution.\n",
        "            # desired: -1, state_dim (4*20*20)\n",
        "            # mat_states: number_trj, trj_len, state_dim\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    ## potential code optimization: break the for-loop when done\n",
        "                    mat_state_cur_unflattened = unflatten_state(mat_states[i, j, t].reshape(1, -1))\n",
        "                    mat_state_cur_unflattened = mat_state_cur_unflattened.to(self.device)\n",
        "                    # instead of calling the network multiple times; \n",
        "                    log_pi = p(mat_state_cur_unflattened).log_prob(mat_actions[i, j, t].to(self.device))\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    # store the log probality of action given state in tensor; set log_prob = 0 if action_mask = 0\n",
        "                    unbatched_log_probs[i, j, t] = log_pi*mat_action_mask[i, j, t]\n",
        "                    unbatched_gamma_tensor[i, j, t] = factor\n",
        "\n",
        "                    if(j==0 and t==0):\n",
        "                        unbatched_policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        unbatched_policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            unbatched_gradient_losses.append(-unbatched_policyloss/batch_size)\n",
        "            '''\n",
        "        \n",
        "        ''' - TEST BATCHING GRADIENT LOSSES 3/3\n",
        "        print(\"TEST BATCHING GRADIENT LOSSES (a):\", torch.allclose(torch.tensor(unbatched_gradient_losses), torch.tensor(gradient_losses)))\n",
        "        print(\"TEST BATCHING GRADIENT LOSSES (b):\", torch.allclose(unbatched_log_probs, log_probs))\n",
        "        print(\"TEST BATCHING GRADIENT LOSSES (c):\", torch.allclose(unbatched_gamma_tensor, gamma_tensor))\n",
        "        '''\n",
        "\n",
        "        cumsum_log_probs = torch.cumsum(log_probs, dim=2)\n",
        "        #cumsum_log_probs = torch.zeros_like(log_probs, device=self.device)\n",
        "        cumsum_log_probs[:,:,1:]=cumsum_log_probs[:,:,0:trj_len-1]\n",
        "        cumsum_log_probs[:,:,0]=0\n",
        "\n",
        "        # Compute Hessian objectives.\n",
        "        hessian_losses = torch.zeros((num_agent, num_agent))\n",
        "        for i in range(num_agent):\n",
        "            for j in range(num_agent):\n",
        "                if (i != j):\n",
        "                    term0 = (gamma_tensor[i]*log_probs[i] * log_probs[j] * advantages[i]).sum()/batch_size\n",
        "                    term1 = (gamma_tensor[i]*cumsum_log_probs[i] * log_probs[j] * advantages[i]).sum()/batch_size\n",
        "                    term2 = (gamma_tensor[i]*cumsum_log_probs[j] * log_probs[i] * advantages[i]).sum()/batch_size\n",
        "                    hessian_losses[i, j] = -term0-term1-term2\n",
        "\n",
        "\n",
        "        ## The above part might be the main cause that the code is slow\n",
        "\n",
        "        # TODO(yyshi): REVISIT this part - the Hessian matrix is not symmetric; we use (H_o+H_o.T)/2\n",
        "        # hessian_losses = (hessian_losses+hessian_losses.T)/2\n",
        "\n",
        "        # Update the policy parameters.\n",
        "        self.policy_optim.zero_grad()\n",
        "        self.policy_optim.step(gradient_losses, hessian_losses, cgd=True)\n",
        "\n",
        "        gradient_losses.clear()\n",
        "        # hessian_losses.clear()\n",
        "\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            print('MultiCoPG step took:', time.time() - step_start_time)"
      ],
      "metadata": {
        "id": "83TOMehN6EQE"
      },
      "execution_count": 799,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiCoPG Algorithm"
      ],
      "metadata": {
        "id": "Jwi3Fdov7TCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings (CHECK THESE BEFORE RUNNING).\n",
        "device = torch.device('cuda:0')\n",
        "# device = torch.device('cpu') # Uncomment to use CPU.\n",
        "batch_size = 16\n",
        "#policy training iteration\n",
        "n_steps = 1000\n",
        "last_teps = None\n",
        "verbose = True\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "tySN812IGaNQ"
      },
      "execution_count": 800,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a policy and critic; we will use self play and a symmetric critic for this game.\n",
        "p1 = policy().to(device).type(dtype)\n",
        "q = critic().to(device).type(dtype)\n",
        "## copy the policy and critic 4 times\n",
        "policies = [p1 for _ in range(4)]\n",
        "critics = [q for _ in range(4)]"
      ],
      "metadata": {
        "id": "EIr1D4kdGaNS"
      },
      "execution_count": 801,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training environment with env provided.\n",
        "train_wrap = MultiCoPG(\n",
        "    env,\n",
        "    policies,\n",
        "    critics,\n",
        "    batch_size=batch_size,\n",
        "    antisymetric=True,\n",
        "    self_play=False,\n",
        "    policy_lr=0.001,\n",
        "    tol=1e-2,\n",
        "    atol=5e-2,\n",
        "    n_steps = 20,\n",
        "    critic_lr=1e-3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print('device:', device)\n",
        "print('batch_size:', batch_size)\n",
        "print('n_steps:', n_steps)\n",
        "\n",
        "run_id = \"copg_try1_lr1e-3_smallnetwork\"\n",
        "run_location = os.path.join('/content/PCGD/', run_id)\n",
        "if not os.path.exists(run_location):\n",
        "    os.makedirs(run_location)\n",
        "\n",
        "last_teps = None\n",
        "if last_teps is None:\n",
        "    last_teps = 0\n",
        "    writer_tps = []\n",
        "    writer_p1 = []\n",
        "    writer_p2 = []\n",
        "    writer_p3 = []\n",
        "    writer_p4 = []\n",
        "    writer_eplen = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRyQ69dT38SG",
        "outputId": "185ed006-f167-4da5-dfea-40250472b0ee"
      },
      "execution_count": 804,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "batch_size: 16\n",
            "n_steps: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t_eps in range(last_teps, n_steps):\n",
        "    # Sample and compute update.\n",
        "    print('t_eps', t_eps)\n",
        "    states, actions, action_mask, rewards, done, unflatten_state = train_wrap.sample(verbose=verbose)\n",
        "    train_wrap.step(states, actions, action_mask, rewards, done, unflatten_state, verbose=verbose)\n",
        "\n",
        "    if ((t_eps + 1) % 5) == 0:\n",
        "        print(\"logging progress:\", t_eps + 1)\n",
        "\n",
        "        # Calculating discounted average reward for each agent.\n",
        "        disc_avg_reward = []\n",
        "        # pdb.set_trace()\n",
        "        for i in range(4):\n",
        "            total_sum = 0.\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                cumsum = 0.\n",
        "                for t in range(rewards.shape[2]):\n",
        "                    cumsum *= 0.99\n",
        "                    cumsum += rewards[i, j, t].cpu().item()\n",
        "                    if (done[i, j, t] == 0):\n",
        "                        total_sum += cumsum\n",
        "                        break\n",
        "\n",
        "            disc_avg_reward.append(total_sum/batch_size)\n",
        "\n",
        "        print('discounted sum of reward', disc_avg_reward, 'steps', t_eps)\n",
        "        # Log values to Tensorboard.\n",
        "\n",
        "        writer_tps.append(t_eps)\n",
        "        writer_p1.append(disc_avg_reward[0])\n",
        "        writer_p2.append(disc_avg_reward[1])\n",
        "        writer_p3.append(disc_avg_reward[2])\n",
        "        writer_p4.append(disc_avg_reward[3])\n",
        "        writer_eplen.append(rewards.shape[2])\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if ((t_eps + 1) % 20) == 0:\n",
        "        print('saving checkpoint:', t_eps + 1)\n",
        "\n",
        "        actor_path = os.path.join(run_location, 'actor1_' + str(t_eps + 1) + '.pth')\n",
        "        critic_path = os.path.join(run_location, 'critic1_' + str(t_eps + 1) + '.pth')\n",
        "        torch.save(p1.state_dict(), actor_path)\n",
        "        torch.save(q.state_dict(), critic_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "9uvJ-PJZ7f_P",
        "outputId": "43a1f38d-8eb2-4787-a7a1-858958ceced0"
      },
      "execution_count": 805,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_eps 0\n",
            "sample took: 17.767566204071045\n",
            "DEBUG tensor(-0.0735, device='cuda:0')\n",
            "TEST BATCHING VALUES AND ADVANTAGES (a): True\n",
            "TEST BATCHING VALUES AND ADVANTAGES (b): True\n",
            "TEST BATCHING VALUES AND ADVANTAGES (c): True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-805-a70926076547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't_eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_eps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-799-7ae5f5f19518>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# Update the policy parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mgradient_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-798-b2cbf963b6b0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, grad_loss_list, hessian_loss_list, cgd)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mgrad_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mhessian_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-797-d14456122fea>\u001b[0m in \u001b[0;36mmetamatrix_conjugate_gradient\u001b[0;34m(grad_loss_list, hessian_loss_list, player_list, player_list_flattened, vector_list_flattened, mvp, n_steps, tol, atol, device)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_tensors\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Get vector list of negative gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         grad_raw = autograd.grad(loss, param_tensors,\n\u001b[0m\u001b[1;32m    195\u001b[0m                                  \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                                  allow_unused=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(actor_path) \n",
        "files.download(critic_path) "
      ],
      "metadata": {
        "id": "RbXvthRG0zCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p2 = policy().to(device).type(dtype)\n",
        "p2.load_state_dict(torch.load(actor_path))"
      ],
      "metadata": {
        "id": "AtmXnOBV1kfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiSimGD Algorithm"
      ],
      "metadata": {
        "id": "7nqvR7tT5wrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings (CHECK THESE BEFORE RUNNING).\n",
        "device = torch.device('cuda:0')\n",
        "# device = torch.device('cpu') # Uncomment to use CPU.\n",
        "batch_size = 16\n",
        "#policy training iteration\n",
        "n_steps = 2000\n",
        "last_teps = None\n",
        "verbose = False\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "A2T7BN8doYXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a policy and critic; we will use self play and a symmetric critic for this game.\n",
        "p1 = policy().to(device).type(dtype)\n",
        "q = critic().to(device).type(dtype)\n",
        "## copy the policy and critic 4 times\n",
        "policies = [p1 for _ in range(4)]\n",
        "critics = [q for _ in range(4)]"
      ],
      "metadata": {
        "id": "dj7jVRt-o3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training environment with env provided.\n",
        "train_wrap = MultiSimGD(\n",
        "    env,\n",
        "    policies,\n",
        "    critics,\n",
        "    batch_size=batch_size,\n",
        "    self_play=False,\n",
        "    critic_lr=1e-3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print('device:', device)\n",
        "print('batch_size:', batch_size)\n",
        "print('n_steps:', n_steps)"
      ],
      "metadata": {
        "id": "VLdtH_JXplP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if last_teps is None:\n",
        "    last_teps = 0\n",
        "\n",
        "writer_tps = []\n",
        "writer_p1 = []\n",
        "writer_p2 = []\n",
        "writer_p3 = []\n",
        "writer_p4 = []\n",
        "writer_eplen = []\n",
        "for t_eps in range(last_teps, n_steps):\n",
        "    # Sample and compute update.\n",
        "    print('t_eps', t_eps)\n",
        "    states, actions, action_mask, rewards, done, unflatten_state = train_wrap.sample(verbose=verbose)\n",
        "    train_wrap.step(states, actions, action_mask, rewards, done, unflatten_state, verbose=verbose)\n",
        "\n",
        "    if ((t_eps + 1) % 20) == 0:\n",
        "        print(\"logging progress:\", t_eps + 1)\n",
        "\n",
        "        # Calculating discounted average reward for each agent.\n",
        "        disc_avg_reward = []\n",
        "        # pdb.set_trace()\n",
        "        for i in range(4):\n",
        "            total_sum = 0.\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                cumsum = 0.\n",
        "                for t in range(rewards.shape[2]):\n",
        "                    cumsum *= 0.99\n",
        "                    cumsum += rewards[i, j, t].cpu().item()\n",
        "                    if (done[i, j, t] == 0):\n",
        "                        total_sum += cumsum\n",
        "                        break\n",
        "\n",
        "            disc_avg_reward.append(total_sum/batch_size)\n",
        "\n",
        "        print('discounted sum of reward', disc_avg_reward, 'steps', t_eps)\n",
        "        # Log values to Tensorboard.\n",
        "\n",
        "        writer_tps.append(t_eps)\n",
        "        writer_p1.append(disc_avg_reward[0])\n",
        "        writer_p2.append(disc_avg_reward[1])\n",
        "        writer_p3.append(disc_avg_reward[2])\n",
        "        writer_p4.append(disc_avg_reward[3])\n",
        "        writer_eplen.append(rewards.shape[2])\n",
        "\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FbYdBUQ3o1Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}