{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "9c9iiuEGkGKi",
        "outputId": "62ae29e8-a531-4233-8bc8-c554d161eb06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kc-ml2/marlenv.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8bdVlGgvzxW",
        "outputId": "14142cba-d9e1-43e3-a623-f25e7f410e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marlenv'...\n",
            "remote: Enumerating objects: 770, done.\u001b[K\n",
            "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 770 (delta 156), reused 183 (delta 100), pack-reused 485\u001b[K\n",
            "Receiving objects: 100% (770/770), 4.52 MiB | 9.95 MiB/s, done.\n",
            "Resolving deltas: 100% (433/433), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/marlenv/"
      ],
      "metadata": {
        "id": "1fYAHaQfs7en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339b126f-5ea8-4136-e867-fb25207991a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/marlenv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kel1f9RSwJPK",
        "outputId": "aa6b4de0-8af1-46a6-91b6-8cbeb9e365fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/marlenv\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym==0.24.1\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.24.1->marlenv===1.0.0test1) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym==0.24.1->marlenv===1.0.0test1) (3.12.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793153 sha256=29a40250599272daaa0057a9ba8948be609e3bcb387895bc315a602a27e13ee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/e9/0b/5536e77ed2edbbf067ecff287ec039633d40daee4d8dac7716\n",
            "Successfully built gym\n",
            "Installing collected packages: gym, marlenv\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py develop for marlenv\n",
            "Successfully installed gym-0.24.1 marlenv-1.0.0test1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import marlenv\n",
        "import marlenv.envs\n",
        "from google.colab import files\n",
        "\n",
        "custom_reward_func = {\n",
        "    'fruit': 1.0,\n",
        "    'kill': 20.0,\n",
        "    'lose': -1.0,\n",
        "    'time': 0.0,\n",
        "    'win': 1.0\n",
        "}\n",
        "env = gym.make(\n",
        "    'Snake-v1',\n",
        "    # height=20,       # Height of the grid map\n",
        "    # width=20,        # Width of the grid map\n",
        "    # num_snakes=4,    # Number of snakes to spawn on grid\n",
        "    # snake_length=3,  # Initial length of the snake at spawn time\n",
        "    # vision_range=10,  # Vision range (both width height), map returned if None\n",
        "    # frame_stack=1,   # Number of observations to stack on return\n",
        "    # num_envs=2,\n",
        "    height=20,\n",
        "    width=20,\n",
        "    num_snakes=4,\n",
        "    num_fruits = 8,\n",
        "    snake_length=3,\n",
        "    vision_range=None,\n",
        "    frame_stack=1,\n",
        "    observer='snake',\n",
        "    # full_observation=False,\n",
        "    reward_func=custom_reward_func\n",
        ")\n",
        "\n",
        "# env = gym.make('Snake-v1', reward_func=custom_reward_func)\n"
      ],
      "metadata": {
        "id": "9Gcab50zva4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002a6c56-900d-4a6b-d08b-7b36ff132bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()"
      ],
      "metadata": {
        "id": "pmJUrM2QrHWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782d82ac-8044-4614-d079-6dc1d9993312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. \u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbW9OYr5rm-d",
        "outputId": "5812c9e2-5361-4551-b3c8-018d1dd2b994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 20, 20, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(obs[0, :, :, 0]) #locations of the walls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Fm7efF5DpvE8",
        "outputId": "2ad0dd1f-8cb7-4ed9-9cd5-e93680fab194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5ba48d7070>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(obs[0, :, :, 1]) #locations of the fruits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "0wVZM5uXuFso",
        "outputId": "fc30f165-23a1-42f8-8037-66fa5c0e2b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5ba48608e0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(obs[0, :, :, 2]+obs[0, :, :, 3]+obs[0, :, :, 4]) #locations of oppenents?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "0TbAU-fluNao",
        "outputId": "e2941161-ec3b-458a-c04b-f1312200a248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5ba437b190>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(obs[0, :, :, 5]+obs[0, :, :, 6]+obs[0, :, :, 7]) #locations of the snake itself？"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Vo2wVBwLwSsZ",
        "outputId": "dc76b574-cae5-41fc-cb55-77ef601188e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5ba4349070>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD4CAYAAADl7fPiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANhUlEQVR4nO3df+xddX3H8edrLT+yDgfIqPyaEteQMDM705SZsQWGIhBidTGuZNm6jaTMSDKTLQvbEjHuH5eFkWwQSHUNuCjgflSbWIGmW4IkihRSfimMjmDoV6TTOhB1YPG9P76ny/fz7f229Z57v/d+vzwfyTf3nM/nc8/5nNzk1XPOPb3vVBWSdMjPTHoCkqaLoSCpYShIahgKkhqGgqTGyklPYJDjc0KdyKpJT0Natv6XH/BqvZJBfVMZCieyigtyyaSnIS1bD9SuBfu8fJDU6BUKSS5L8lSSvUmuG9B/QpK7uv4Hkrylz/4kjd/QoZBkBXAzcDlwPnBVkvPnDbsa+F5V/RJwI/A3w+5P0uLoc6awHthbVc9U1avAncCGeWM2ALd3y/8CXJJk4M0NSdOhTyicBTw3Z31f1zZwTFUdBF4E3jhoY0k2J9mdZPePeaXHtCT1MTU3GqtqS1Wtq6p1x3HCpKcjvW71CYUZ4Jw562d3bQPHJFkJ/Dzw3R77lDRmfULhQWBNknOTHA9sBLbPG7Md2NQtfwD49/L/aktTbeiHl6rqYJJrgXuAFcDWqnoiyceB3VW1HfhH4J+S7AUOMBsckqZYpvEf7jfk1PKJRml8HqhdvFQHBn4TODU3GiVNB0NBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUqNPhahzkvxHkq8neSLJnwwYc1GSF5Ps6f4+2m+6ksatT9Xpg8CfVtXDSU4CHkqys6q+Pm/cl6vqyh77kbSIhj5TqKrnq+rhbvn7wDc4vEKUpCVmJPcUumrSvwo8MKD7nUkeSfKlJL98hG1YNk6aAn0uHwBI8nPAvwIfqaqX5nU/DLy5ql5OcgXweWDNoO1U1RZgC8z+xHvfeUkaTq8zhSTHMRsIn6mqf5vfX1UvVdXL3fIO4Lgkp/XZp6Tx6vPtQ5itAPWNqvq7Bca86VDp+STru/1ZS1KaYn0uH34d+D3gsSR7ura/BH4RoKpuZbZ+5IeSHAR+BGy0lqQ03frUkrwfGFh2as6Ym4Cbht2HpMXnE42SGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpEbvUEjybJLHurJwuwf0J8nfJ9mb5NEk7+i7T0nj07vuQ+fiqvrOAn2XM1vrYQ1wAXBL9yppCi3G5cMG4NM166vAyUnOWIT9ShrCKEKhgHuTPJRk84D+s4Dn5qzvY0DNScvGSdNhFJcPF1bVTJLTgZ1Jnqyq+37ajVg2TpoOvc8Uqmqme90PbAPWzxsyA5wzZ/3srk3SFOpbS3JVkpMOLQOXAo/PG7Yd+P3uW4hfA16squf77FfS+PS9fFgNbOvKRa4EPltVdyf5Y/j/0nE7gCuAvcAPgT/suU9JY9QrFKrqGeDtA9pvnbNcwIf77EfS4vGJRkkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSY+hQSHJeVyru0N9LST4yb8xFSV6cM+aj/acsaZyG/o3GqnoKWAuQZAWzP9u+bcDQL1fVlcPuR9LiGtXlwyXAf1XVN0e0PUkTMqoCsxuBOxboe2eSR4BvAX9WVU8MGtSVnNsMcCI/O6Jpab57vrVnLNt9z5lrx7JdLb5RlKI/Hngv8M8Duh8G3lxVbwf+Afj8Qtupqi1Vta6q1h3HCX2nJWlIo7h8uBx4uKpemN9RVS9V1cvd8g7guCSnjWCfksZkFKFwFQtcOiR5U7ryUUnWd/v77gj2KWlMet1T6OpHvhu4Zk7b3JJxHwA+lOQg8CNgY1cxStKU6ls27gfAG+e1zS0ZdxNwU599SFpcPtEoqWEoSGoYCpIahoKkhqEgqTGqx5y1RPw0jyOP65FoTTfPFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDR9z1oL8hebXJ88UJDWOKRSSbE2yP8njc9pOTbIzydPd6ykLvHdTN+bpJJtGNXFJ43GsZwq3AZfNa7sO2FVVa4Bd3XojyanA9cAFwHrg+oXCQ9J0OKZQqKr7gAPzmjcAt3fLtwPvG/DW9wA7q+pAVX0P2Mnh4SJpivS5p7C6qp7vlr8NrB4w5izguTnr+7o2SVNqJDcau1oOveo5JNmcZHeS3T/mlVFMS9IQ+oTCC0nOAOhe9w8YMwOcM2f97K7tMNaSlKZDn1DYDhz6NmET8IUBY+4BLk1ySneD8dKuTdKUOtavJO8AvgKcl2RfkquBTwDvTvI08K5unSTrknwKoKoOAH8NPNj9fbxrkzSlMo2lHd+QU+uCXDLpaUjL1gO1i5fqQAb1+USjpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqHDUUFqgj+bdJnkzyaJJtSU5e4L3PJnksyZ4ku0c5cUnjcSxnCrdxeKm3ncDbqupXgP8E/uII77+4qtZW1brhpihpMR01FAbVkayqe6vqYLf6VWaLvEhaBkZxT+GPgC8t0FfAvUkeSrL5SBuxbJw0HVb2eXOSvwIOAp9ZYMiFVTWT5HRgZ5InuzOPw1TVFmALzNZ96DMvScMb+kwhyR8AVwK/WwtUlKmqme51P7ANWD/s/iQtjqFCIcllwJ8D762qHy4wZlWSkw4tM1tH8vFBYyVNj2P5SnJQHcmbgJOYvSTYk+TWbuyZSXZ0b10N3J/kEeBrwBer6u6xHIWkkbGWpPQ6ZC1JScfMUJDUMBQkNQwFSQ1DQVLDUJDUMBQkNQwFSQ1DQVLDUJDUMBQkNQwFSQ1DQVLDUJDUMBQkNQwFSQ1DQVLDUJDUGLZs3MeSzHS/z7gnyRULvPeyJE8l2ZvkulFOXNJ4DFs2DuDGrhzc2qraMb8zyQrgZuBy4HzgqiTn95mspPEbqmzcMVoP7K2qZ6rqVeBOYMMQ25G0iPrcU7i2qzq9NckpA/rPAp6bs76vaxvIsnHSdBg2FG4B3gqsBZ4Hbug7karaUlXrqmrdcZzQd3OShjRUKFTVC1X1WlX9BPgkg8vBzQDnzFk/u2uTNMWGLRt3xpzV9zO4HNyDwJok5yY5HtgIbB9mf5IWz1GrTndl4y4CTkuyD7geuCjJWmZLzT8LXNONPRP4VFVdUVUHk1wL3AOsALZW1RNjOQpJI2PZOOl1yLJxko6ZoSCpYShIahgKkhqGgqSGoSCpYShIahgKkhqGgqSGoSCpYShIahgKkhqGgqSGoSCpYShIahgKkhqGgqSGoSCpcSy/0bgVuBLYX1Vv69ruAs7rhpwM/E9VrR3w3meB7wOvAQerat2I5i1pTI4aCsyWjbsJ+PShhqr6nUPLSW4AXjzC+y+uqu8MO0FJi+uooVBV9yV5y6C+JAE+CPzWaKclaVL63lP4DeCFqnp6gf4C7k3yUJLNR9qQZeOk6XAslw9HchVwxxH6L6yqmSSnAzuTPNkVrD1MVW0BtsDsT7z3nJekIQ19ppBkJfDbwF0Ljamqme51P7CNweXlJE2RPpcP7wKerKp9gzqTrEpy0qFl4FIGl5eTNEWOGgpd2bivAOcl2Zfk6q5rI/MuHZKcmWRHt7oauD/JI8DXgC9W1d2jm7qkcbBsnPQ6ZNk4ScfMUJDUMBQkNQwFSQ1DQVLDUJDUMBQkNQwFSQ1DQVLDUJDUMBQkNQwFSQ1DQVLDUJDUMBQkNQwFSY2p/JGVJP8NfHNe82nAcqwfsVyPC5bvsS2H43pzVf3CoI6pDIVBkuxejhWmlutxwfI9tuV6XId4+SCpYShIaiylUNgy6QmMyXI9Lli+x7ZcjwtYQvcUJC2OpXSmIGkRGAqSGksiFJJcluSpJHuTXDfp+YxKkmeTPJZkT5Ldk55PH0m2Jtmf5PE5bacm2Znk6e71lEnOcRgLHNfHksx0n9ueJFdMco6jNvWhkGQFcDNwOXA+cFWS8yc7q5G6uKrWLoPvvW8DLpvXdh2wq6rWALu69aXmNg4/LoAbu89tbVXtGNC/ZE19KDBbqXpvVT1TVa8CdwIbJjwnzVNV9wEH5jVvAG7vlm8H3reokxqBBY5rWVsKoXAW8Nyc9X1d23JQwL1JHkqyedKTGYPVVfV8t/xtZosOLxfXJnm0u7xYcpdFR7IUQmE5u7Cq3sHspdGHk/zmpCc0LjX73fdy+f77FuCtwFrgeeCGyU5ntJZCKMwA58xZP7trW/KqaqZ73Q9sY/ZSaTl5IckZAN3r/gnPZySq6oWqeq2qfgJ8kmX2uS2FUHgQWJPk3CTHAxuB7ROeU29JViU56dAycCnw+JHfteRsBzZ1y5uAL0xwLiNzKOg672eZfW4rJz2Bo6mqg0muBe4BVgBbq+qJCU9rFFYD25LA7Ofw2aq6e7JTGl6SO4CLgNOS7AOuBz4BfC7J1cz+V/gPTm6Gw1nguC5KspbZy6FngWsmNsEx8DFnSY2lcPkgaREZCpIahoKkhqEgqWEoSGoYCpIahoKkxv8BeszE8kXGuzwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class policy(nn.Module):\n",
        "    \"\"\"General policy model for calculating action policy from state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(policy, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Conv2d(8, 4, 2),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Conv2d(4, 4, 2),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Flatten(),\n",
        "                                   nn.Linear(1296, 64),\n",
        "                                   nn.ReLU(),\n",
        "                                  #  nn.Linear(256, 64),\n",
        "                                  #  nn.ReLU(),\n",
        "                                  #  nn.Linear(64, 64),\n",
        "                                  #  nn.ReLU(),\n",
        "                                   nn.Linear(64, 3),\n",
        "                                   nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        mu = self.actor(state)\n",
        "        return Categorical(mu)\n",
        "\n",
        "class critic(nn.Module):\n",
        "    \"\"\"Critic model for estimating value from state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(critic, self).__init__()\n",
        "\n",
        "        self.critic = nn.Sequential(nn.Conv2d(8, 8, 2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(8, 8, 2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Flatten(),\n",
        "                                    nn.Linear(2592, 256),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(256, 64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(64, 64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value"
      ],
      "metadata": {
        "id": "jRjOBpbHomPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import pdb\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import pdb\n",
        "\n",
        "# TODO(yyshi): Casework for this?\n",
        "DEFAULT_DTYPE = torch.float32\n",
        "torch.set_default_dtype(DEFAULT_DTYPE)\n",
        "\n",
        "def zero_grad(params):\n",
        "    \"\"\"Given some list of Tensors, zero and reset gradients.\"\"\"\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            p.grad.detach()\n",
        "            p.grad.zero_()\n",
        "\n",
        "def flatten_filter_none(grad_list, param_list,\n",
        "                        detach=False,\n",
        "                        neg=False,\n",
        "                        device=torch.device('cpu')):\n",
        "    \"\"\"\n",
        "    Given a list of Tensors with possible None values, returns single Tensor\n",
        "    with None removed and flattened.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for grad, param in zip(grad_list, param_list):\n",
        "        if grad is None:\n",
        "            filtered.append(torch.zeros(param.numel(), device=device, requires_grad=True))\n",
        "        else:\n",
        "            filtered.append(grad.contiguous().view(-1))\n",
        "\n",
        "    result = torch.cat(filtered) if not neg else -torch.cat(filtered)\n",
        "\n",
        "    # Use this only if higher order derivatives are not needed.\n",
        "    if detach:\n",
        "        result.detach()\n",
        "\n",
        "    return result\n",
        "\n",
        "# TODO(anonymous): make this user interface cleaner.\n",
        "class SGD(object):\n",
        "    \"\"\"Optimizer class for simultaneous SGD\"\"\"\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 device=torch.device('cpu')\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param lr_list: list of learning rates per player optimizer.\n",
        "        \"\"\"\n",
        "        # Store optimizer state.\n",
        "        player_list = [list(elem) for elem in player_list]\n",
        "        self.state = {'step': 0,\n",
        "                      'player_list': player_list,\n",
        "                      'lr_list': lr_list}\n",
        "        # TODO(anonymous): set this device in CMD algorithm.\n",
        "        self.device = device\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for player in self.state['player_list']:\n",
        "            zero_grad(player)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.state\n",
        "\n",
        "    def step(self, loss_list):\n",
        "        #print('loss function', loss_list)\n",
        "        grad_list = [\n",
        "            autograd.grad(loss, player, retain_graph=True, allow_unused=True)\n",
        "            for loss, player in zip(loss_list, self.state['player_list'])\n",
        "        ]\n",
        "\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[0]))\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[1]))\n",
        "        # print(\"largest gradient:\", max(torch.max(tensor) for tensor in grad_list[2]))\n",
        "\n",
        "\n",
        "\n",
        "        for grad, player, lr in zip(grad_list, self.state['player_list'], self.state['lr_list']):\n",
        "            for player_elem, grad_elem in zip(player, grad):\n",
        "                if grad_elem is not None:\n",
        "                    player_elem.data -= grad_elem * lr\n",
        "\n",
        "def critic_update(mat_states, mat_action_mask, returns, q, optim_q, batch_size, trj_len, unflatten_state, device):\n",
        "\n",
        "    # run batched critics\n",
        "    batch_unflattened_q = torch.nn.Unflatten(dim=1, unflattened_size=torch.Size([8, 20, 20]))(mat_states.reshape(-1, 3200))\n",
        "    batch_unflattened_q = batch_unflattened_q.to(device)\n",
        "    evaluated_policies_q = q(batch_unflattened_q)\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            current_index = trj_len * j + t\n",
        "            if (j==0 and t == 0):\n",
        "                #mat_state_list = mat_state_unflattened_t0\n",
        "                value_pred = evaluated_policies_q[current_index]\n",
        "            else:\n",
        "                if(mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                #mat_state_list = torch.cat((mat_state_list, mat_state_unflattened_t0), dim = 0)\n",
        "                value_pred_current = evaluated_policies_q[current_index]\n",
        "                value_pred = torch.cat((value_pred, value_pred_current), dim=0)\n",
        "\n",
        "    value_pred = value_pred.reshape(-1, 1)\n",
        "\n",
        "    ''' - TEST BATCHING CRITIC UPDATES 1/1\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            mat_state_unflattened_t0 = unflatten_state(mat_states[j, t, :].reshape(1, -1))\n",
        "            mat_state_unflattened_t0 = mat_state_unflattened_t0.to(device)\n",
        "            if (j==0 and t == 0):\n",
        "                #mat_state_list = mat_state_unflattened_t0\n",
        "                unbatched_value_pred = q(mat_state_unflattened_t0)\n",
        "            else:\n",
        "                if(mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                #mat_state_list = torch.cat((mat_state_list, mat_state_unflattened_t0), dim = 0)\n",
        "                unbatched_value_pred_current = q(mat_state_unflattened_t0)\n",
        "                unbatched_value_pred = torch.cat((unbatched_value_pred, unbatched_value_pred_current), dim=0)\n",
        "\n",
        "    print(\"TEST BATCHING CRITIC UPDATES (a):\", torch.allclose(unbatched_value_pred, value_pred))\n",
        "    '''\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        for t in range(trj_len):\n",
        "            return_current = returns[j, t].reshape(1, -1)\n",
        "            return_current = return_current.to(device)\n",
        "            if (j == 0 and t == 0):\n",
        "                return_list = return_current\n",
        "            else:\n",
        "                if (mat_action_mask[j, t] == 0):\n",
        "                    break\n",
        "                return_list = torch.cat((return_list, return_current), dim=0)\n",
        "\n",
        "    #value_pred = q(c)\n",
        "    critic_loss = (return_list - value_pred).pow(2).mean()\n",
        "    optim_q.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    optim_q.step()\n",
        "    critic_loss = critic_loss.detach().cpu()\n",
        "\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    return critic_loss\n",
        "\n",
        "def get_advantage(\n",
        "        next_value, reward_mat, value_mat, mat_done,\n",
        "        gamma=0.99, tau=0.95, device=torch.device('cpu')\n",
        "):\n",
        "    insert_tensor = torch.tensor([[float(next_value)]], device=device)\n",
        "    value_mat = torch.cat([value_mat, insert_tensor])\n",
        "    gae = 0\n",
        "    returns = []\n",
        "\n",
        "    for i in reversed(range(len(reward_mat))):\n",
        "        delta = reward_mat[i] + gamma * value_mat[i + 1] * mat_done[i] - value_mat[i]\n",
        "        gae = delta + gamma * tau * mat_done[i] * gae\n",
        "        returns.append(gae + value_mat[i])\n",
        "\n",
        "    # Reverse ordering.\n",
        "    returns.reverse()\n",
        "\n",
        "    vals = torch.cat(returns).reshape(-1, 1)\n",
        "    return vals\n",
        "\n",
        "# def critic_update(state_mat, return_mat, q, optim_q):\n",
        "#     val_loc = q(state_mat)\n",
        "\n",
        "#     critic_loss = (return_mat - val_loc).pow(2).mean()\n",
        "\n",
        "#     optim_q.zero_grad()\n",
        "#     critic_loss.backward()\n",
        "#     optim_q.step()\n",
        "\n",
        "#     critic_loss = critic_loss.detach().cpu()\n",
        "\n",
        "#     return critic_loss\n",
        "\n",
        "# TODO(anonymous): Revisit this?\n",
        "# def get_advantage(\n",
        "#         next_value, reward_mat, value_mat, masks,\n",
        "#         gamma=0.99, tau=0.95, device=torch.device('cpu')\n",
        "# ):\n",
        "#     insert_tensor = torch.tensor([[float(next_value)]], device=device)\n",
        "#     value_mat = torch.cat([value_mat, insert_tensor])\n",
        "#     gae = 0\n",
        "#     returns = []\n",
        "\n",
        "#     for i in reversed(range(len(reward_mat))):\n",
        "#         delta = reward_mat[i] + gamma * value_mat[i + 1] * masks[i] - value_mat[i]\n",
        "#         gae = delta + gamma * tau * masks[i] * gae\n",
        "#         returns.append(gae + value_mat[i])\n",
        "\n",
        "#     # Reverse ordering.\n",
        "#     returns.reverse()\n",
        "\n",
        "#     vals = torch.cat(returns).reshape(-1, 1)\n",
        "#     return vals"
      ],
      "metadata": {
        "id": "KkBlB1lWpDzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingWrapper:\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-3,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            self_play=False,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "            critic_optim_kwargs={},\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store game environment.\n",
        "        self.env = env\n",
        "        # Training parameters.\n",
        "        self.policies = policies\n",
        "        self.critics = critics\n",
        "\n",
        "        # Sampling parameters.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Device to be used.\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # TODO(anonymous): Implement self play in a cleaner way.\n",
        "        self.self_play = self_play\n",
        "        if self.self_play:\n",
        "            assert (len(self.critics) == 1)\n",
        "            self.critic_optim = [\n",
        "                torch.optim.Adam(self.critics[0].parameters(), lr=critic_lr)\n",
        "            ]\n",
        "        else:\n",
        "            self.critic_optim = [\n",
        "                torch.optim.Adam(c.parameters(), lr=critic_lr, **critic_optim_kwargs) for c in self.critics\n",
        "            ]\n",
        "\n",
        "            # GAE estimation work.\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "    ## start from here on 2023/01/05 - change the data storage type to tensor\n",
        "    def sample(self, verbose=True):\n",
        "        \"\"\"\n",
        "        :param verbose: Print debugging information if requested.\n",
        "        Sample observations actions and states. Returns trajectory observations\n",
        "        actions rewards in single list format (which seperates trajectories\n",
        "        using done mask).\n",
        "        \"\"\"\n",
        "        # We collect trajectories all into one list (using a done mask) for simplicity.\n",
        "        num_agents = len(self.policies)\n",
        "        max_traj_length = 1000\n",
        "        mat_all_states_t = []\n",
        "        mat_all_actions_t = []\n",
        "        mat_all_action_mask_t = []\n",
        "        mat_all_rewards_t = []\n",
        "        mat_all_done_t = []\n",
        "\n",
        "        # If verbose, show how long sampling takes.\n",
        "        if verbose:\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "        for j in range(self.batch_size):\n",
        "            # Reset environment for each trajectory in batch.\n",
        "            mat_states_t = []\n",
        "            mat_actions_t = []\n",
        "            mat_action_mask_t = []\n",
        "            mat_rewards_t = []\n",
        "            mat_done_t = []\n",
        "\n",
        "            obs = self.env.reset()\n",
        "            dones = [False for _ in range(num_agents)]\n",
        "\n",
        "            for t in range(max_traj_length):\n",
        "                # Record state...\n",
        "                mat_states_t.append(obs)\n",
        "                # action_mask = 1 if actively taking action; otherwise, action_mask = 0\n",
        "                mat_action_mask_t.append([1. - int(elem) for elem in dones])\n",
        "\n",
        "                # Since env is usually on CPU, send observation to GPU,\n",
        "                # sample, then collect back to CPU.\n",
        "                actions = []\n",
        "                for i in range(num_agents):\n",
        "                    policy = self.policies[i]\n",
        "                    obs_gpu = torch.tensor([obs[i]], device=self.device, dtype=self.dtype).permute(0,3,1,2)\n",
        "                    dist = policy(obs_gpu)\n",
        "\n",
        "                    action = dist.sample().cpu().numpy()\n",
        "\n",
        "                    # TODO(anonymous): Pytorch doesn't handle 0-dim tensors (a.k.a scalars well)\n",
        "                    if action.ndim == 1 and action.size == 1:\n",
        "                        action = action[0]\n",
        "                    else:\n",
        "                        # action = np.squeeze(dist.sample().cpu().numpy(), axis=1)\n",
        "                        action = np.squeeze(action, axis=1)\n",
        "\n",
        "                    actions.append(action)\n",
        "\n",
        "                # Advance environment one step forwards.\n",
        "                obs, rewards, dones, _ = self.env.step(actions)\n",
        "\n",
        "                # Record actions, rewards, and inverse done mask.\n",
        "                mat_actions_t.append(actions)\n",
        "\n",
        "                # 2023/01/05 YYSHI TODO(Comment out the random probablity for breaking out)\n",
        "                # if(t > 10):\n",
        "                #     p = torch.rand(1)\n",
        "                #     if(p>0.96):\n",
        "                #         # find the alive snake\n",
        "                #         index = [i for i, x in enumerate(dones) if not (x)]\n",
        "                #         for j in index:\n",
        "                #             rewards[j] += t*0.3\n",
        "                #             #print('Snake', j, 'Alive, when game finished!!', dones[j], p)\n",
        "                #             dones[j] = True\n",
        "\n",
        "                mat_rewards_t.append(rewards)\n",
        "                mat_done_t.append([1. - int(elem) for elem in dones])\n",
        "\n",
        "                # Break once all players are done.\n",
        "                if all(dones):\n",
        "                    break\n",
        "\n",
        "                # elif sum(dones) == 3: #only 1 snake alive\n",
        "                #     # find the alive snake\n",
        "                #     index = [i for i, x in enumerate(dones) if not (x)]\n",
        "                #     rewards[index] += 20\n",
        "                #     dones[index] = True\n",
        "                #     print('Snake', index, 'Alive!!')\n",
        "                #     break\n",
        "                # else:\n",
        "                #     pass\n",
        "\n",
        "            mat_all_states_t.append(mat_states_t)\n",
        "            mat_all_actions_t.append(mat_actions_t)\n",
        "            mat_all_action_mask_t.append(mat_action_mask_t)\n",
        "            mat_all_rewards_t.append(mat_rewards_t)\n",
        "            mat_all_done_t.append(mat_done_t)\n",
        "\n",
        "        # Create data on GPU for later update step\n",
        "        # TODO(yyshi): list agent, each element of the list contains # of trajectories,\n",
        "        # TODO(yyshi): and each trajectories contain # of samples\n",
        "        # (agent, num_traj, traj_length)\n",
        "        # have trajectory with splits same to the player with the longest trajectory.\n",
        "        trj_len = 0\n",
        "        for j in range(self.batch_size):\n",
        "            trj_len = np.maximum(trj_len, len(mat_all_states_t[j]))\n",
        "\n",
        "        state_dim = int(obs.size/obs.shape[0])\n",
        "\n",
        "        state_shape = obs_gpu.shape[1:]\n",
        "        mat_states = torch.zeros((num_agents, self.batch_size, trj_len, state_dim))\n",
        "        mat_actions = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_action_mask = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_rewards = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "        mat_done = torch.zeros((num_agents, self.batch_size, trj_len))\n",
        "\n",
        "        for j in range(self.batch_size):\n",
        "            current_trj_len = len(mat_all_states_t[j])\n",
        "            for i in range(current_trj_len):\n",
        "                for k in range(num_agents):\n",
        "                    mat_states[k, j, i] = torch.flatten(torch.tensor(mat_all_states_t[j][i][k], dtype=self.dtype, device=self.device).permute(2,0,1))\n",
        "                    mat_actions[k, j, i] = torch.tensor(mat_all_actions_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    mat_rewards[k, j, i] = torch.tensor(mat_all_rewards_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    # if trajectory is not active, mask = 0;\n",
        "                    mat_action_mask[k, j, i] = torch.tensor(mat_all_action_mask_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "                    mat_done[k, j, i] = torch.tensor(mat_all_done_t[j][i][k], dtype=self.dtype, device=self.device)\n",
        "\n",
        "        unflatten_state = torch.nn.Unflatten(1, state_shape)\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            print('sample took:', time.time() - batch_start_time)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # for t in range(trj_len):\n",
        "        #     state_unflatten = unflatten(mat_states[0, 0, t, :].reshape(1, -1))\n",
        "        #     plt.imshow(state_unflatten[0, 0, :, :] + 2*state_unflatten[0, 1, :, :] + 3*state_unflatten[0, 2, :, :] + 4*state_unflatten[0, 3, :, :])\n",
        "        #     plt.savefig(\"Instant{}.png\".format(t))\n",
        "\n",
        "        # mat_states, [num_agent, num_traj, traj_len, obs_dim] observations of all agents\n",
        "        # mat_actions, [num_agent, num_traj, traj_len] actions of all agents\n",
        "        # mat_action_mask, [num_agent, num_traj, traj_len] mat_action_mask[i, j, t] = 0 if agent i in trajectory j has died at time t-1 or earlier\n",
        "        # mat_rewards, [num_agent, num_traj, traj_len], rewards of all agents\n",
        "        # mat_done, [num_agent, num_traj, traj_len], mat_done[i, j, t] = 0 if agent i in trajectory j has died at time t or earlier\n",
        "        return mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state"
      ],
      "metadata": {
        "id": "DHymE_QL3NgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(yyshi): Refine this class structure based on feedback.\n",
        "class MultiSimGD(TrainingWrapper):\n",
        "    # TODO(yyshi): Horizon, gamma tuning?\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-3,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            self_play=False,\n",
        "            policy_lr=0.002,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "            critc_optim_kwargs={}\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "        super(MultiSimGD, self).__init__(\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=batch_size,\n",
        "            critic_lr=critic_lr,\n",
        "            tol=tol,\n",
        "            device=device,\n",
        "            dtype=dtype,\n",
        "            self_play=self_play,\n",
        "            gamma=gamma,\n",
        "            tau=tau,\n",
        "            critic_optim_kwargs=critc_optim_kwargs\n",
        "        )\n",
        "\n",
        "        # Optimizers for policies and critics.\n",
        "        self.policy_optim = SGD(\n",
        "            [p.parameters() for p in self.policies],\n",
        "            [policy_lr for _ in self.policies],\n",
        "            device=device\n",
        "        )\n",
        "    \n",
        "    def step(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute update step for policies and critics.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            step_start_time = time.time()\n",
        "\n",
        "        # TODO(anonymous): Fix this when making self-play more robust.\n",
        "        critics = self.critics\n",
        "        if self.self_play:\n",
        "            critics = [self.critics[0] for _ in range(len(self.policies))]\n",
        "\n",
        "        # Use critic function to get advantage.\n",
        "        # values, returns, advantages = [], [], []\n",
        "\n",
        "        # Compute generalized advantage estimation (GAE).\n",
        "        num_agent = len(critics)\n",
        "        batch_size = mat_states.shape[1]\n",
        "        trj_len = mat_states.shape[2]\n",
        "\n",
        "        values = torch.zeros((num_agent, batch_size, trj_len))\n",
        "        advantages = torch.zeros((num_agent, batch_size, trj_len))\n",
        "        returns = torch.zeros((num_agent, batch_size, trj_len))\n",
        "\n",
        "        for i, q in enumerate(critics):\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                ## compute value & advantage tensor\n",
        "                for t in range(trj_len):\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        values[i, j, t] = q(mat_state_unflattened_t0).detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * values[i, j, t + 1] - values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*values[i, j, t+1] - values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] - values[i, j, t])*mat_done[i, j, t]\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*returns[i, j, tt+1]\n",
        "\n",
        "        # Use sampled values to fit critic model.\n",
        "        if self.self_play:\n",
        "            cat_states = torch.cat([mat_states[i] for i in range(len(mat_states))])\n",
        "            cat_states_mask = torch.cat([mat_action_mask[i] for i in range(len(mat_action_mask))])\n",
        "            cat_returns = torch.cat([returns[i] for i in range(len(returns))])\n",
        "\n",
        "            #critic_update(cat_states, cat_returns, self.critics[0], self.critic_optim[0])\n",
        "            critic_update(cat_states, cat_states_mask, cat_returns, self.critics[0],\n",
        "                          self.critic_optim[0], batch_size, trj_len, unflatten_state, self.device)\n",
        "        else:\n",
        "            for i, q in enumerate(self.critics):\n",
        "                critic_update(mat_states[i], mat_action_mask[i], returns[i], q,\n",
        "                              self.critic_optim[i], batch_size, trj_len, unflatten_state, self.device)\n",
        "\n",
        "        # Include last index to compute pairs.\n",
        "        # traj_indices.append(mat_done[0].size(0))\n",
        "\n",
        "        log_probs = []\n",
        "        gradient_losses = []\n",
        "        for i, p in enumerate(self.policies):\n",
        "            # Our training wrapper assumes that the policy returns a distribution.\n",
        "            # desired: -1, state_dim (4*20*20)\n",
        "            # mat_states: number_trj, trj_len, state_dim\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    ## potential code optimization: break the for-loop when done\n",
        "                    mat_state_cur_unflattened = unflatten_state(mat_states[i, j, t].reshape(1, -1))\n",
        "                    mat_state_cur_unflattened = mat_state_cur_unflattened.to(self.device)\n",
        "                    log_pi = p(mat_state_cur_unflattened).log_prob(mat_actions[i, j, t].to(self.device))\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    if(j==0 and t==0):\n",
        "                        policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            gradient_losses.append(-policyloss/batch_size)\n",
        "\n",
        "\n",
        "        # Update the policy parameters.\n",
        "        self.policy_optim.zero_grad()\n",
        "        self.policy_optim.step(gradient_losses)\n",
        "        gradient_losses.clear()\n",
        "\n",
        "\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            print('MultiSimGD step took:', time.time() - step_start_time)"
      ],
      "metadata": {
        "id": "ffAzsb2b14Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Dx(x_dual, alpha = 1):\n",
        "        return x_dual / alpha\n",
        "\n",
        "def Dx_inv(x_dual, alpha = 1):\n",
        "    return x_dual * alpha\n",
        "\n",
        "def Dxx_vp(x_primal, vec, alpha = 1):\n",
        "    # Does not need to be in-place.\n",
        "    return vec * alpha\n",
        "\n",
        "def Dxx_inv_vp(x_primal, vec, alpha = 1):\n",
        "    return vec / alpha\n",
        "\n",
        "def antivp(\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened,\n",
        "        transpose=False,\n",
        "        device=torch.device('cpu'),\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param hessian_loss_list: list of objective functions for hessian computation\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: list of flattened vectors for each player\n",
        "    :param bregman: dictionary representing bregman potential to use\n",
        "    :param transpose: compute product against transpose if set\n",
        "\n",
        "    Computes right product of antisymmetric metamatrix with a vector of player vectors.\n",
        "    \"\"\"\n",
        "    # TODO(yyshi): add error handling and assertions\n",
        "    # assert(len(hessian_loss_list) == len(player_list))\n",
        "    # assert(len(hessian_loss_list) == len(vector_list))\n",
        "    prod_list = [torch.zeros_like(v, device=device) for v in vector_list_flattened]\n",
        "\n",
        "    for i, row_params in enumerate(player_list):\n",
        "        for j, (col_params, vector_elem) in enumerate(zip(player_list, vector_list_flattened)):\n",
        "            if i == j:\n",
        "                # Diagonal element is the Bregman term.\n",
        "                prod_list[i] += Dxx_vp(player_list_flattened[i], vector_elem)\n",
        "                continue\n",
        "\n",
        "            # Otherwise, we construct our Hessian vector products. Variable\n",
        "            # retain_graph must be set to true, or we cant compute multiple\n",
        "            # subsequent Hessians any more.\n",
        "            left_loss, right_loss = hessian_loss_list[i], hessian_loss_list[j]\n",
        "            \n",
        "            if transpose:\n",
        "                left_loss, right_loss = right_loss, left_loss\n",
        "\n",
        "            # Anti-symmetric decomposition (1/2)(A - A^T)...\n",
        "            left_grad_raw = autograd.grad(left_loss, col_params,\n",
        "                                          create_graph=True,\n",
        "                                          retain_graph=True,\n",
        "                                          allow_unused=True)\n",
        "            left_grad_flattened = flatten_filter_none(left_grad_raw, col_params,\n",
        "                                                      device=device)\n",
        "\n",
        "            left_hvp_raw = autograd.grad(left_grad_flattened, row_params,\n",
        "                                         grad_outputs=vector_elem,\n",
        "                                         create_graph=False,\n",
        "                                         retain_graph=True,\n",
        "                                         allow_unused=True)\n",
        "            left_hvp_flattened = flatten_filter_none(left_hvp_raw, row_params,\n",
        "                                                     device=device)\n",
        "\n",
        "            right_grad_raw = autograd.grad(right_loss, col_params,\n",
        "                                           create_graph=True,\n",
        "                                           retain_graph=True,\n",
        "                                           allow_unused=True)\n",
        "            right_grad_flattened = flatten_filter_none(right_grad_raw, col_params,\n",
        "                                                       device=device)\n",
        "\n",
        "            right_hvp_raw = autograd.grad(right_grad_flattened, row_params,\n",
        "                                          grad_outputs=vector_elem,\n",
        "                                          create_graph=False,\n",
        "                                          retain_graph=True,\n",
        "                                          allow_unused=True)\n",
        "            right_hvp_flattened = flatten_filter_none(right_hvp_raw, row_params,\n",
        "                                                      device=device)\n",
        "\n",
        "            prod_list[i] += 0.5 * (left_hvp_flattened - right_hvp_flattened)\n",
        "\n",
        "    # Detach to get memory back.\n",
        "    prod_list = [elem.detach() for elem in prod_list]\n",
        "\n",
        "    return prod_list\n",
        "\n",
        "def avp(\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened,\n",
        "        transpose=False,\n",
        "        device=torch.device('cpu'),\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param hessian_loss_list: list of objective functions for hessian computation\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: list of flattened vectors for each player\n",
        "    :param bregman: dictionary representing bregman potential to use\n",
        "    :param transpose: compute product against transpose if set\n",
        "\n",
        "    Computes right product of metamatrix with a vector of player vectors.\n",
        "    \"\"\"\n",
        "    # TODO(anonymous): add error handling and assertions\n",
        "    # assert(len(hessian_loss_list) == len(player_list))\n",
        "    # assert(len(hessian_loss_list) == len(vector_list))\n",
        "    # TODO(yyshi): add eta to the function input\n",
        "    \n",
        "    eta = 0.8\n",
        "    \n",
        "    prod_list = [torch.zeros_like(v, device=device) for v in vector_list_flattened]\n",
        "\n",
        "    for i, row_params in enumerate(player_list):\n",
        "        for j, (col_params, vector_elem) in enumerate(zip(player_list, vector_list_flattened)):\n",
        "            if i == j:\n",
        "                # Diagonal element is the Bregman term, i.e., I * vector_elem\n",
        "                prod_list[i] += Dxx_vp(player_list_flattened[i], vector_elem, alpha = 1)\n",
        "                continue\n",
        "\n",
        "            # Otherwise, we construct our Hessian vector products. Variable\n",
        "            # retain_graph must be set to true, or we cant compute multiple\n",
        "            # subsequent Hessians any more.\n",
        "\n",
        "            loss = hessian_loss_list[i, j] if not transpose else hessian_loss_list[j, i]\n",
        "\n",
        "            grad_raw = autograd.grad(loss, col_params,\n",
        "                                     create_graph=True,\n",
        "                                     retain_graph=True,\n",
        "                                     allow_unused=True)\n",
        "\n",
        "            grad_flattened = flatten_filter_none(grad_raw, col_params,\n",
        "                                                 device=device)\n",
        "\n",
        "            # Don't need any higher order derivatives, so create_graph = False.\n",
        "            hvp_raw = autograd.grad(grad_flattened, row_params,\n",
        "                                    grad_outputs=vector_elem,\n",
        "                                    create_graph=False,\n",
        "                                    retain_graph=True,\n",
        "                                    allow_unused=True)\n",
        "            \n",
        "            # hvp_flattened = hvp_raw[0]\n",
        "            hvp_flattened = flatten_filter_none(hvp_raw, row_params,\n",
        "                                                device=device)\n",
        "\n",
        "            prod_list[i] += eta*hvp_flattened\n",
        "\n",
        "    # Detach to get memory back.\n",
        "    prod_list = [elem.detach() for elem in prod_list]\n",
        "    \n",
        "\n",
        "    return prod_list\n",
        "\n",
        "def metamatrix_conjugate_gradient(\n",
        "        grad_loss_list,\n",
        "        hessian_loss_list,\n",
        "        player_list,\n",
        "        player_list_flattened,\n",
        "        vector_list_flattened=None,\n",
        "        mvp=avp,\n",
        "        n_steps=20,\n",
        "        tol=1e-3,\n",
        "        atol=1e-3,\n",
        "        device=torch.device('cpu')\n",
        "):\n",
        "    \"\"\"\n",
        "    :param grad_loss_list: list of loss tensors for each player to compute gradient\n",
        "    :param hessian_loss_list: list of loss tensors for each player to compute hessian\n",
        "    :param player_list: list of list of params for each player to compute gradients from\n",
        "    :param player_list_flattened: list of flattened player tensors (without gradients)\n",
        "    :param vector_list_flattened: initial guess for update solution\n",
        "    :param bregman: dict representing a Bregman potential to be used\n",
        "    :param n_steps: number of iteration steps for conjugate gradient\n",
        "    :param tol: relative residual tolerance threshold from initial vector guess\n",
        "    :param atol: absolute residual tolerance threshold\n",
        "\n",
        "    Compute solution to meta-matrix game form using preconditioned conjugate\n",
        "    gradient method. Since the metamatrix A is not p.s.d, we multiply both sides\n",
        "    by the transpose to ensure p.s.d.\n",
        "\n",
        "    In other words, note that solving Ax = b (where A is meta matrix, x is\n",
        "    vector of update vectors and b is learning rate times vector of gradients\n",
        "    is the same as solving A'x = b' (where A' = (A^T)A and b' = (A^T)b.\n",
        "    \"\"\"\n",
        "\n",
        "    b = []\n",
        "    for loss, param_tensors in zip(grad_loss_list, player_list):\n",
        "        # Get vector list of negative gradients.\n",
        "        grad_raw = autograd.grad(loss, param_tensors,\n",
        "                                 retain_graph=True,\n",
        "                                 allow_unused=True)\n",
        "        \n",
        "        grad_flattened = flatten_filter_none(grad_raw, param_tensors,\n",
        "                                             neg=True, detach=True, device=device)\n",
        "        \n",
        "        b.append(grad_flattened.detach())\n",
        "        # b.append(grad_raw[0].detach())\n",
        "\n",
        "    # Multiplying both sides by transpose to ensure p.s.d.\n",
        "    # r = A^t * b (before we subtract)\n",
        "    r = mvp(hessian_loss_list, player_list, player_list_flattened, b, \n",
        "            transpose=True, device=device)\n",
        "\n",
        "    # Set relative residual threshold based on norm of b.\n",
        "    # norm_At_b = sum(r_elem*r_elem for r_elem in r)\n",
        "    \n",
        "    \n",
        "    norm_At_b = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "    residual_tol = tol * norm_At_b\n",
        "    print('residual_tolerance', residual_tol)\n",
        "\n",
        "    # If no guess provided, start from zero vector.\n",
        "    if vector_list_flattened is None:\n",
        "        vector_list_flattened = [torch.zeros_like(p, device=device)\n",
        "                                 for p in player_list_flattened]\n",
        "    else:\n",
        "        # Compute initial residual if a guess is given.\n",
        "        A_x = mvp(hessian_loss_list, player_list, player_list_flattened, vector_list_flattened,\n",
        "                  transpose=False, device=device)\n",
        "        At_A_x = mvp(hessian_loss_list, player_list, player_list_flattened, A_x,\n",
        "                     transpose=True, device=device)\n",
        "\n",
        "        # torch._foreach_sub_(r, At_A_x)\n",
        "        for r_elem, At_A_x_elem in zip(r, At_A_x):\n",
        "            r_elem -= At_A_x_elem\n",
        "\n",
        "    ## up until here, we have computed r_0 = A^tb - A^t A x0\n",
        "    ## Use preconditioner if available...\n",
        "    # z = r\n",
        "    # if 'Dxx_inv_vp' in bregman:\n",
        "    #     z = [bregman['Dxx_inv_vp'](params, r_elems)\n",
        "    #          for params, r_elems in zip(player_list_flattened, r)]\n",
        "\n",
        "    # Early exit if solution already found.\n",
        "    # rdotr = sum(r_elem*r_elem for r_elem in r)\n",
        "    rdotr = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "\n",
        "    # rdotz = sum(torch.dot(r_elem, z_elem) for r_elem, z_elem in zip(r, z))\n",
        "    if rdotr < residual_tol or rdotr < atol:\n",
        "        return vector_list_flattened, 0, rdotr\n",
        "\n",
        "    # Define p and measure current candidate vector.\n",
        "    p = [r_elem.clone().detach() for r_elem in r]\n",
        "\n",
        "    # Use conjugate gradient to find vector solution.\n",
        "    for i in range(n_steps):\n",
        "        print('conjugate number of step,', i, 'rdotr', rdotr)\n",
        "        step_3 = time.time()\n",
        "        A_p = mvp(hessian_loss_list, player_list, player_list_flattened, p,\n",
        "                  transpose=False, device=device)\n",
        "        At_A_p = mvp(hessian_loss_list, player_list, player_list_flattened, A_p,\n",
        "                     transpose=True, device=device)\n",
        "        #print('One conjugate gradient step took:', time.time() - step_3)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            alpha = torch.div(rdotr, sum(torch.dot(e1, e2) for e1, e2 in zip(p, At_A_p)))\n",
        "            #alpha = torch.div(rdotr, sum(e1*e2 for e1, e2 in zip(p, At_A_p)))\n",
        "\n",
        "            # Update candidate solution and residual, where:\n",
        "            # (1) x_new = x + alpha * p\n",
        "            # (2) r_new = r - alpha * A^t A * p\n",
        "\n",
        "            # torch._foreach_add_(vector_list_flattened, p, alpha=alpha)\n",
        "            # torch._foreach_sub_(r, At_A_p, alpha=alpha)\n",
        "\n",
        "            for vlf_elem, p_elem in zip(vector_list_flattened, p):\n",
        "                vlf_elem += p_elem * alpha\n",
        "            for r_elem, At_A_P_elem in zip(r, At_A_p):\n",
        "                r_elem -= At_A_P_elem * alpha\n",
        "\n",
        "            # Calculate new residual metric\n",
        "            #new_rdotr = sum(r_elem*r_elem for r_elem in r)\n",
        "            new_rdotr = sum(torch.dot(r_elem, r_elem) for r_elem in r)\n",
        "\n",
        "            # Break if solution is within threshold\n",
        "            if new_rdotr < atol or new_rdotr < residual_tol:\n",
        "                break\n",
        "\n",
        "            # If preconditioner provided, use it...\n",
        "            # z = r\n",
        "            # if 'Dxx_inv_vp' in bregman:\n",
        "            #     z = [bregman['Dxx_inv_vp'](params, r_elems)\n",
        "            #          for params, r_elems in zip(player_list_flattened, r)]\n",
        "            # new_rdotz = sum(torch.dot(r_elem, z_elem) for r_elem, z_elem in zip(r, z))\n",
        "\n",
        "            # Otherwise, update and continue.\n",
        "            # (3) p_new = r_new + beta * p\n",
        "\n",
        "            # torch._foreach_add(z, p, alpha=beta)\n",
        "            beta = torch.div(new_rdotr, rdotr)\n",
        "            p = [r_elem + p_elem * beta for r_elem, p_elem in zip(r, p)]\n",
        "\n",
        "            rdotr = new_rdotr\n",
        "            # rdotz = new_rdotz\n",
        "\n",
        "    return vector_list_flattened, i + 1, rdotr\n"
      ],
      "metadata": {
        "id": "MzYSB21T56sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CMD(object):\n",
        "    \"\"\"Optimizer class for the CMD algorithm with differentiable player objectives.\"\"\"\n",
        "\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 antisymetric=False,\n",
        "                 tol=None,\n",
        "                 atol=None,\n",
        "                 n_steps=None,\n",
        "                 device=torch.device('cpu')\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param bregman: dict representing Bregman potential to be used\n",
        "        \"\"\"\n",
        "\n",
        "        # In case, parameter generators are provided.\n",
        "        player_list = [list(elem) for elem in player_list]\n",
        "\n",
        "        # Conjugate gradient will provably converge in number of params steps.\n",
        "        if n_steps is None:\n",
        "            n_steps = sum([sum([elem.numel() for elem in param_list])\n",
        "                           for param_list in player_list])\n",
        "\n",
        "        # Store optimizer state.\n",
        "        self.state = {'step': 0,\n",
        "                      'player_list': player_list,\n",
        "                      'lr_list': lr_list,\n",
        "                      'tol': tol,\n",
        "                      'atol': atol,\n",
        "                      'n_steps': n_steps,\n",
        "                      'last_dual_soln': None,\n",
        "                      'last_dual_soln_n_iter': 0,\n",
        "                      'last_dual_residual': 0.,\n",
        "                      'antisymetric': antisymetric}\n",
        "        # TODO(yyshi): set this device in CMD algorithm.\n",
        "        self.device = device\n",
        "        self.mvp = avp if not antisymetric else antivp\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for player in self.state['player_list']:\n",
        "            zero_grad(player)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.state\n",
        "\n",
        "    def player_list(self):\n",
        "        return self.state['player_list']\n",
        "\n",
        "    def step(self, loss_list):\n",
        "        # Compute flattened player list for some small optimization.\n",
        "        player_list = self.state['player_list']\n",
        "        player_list_flattened = [flatten_filter_none(player, player,\n",
        "                                                     detach=True, device=self.device)\n",
        "                                 for player in player_list]\n",
        "\n",
        "        # Compute dual solution first, before mapping back to primal.\n",
        "        # Use dual solution as initial guess for numerical speed.\n",
        "        nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n",
        "            loss_list,\n",
        "            loss_list,\n",
        "            player_list,\n",
        "            player_list_flattened,\n",
        "            vector_list_flattened=self.state['last_dual_soln'],\n",
        "            tol=self.state['tol'],\n",
        "            atol=self.state['atol'],\n",
        "            n_steps=self.state['n_steps'],\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Store state for use in next nash computation..\n",
        "        self.state['step'] += 1\n",
        "        self.state['last_dual_soln'] = nash_list_flattened\n",
        "        self.state['last_dual_soln_n_iter'] = n_iter\n",
        "        self.state['last_dual_residual'] = res\n",
        "\n",
        "        # Map dual solution back into primal space.\n",
        "        # mapped_list_flattened = exp_map(player_list_flattened,\n",
        "        #                                 nash_list_flattened,\n",
        "        #                                 bregman=self.bregman)\n",
        "\n",
        "        # Update parameters in place to update players as optimizer.\n",
        "        for player, mapped_flattened in zip(self.state['player_list'], mapped_list_flattened):\n",
        "            idx = 0\n",
        "            for p in player:\n",
        "                p.data = mapped_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                idx += p.numel()\n",
        "\n",
        "\n",
        "# TODO(anonymous): May need to fix this optimizer for self-play, specifically update\n",
        "# method, since we need to update data and not just replace it. If this were\n",
        "# self play, only one of the updates calculated from nash would carry through.\n",
        "class CMD_RL(CMD):\n",
        "    \"\"\"RL optimizer using CMD algorithm, using derivation from CoPG paper.\"\"\"\n",
        "\n",
        "    def __init__(self, player_list,\n",
        "                 lr_list=None,\n",
        "                 antisymetric=False,\n",
        "                 tol=None,\n",
        "                 atol=None,\n",
        "                 n_steps=None,\n",
        "                 device=torch.device('cpu')\n",
        "                 ):\n",
        "        ## TODO(yyshi): change this hyperparameter to program input\n",
        "        # self.policy_lr = 0.001\n",
        "        self.used_ = \"\"\"\n",
        "        :param player_list: list (per player) of list of Tensors, representing parameters\n",
        "        :param bregman: dict representing Bregman potential to be used\n",
        "        \"\"\"\n",
        "        super(CMD_RL, self).__init__(player_list,\n",
        "                                     lr_list,\n",
        "                                     antisymetric=antisymetric,\n",
        "                                     tol=tol, atol=atol,\n",
        "                                     n_steps=n_steps,\n",
        "                                     device=device)\n",
        "\n",
        "    def step(self, grad_loss_list, hessian_loss_list, cgd=False):\n",
        "        \"\"\"\n",
        "        CMD algorithm using derivation for gradient and hessian term from CoPG.\n",
        "        \"\"\"\n",
        "        # Compute flattened player list for some small optimization.\n",
        "        player_list = self.state['player_list']\n",
        "        player_list_flattened = [flatten_filter_none(player, player, detach=True, device=self.device)\n",
        "                                 for player in player_list]\n",
        "\n",
        "\n",
        "        \"\"\" Test Method I\"\"\"\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        nash_list_flattened, n_iter, res = metamatrix_conjugate_gradient(\n",
        "            grad_loss_list,\n",
        "            hessian_loss_list,\n",
        "            player_list,\n",
        "            player_list_flattened,\n",
        "            vector_list_flattened=self.state['last_dual_soln'],\n",
        "            tol=self.state['tol'],\n",
        "            atol=self.state['atol'],\n",
        "            n_steps=self.state['n_steps'],\n",
        "            device=self.device\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        print('metamatrix_conjugate_gradient time', end_time-start_time)\n",
        "\n",
        "        # Store state for use in next nash computation..\n",
        "        self.state['step'] += 1\n",
        "        self.state['last_dual_soln'] = nash_list_flattened\n",
        "        self.state['last_dual_soln_n_iter'] = n_iter\n",
        "        self.state['last_dual_residual'] = res\n",
        "\n",
        "        # Edge case to enable self play in CGD case (since we can compute\n",
        "        # element-wise in place operations in CGD).\n",
        "        for player, nash_flattened, lr in zip(self.state['player_list'], nash_list_flattened, self.state['lr_list']):\n",
        "            idx = 0\n",
        "            for p in player:\n",
        "                ## TODO(shi): double-check this line, to see whether adding or substracting gradient\n",
        "                #p.data += lr*nash_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                p.data -= lr*nash_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "                idx += p.numel()\n",
        "        return\n",
        "\n",
        "\n",
        "        # Map dual solution back into primal space.\n",
        "        # mapped_list_flattened = exp_map(player_list_flattened,\n",
        "        #                                 nash_list_flattened,\n",
        "        #                                 bregman=self.bregman)\n",
        "\n",
        "        # # Update parameters in place to update players as optimizer.\n",
        "        # for player, mapped_flattened in zip(self.state['player_list'], mapped_list_flattened):\n",
        "        #     idx = 0\n",
        "        #     for p in player:\n",
        "        #         p.data = mapped_flattened[idx: idx + p.numel()].reshape(p.shape)\n",
        "        #         idx += p.numel()"
      ],
      "metadata": {
        "id": "FS_FMqEZAjya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiCoPG(TrainingWrapper):\n",
        "    # TODO(yyshi): Horizon, gamma tuning?\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=32,\n",
        "            critic_lr=1e-3,\n",
        "            tol=1e-6,\n",
        "            atol=1e-6,\n",
        "            antisymetric=False,\n",
        "            n_steps = 100,\n",
        "            device=torch.device('cpu'),\n",
        "            dtype=DEFAULT_DTYPE,\n",
        "            policy_lr=0.002,\n",
        "            self_play=False,\n",
        "            gamma=0.99,\n",
        "            tau=0.95,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: OpenAI gym instance to train on\n",
        "        :param policies: List of policies in same order as gym observations.\n",
        "        :param batch_size: Number of trajectories to collect for each training step.\n",
        "        :param potential: Bregman potential to use for optimizer.\n",
        "        :param device: Device to compute on.\n",
        "        Initialize training wrapper with any gym and policies.\n",
        "        \"\"\"\n",
        "        super(MultiCoPG, self).__init__(\n",
        "            env,\n",
        "            policies,\n",
        "            critics,\n",
        "            batch_size=batch_size,\n",
        "            critic_lr=critic_lr,\n",
        "            tol=tol,\n",
        "            device=device,\n",
        "            dtype=dtype,\n",
        "            self_play=self_play,\n",
        "            gamma=gamma,\n",
        "            tau=tau,\n",
        "        )\n",
        "\n",
        "        # Optimizers for policies and critics.\n",
        "        self.policy_optim = CMD_RL(\n",
        "            [p.parameters() for p in self.policies],\n",
        "            [policy_lr for _ in self.policies],\n",
        "            antisymetric=antisymetric,\n",
        "            tol=tol,\n",
        "            atol=atol,\n",
        "            n_steps = n_steps,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    def step(self, mat_states, mat_actions, mat_action_mask, mat_rewards, mat_done, unflatten_state, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute update step for policies and critics.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            step_start_time = time.time()\n",
        "\n",
        "        # # Use critic function to get advantage.\n",
        "        # values, returns, advantages = [], [], []\n",
        "\n",
        "        # TODO(anonymous): Fix this when making self-play more robust.\n",
        "        critics = self.critics\n",
        "        if self.self_play:\n",
        "            critics = [self.critics[0] for _ in range(len(self.policies))]\n",
        "\n",
        "        num_agent = len(critics)\n",
        "        batch_size = mat_states.shape[1]\n",
        "        trj_len = mat_states.shape[2]\n",
        "        values = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        advantages = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        returns = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "\n",
        "        ''' - TEST BATCHING VALUES AND ADVANTAGES 1/3\n",
        "        unbatched_values = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        unbatched_advantages = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        unbatched_returns = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        '''\n",
        "\n",
        "        for i, q in enumerate(critics):\n",
        "\n",
        "            # run batched critics\n",
        "            batch_unflattened_q = torch.nn.Unflatten(dim=1, unflattened_size=torch.Size([8, 20, 20]))(mat_states[i, :, :, :].reshape(-1, 3200))\n",
        "            batch_unflattened_q = batch_unflattened_q.to(self.device)\n",
        "            evaluated_policies_q = q(batch_unflattened_q)\n",
        "\n",
        "            current_index = 0\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                ## compute value & advantage tensor\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        values[i, j, t] = evaluated_policies_q[current_index].detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = evaluated_policies_q[current_index + 1].detach()\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * values[i, j, t + 1] - values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        values[i, j, t+1] = evaluated_policies_q[current_index + 1].detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*values[i, j, t+1] - values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        advantages[i, j, t] = (mat_rewards[i, j, t] - values[i, j, t])*mat_done[i, j, t]\n",
        "                    current_index += 1\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*returns[i, j, tt+1]\n",
        "\n",
        "            ''' - TEST BATCHING VALUES AND ADVANTAGES 2/3\n",
        "            # val is V(s; θ_i)\n",
        "            for j in range(batch_size):\n",
        "                ## compute value & advantage tensor\n",
        "                for t in range(trj_len):\n",
        "                    if(t==0):\n",
        "                        #value function at s_0\n",
        "                        mat_state_unflattened_t0 = unflatten_state(mat_states[i, j, t, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t0 = mat_state_unflattened_t0.to(self.device)\n",
        "                        unbatched_values[i, j, t] = q(mat_state_unflattened_t0).detach()\n",
        "                        #value function at s_1\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        unbatched_values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        unbatched_advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma * unbatched_values[i, j, t + 1] - unbatched_values[i, j, t]) * mat_action_mask[i, j, t]\n",
        "                    elif(t<trj_len-1):\n",
        "                        mat_state_unflattened_t1 = unflatten_state(mat_states[i, j, t+1, :].reshape(1, -1))\n",
        "                        mat_state_unflattened_t1 = mat_state_unflattened_t1.to(self.device)\n",
        "                        unbatched_values[i, j, t+1] = q(mat_state_unflattened_t1).detach()\n",
        "                        # we use (45) in https://arxiv.org/pdf/2006.10611.pdf\n",
        "                        unbatched_advantages[i, j, t] = (mat_rewards[i, j, t] + self.gamma*unbatched_values[i, j, t+1] - unbatched_values[i, j, t])*mat_action_mask[i, j, t]\n",
        "                    else:\n",
        "                        unbatched_advantages[i, j, t] = (mat_rewards[i, j, t] - unbatched_values[i, j, t])*mat_done[i, j, t]\n",
        "\n",
        "                ## compute return\n",
        "                for tt in reversed(range(trj_len)):\n",
        "                    if mat_action_mask[i, j, tt] == 0:\n",
        "                        unbatched_returns[i, j, tt] = 0\n",
        "                    else:\n",
        "                        if tt == trj_len-1:\n",
        "                            unbatched_returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        elif mat_action_mask[i, j, tt+1] == 0:\n",
        "                            unbatched_returns[i, j, tt] = mat_rewards[i, j, tt]\n",
        "                        else:\n",
        "                            unbatched_returns[i, j, tt] = mat_rewards[i, j, tt] + self.gamma*unbatched_returns[i, j, tt+1]\n",
        "            '''\n",
        "        \n",
        "        ''' - TEST BATCHING VALUES AND ADVANTAGES 3/3\n",
        "        # This segment is to justify the change of atol for (b) since the values are small\n",
        "        # print(\"RANGES\", len(critics), batch_size, trj_len)\n",
        "        # for i, q in enumerate(critics):\n",
        "        #     for j in range(batch_size):\n",
        "        #         for t in range(trj_len):\n",
        "        #           if not torch.isclose(unbatched_advantages[i, j, t], advantages[i, j, t]):\n",
        "        #             print(\"MISMATCH\", i, j, t, unbatched_advantages[i, j, t], advantages[i, j, t])\n",
        "        print(\"TEST BATCHING VALUES AND ADVANTAGES (a):\", torch.allclose(unbatched_values, values))\n",
        "        print(\"TEST BATCHING VALUES AND ADVANTAGES (b):\", torch.allclose(unbatched_advantages, advantages, atol=1e-4))\n",
        "        print(\"TEST BATCHING VALUES AND ADVANTAGES (c):\", torch.allclose(unbatched_returns, returns))\n",
        "        '''\n",
        "\n",
        "        if self.self_play:\n",
        "            # TODO(anonymous): Currently only supports all symmetric players.\n",
        "            cat_states = torch.cat([mat_states[i] for i in range(len(mat_states))])\n",
        "            cat_states_mask = torch.cat([mat_action_mask[i] for i in range(len(mat_action_mask))])\n",
        "            cat_returns = torch.cat([returns[i] for i in range(len(returns))])\n",
        "\n",
        "            #critic_update(cat_states, cat_returns, self.critics[0], self.critic_optim[0])\n",
        "            critic_update(cat_states, cat_states_mask, cat_returns, self.critics[0],\n",
        "                          self.critic_optim[0], batch_size, trj_len, unflatten_state, self.device)\n",
        "        else:\n",
        "            for i, q in enumerate(self.critics):\n",
        "                critic_update(mat_states[i], mat_action_mask[i], returns[i], q,\n",
        "                              self.critic_optim[i], batch_size, trj_len, unflatten_state, self.device)\n",
        "\n",
        "        ## This part compute the simultaneous gradient descent\n",
        "        gradient_losses = []\n",
        "        log_probs = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        gamma_tensor = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "\n",
        "        ''' - TEST BATCHING GRADIENT LOSSES 1/3\n",
        "        unbatched_gradient_losses = []\n",
        "        unbatched_log_probs = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        unbatched_gamma_tensor = torch.zeros((num_agent, batch_size, trj_len), device=self.device)\n",
        "        '''\n",
        "        \n",
        "        ## The following part might be the main cause that the code is slow\n",
        "        for i, p in enumerate(self.policies):\n",
        "\n",
        "            # run batched policies\n",
        "            batch_unflattened = torch.nn.Unflatten(dim=1, unflattened_size=torch.Size([8, 20, 20]))(mat_states[i, :, :].reshape(-1, 3200))\n",
        "            batch_unflattened = batch_unflattened.to(self.device)\n",
        "            evaluated_policies = p(batch_unflattened)\n",
        "            flattened_actions = mat_actions[i, :, :].reshape(-1, 1)\n",
        "            flattened_actions = flattened_actions.to(self.device)\n",
        "            batch_log_probs = evaluated_policies.log_prob(flattened_actions)\n",
        "\n",
        "            diag_log_probs = torch.diag(batch_log_probs).reshape(batch_size, trj_len)\n",
        "            factors = torch.vander(torch.tensor(self.gamma) * torch.ones(batch_size), trj_len, increasing=True).to(self.device)\n",
        "\n",
        "            log_probs[i, :, :] = diag_log_probs * mat_action_mask[i, :, :].to(self.device)\n",
        "            gamma_tensor[i, :, :] = factors\n",
        "            policyloss = torch.sum(diag_log_probs * factors * advantages[i, :, :].to(self.device))\n",
        "            gradient_losses.append(-policyloss/batch_size)\n",
        "\n",
        "            ''' - TEST BATCHING GRADIENT LOSSES 2/3\n",
        "            # Our training wrapper assumes that the policy returns a distribution.\n",
        "            # desired: -1, state_dim (4*20*20)\n",
        "            # mat_states: number_trj, trj_len, state_dim\n",
        "            for j in range(batch_size):\n",
        "                for t in range(trj_len):\n",
        "                    ## potential code optimization: break the for-loop when done\n",
        "                    mat_state_cur_unflattened = unflatten_state(mat_states[i, j, t].reshape(1, -1))\n",
        "                    mat_state_cur_unflattened = mat_state_cur_unflattened.to(self.device)\n",
        "                    # instead of calling the network multiple times; \n",
        "                    log_pi = p(mat_state_cur_unflattened).log_prob(mat_actions[i, j, t].to(self.device))\n",
        "                    factor = torch.pow(torch.tensor(self.gamma), t)\n",
        "                    # store the log probality of action given state in tensor; set log_prob = 0 if action_mask = 0\n",
        "                    unbatched_log_probs[i, j, t] = log_pi*mat_action_mask[i, j, t]\n",
        "                    unbatched_gamma_tensor[i, j, t] = factor\n",
        "\n",
        "                    if(j==0 and t==0):\n",
        "                        unbatched_policyloss = log_pi * factor * advantages[i, j, t]\n",
        "                    else:\n",
        "                        unbatched_policyloss += log_pi * factor * advantages[i, j, t]\n",
        "\n",
        "            unbatched_gradient_losses.append(-unbatched_policyloss/batch_size)\n",
        "            '''\n",
        "        \n",
        "        ''' - TEST BATCHING GRADIENT LOSSES 3/3\n",
        "        print(unbatched_gradient_losses, gradient_losses)\n",
        "        print(\"TEST BATCHING GRADIENT LOSSES (a):\", torch.allclose(torch.tensor(unbatched_gradient_losses), torch.tensor(gradient_losses)))\n",
        "        print(\"TEST BATCHING GRADIENT LOSSES (b):\", torch.allclose(unbatched_log_probs, log_probs))\n",
        "        print(\"TEST BATCHING GRADIENT LOSSES (c):\", torch.allclose(unbatched_gamma_tensor, gamma_tensor))\n",
        "        '''\n",
        "\n",
        "        cumsum_log_probs = torch.cumsum(log_probs, dim=2)\n",
        "        #cumsum_log_probs = torch.zeros_like(log_probs, device=self.device)\n",
        "        cumsum_log_probs[:,:,1:]=cumsum_log_probs[:,:,0:trj_len-1]\n",
        "        cumsum_log_probs[:,:,0]=0\n",
        "\n",
        "        # Compute Hessian objectives.\n",
        "        hessian_losses = torch.zeros((num_agent, num_agent))\n",
        "        for i in range(num_agent):\n",
        "            for j in range(num_agent):\n",
        "                if (i != j):\n",
        "                    term0 = (gamma_tensor[i]*log_probs[i] * log_probs[j] * advantages[i]).sum()/batch_size\n",
        "                    term1 = (gamma_tensor[i]*cumsum_log_probs[i] * log_probs[j] * advantages[i]).sum()/batch_size\n",
        "                    term2 = (gamma_tensor[i]*cumsum_log_probs[j] * log_probs[i] * advantages[i]).sum()/batch_size\n",
        "                    hessian_losses[i, j] = -term0-term1-term2\n",
        "\n",
        "\n",
        "        ## The above part might be the main cause that the code is slow\n",
        "\n",
        "        # TODO(yyshi): REVISIT this part - the Hessian matrix is not symmetric; we use (H_o+H_o.T)/2\n",
        "        # hessian_losses = (hessian_losses+hessian_losses.T)/2\n",
        "\n",
        "        # Update the policy parameters.\n",
        "        self.policy_optim.zero_grad()\n",
        "        self.policy_optim.step(gradient_losses, hessian_losses, cgd=True)\n",
        "\n",
        "        gradient_losses.clear()\n",
        "        # hessian_losses.clear()\n",
        "\n",
        "        # Print sampling time for debugging purposes.\n",
        "        if verbose:\n",
        "            torch.cuda.synchronize()\n",
        "            print('MultiCoPG step took:', time.time() - step_start_time)"
      ],
      "metadata": {
        "id": "83TOMehN6EQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiCoPG Algorithm"
      ],
      "metadata": {
        "id": "Jwi3Fdov7TCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings (CHECK THESE BEFORE RUNNING).\n",
        "device = torch.device('cuda:0')\n",
        "# device = torch.device('cpu') # Uncomment to use CPU.\n",
        "batch_size = 16\n",
        "#policy training iteration\n",
        "n_steps = 1000\n",
        "last_teps = None\n",
        "verbose = True\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "tySN812IGaNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a policy and critic; we will use self play and a symmetric critic for this game.\n",
        "p1 = policy().to(device).type(dtype)\n",
        "q = critic().to(device).type(dtype)\n",
        "## copy the policy and critic 4 times\n",
        "policies = [p1 for _ in range(4)]\n",
        "critics = [q for _ in range(4)]"
      ],
      "metadata": {
        "id": "EIr1D4kdGaNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training environment with env provided.\n",
        "train_wrap = MultiCoPG(\n",
        "    env,\n",
        "    policies,\n",
        "    critics,\n",
        "    batch_size=batch_size,\n",
        "    antisymetric=True,\n",
        "    self_play=False,\n",
        "    policy_lr=0.001,\n",
        "    tol=1e-2,\n",
        "    atol=5e-2,\n",
        "    n_steps = 20,\n",
        "    critic_lr=1e-3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print('device:', device)\n",
        "print('batch_size:', batch_size)\n",
        "print('n_steps:', n_steps)\n",
        "\n",
        "run_id = \"copg_try1_lr1e-3_smallnetwork\"\n",
        "run_location = os.path.join('/content/PCGD/', run_id)\n",
        "if not os.path.exists(run_location):\n",
        "    os.makedirs(run_location)\n",
        "\n",
        "last_teps = None\n",
        "if last_teps is None:\n",
        "    last_teps = 0\n",
        "    writer_tps = []\n",
        "    writer_p1 = []\n",
        "    writer_p2 = []\n",
        "    writer_p3 = []\n",
        "    writer_p4 = []\n",
        "    writer_eplen = []"
      ],
      "metadata": {
        "id": "dRyQ69dT38SG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fbcf0b-5f0d-4494-f2c0-512f56bd215e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "batch_size: 16\n",
            "n_steps: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t_eps in range(last_teps, n_steps):\n",
        "    # Sample and compute update.\n",
        "    print('t_eps', t_eps)\n",
        "    states, actions, action_mask, rewards, done, unflatten_state = train_wrap.sample(verbose=verbose)\n",
        "    train_wrap.step(states, actions, action_mask, rewards, done, unflatten_state, verbose=verbose)\n",
        "\n",
        "    if ((t_eps + 1) % 5) == 0:\n",
        "        print(\"logging progress:\", t_eps + 1)\n",
        "\n",
        "        # Calculating discounted average reward for each agent.\n",
        "        disc_avg_reward = []\n",
        "        # pdb.set_trace()\n",
        "        for i in range(4):\n",
        "            total_sum = 0.\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                cumsum = 0.\n",
        "                for t in range(rewards.shape[2]):\n",
        "                    cumsum *= 0.99\n",
        "                    cumsum += rewards[i, j, t].cpu().item()\n",
        "                    if (done[i, j, t] == 0):\n",
        "                        total_sum += cumsum\n",
        "                        break\n",
        "\n",
        "            disc_avg_reward.append(total_sum/batch_size)\n",
        "\n",
        "        print('discounted sum of reward', disc_avg_reward, 'steps', t_eps)\n",
        "        # Log values to Tensorboard.\n",
        "\n",
        "        writer_tps.append(t_eps)\n",
        "        writer_p1.append(disc_avg_reward[0])\n",
        "        writer_p2.append(disc_avg_reward[1])\n",
        "        writer_p3.append(disc_avg_reward[2])\n",
        "        writer_p4.append(disc_avg_reward[3])\n",
        "        writer_eplen.append(rewards.shape[2])\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if ((t_eps + 1) % 20) == 0:\n",
        "        print('saving checkpoint:', t_eps + 1)\n",
        "\n",
        "        actor_path = os.path.join(run_location, 'actor1_' + str(t_eps + 1) + '.pth')\n",
        "        critic_path = os.path.join(run_location, 'critic1_' + str(t_eps + 1) + '.pth')\n",
        "        torch.save(p1.state_dict(), actor_path)\n",
        "        torch.save(q.state_dict(), critic_path)"
      ],
      "metadata": {
        "id": "9uvJ-PJZ7f_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "outputId": "84aba054-0a81-4491-b832-44f6b9e251b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_eps 0\n",
            "sample took: 8.084068059921265\n",
            "residual_tolerance tensor(1.3642, device='cuda:0')\n",
            "conjugate number of step, 0 rdotr tensor(136.4158, device='cuda:0')\n",
            "conjugate number of step, 1 rdotr tensor(21.1333, device='cuda:0')\n",
            "conjugate number of step, 2 rdotr tensor(9.1061, device='cuda:0')\n",
            "conjugate number of step, 3 rdotr tensor(4.8740, device='cuda:0')\n",
            "conjugate number of step, 4 rdotr tensor(4.6504, device='cuda:0')\n",
            "conjugate number of step, 5 rdotr tensor(1.9773, device='cuda:0')\n",
            "conjugate number of step, 6 rdotr tensor(1.4378, device='cuda:0')\n",
            "metamatrix_conjugate_gradient time 7.011493921279907\n",
            "MultiCoPG step took: 9.263248443603516\n",
            "t_eps 1\n",
            "sample took: 8.781927108764648\n",
            "residual_tolerance tensor(0.4991, device='cuda:0')\n",
            "conjugate number of step, 0 rdotr tensor(112.0556, device='cuda:0')\n",
            "conjugate number of step, 1 rdotr tensor(41.1547, device='cuda:0')\n",
            "conjugate number of step, 2 rdotr tensor(14.7880, device='cuda:0')\n",
            "conjugate number of step, 3 rdotr tensor(9.0093, device='cuda:0')\n",
            "conjugate number of step, 4 rdotr tensor(5.9921, device='cuda:0')\n",
            "conjugate number of step, 5 rdotr tensor(5.8407, device='cuda:0')\n",
            "conjugate number of step, 6 rdotr tensor(2.3601, device='cuda:0')\n",
            "conjugate number of step, 7 rdotr tensor(2.3629, device='cuda:0')\n",
            "conjugate number of step, 8 rdotr tensor(0.8779, device='cuda:0')\n",
            "conjugate number of step, 9 rdotr tensor(1.0288, device='cuda:0')\n",
            "conjugate number of step, 10 rdotr tensor(0.5241, device='cuda:0')\n",
            "conjugate number of step, 11 rdotr tensor(0.6803, device='cuda:0')\n",
            "conjugate number of step, 12 rdotr tensor(0.8008, device='cuda:0')\n",
            "metamatrix_conjugate_gradient time 18.703513622283936\n",
            "MultiCoPG step took: 21.058536767959595\n",
            "t_eps 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a70926076547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Sample and compute update.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't_eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-2a08721be07f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mmat_done_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpassive_env_reset_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/marlenv/marlenv/envs/snake_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m                               wall_value=Cell.WALL.value)\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Generate and add snake to the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnakes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_snakes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msnake\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnakes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/marlenv/marlenv/envs/snake_env.py\u001b[0m in \u001b[0;36m_generate_snakes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_snakes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# Depth-first-search through the grid for possible snake positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs_sweep_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnake_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# Randomly select init snake poses untill no overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/marlenv/marlenv/core/grid_util.py\u001b[0m in \u001b[0;36mdfs_sweep_empty\u001b[0;34m(grid, k)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mempty_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dfs_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/marlenv/marlenv/core/grid_util.py\u001b[0m in \u001b[0;36m_dfs_helper\u001b[0;34m(grid, node, history, k)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_head_blocked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     answers.extend(\n\u001b[0;32m---> 97\u001b[0;31m                         \u001b[0m_dfs_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                     )\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/marlenv/marlenv/core/grid_util.py\u001b[0m in \u001b[0;36m_dfs_helper\u001b[0;34m(grid, node, history, k)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_head_blocked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     answers.extend(\n\u001b[0;32m---> 97\u001b[0;31m                         \u001b[0m_dfs_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                     )\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(actor_path) \n",
        "files.download(critic_path) "
      ],
      "metadata": {
        "id": "RbXvthRG0zCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p2 = policy().to(device).type(dtype)\n",
        "p2.load_state_dict(torch.load(actor_path))"
      ],
      "metadata": {
        "id": "AtmXnOBV1kfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiSimGD Algorithm"
      ],
      "metadata": {
        "id": "7nqvR7tT5wrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings (CHECK THESE BEFORE RUNNING).\n",
        "device = torch.device('cuda:0')\n",
        "# device = torch.device('cpu') # Uncomment to use CPU.\n",
        "batch_size = 16\n",
        "#policy training iteration\n",
        "n_steps = 2000\n",
        "last_teps = None\n",
        "verbose = False\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "A2T7BN8doYXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a policy and critic; we will use self play and a symmetric critic for this game.\n",
        "p1 = policy().to(device).type(dtype)\n",
        "q = critic().to(device).type(dtype)\n",
        "## copy the policy and critic 4 times\n",
        "policies = [p1 for _ in range(4)]\n",
        "critics = [q for _ in range(4)]"
      ],
      "metadata": {
        "id": "dj7jVRt-o3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training environment with env provided.\n",
        "train_wrap = MultiSimGD(\n",
        "    env,\n",
        "    policies,\n",
        "    critics,\n",
        "    batch_size=batch_size,\n",
        "    self_play=False,\n",
        "    critic_lr=1e-3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print('device:', device)\n",
        "print('batch_size:', batch_size)\n",
        "print('n_steps:', n_steps)"
      ],
      "metadata": {
        "id": "VLdtH_JXplP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if last_teps is None:\n",
        "    last_teps = 0\n",
        "\n",
        "writer_tps = []\n",
        "writer_p1 = []\n",
        "writer_p2 = []\n",
        "writer_p3 = []\n",
        "writer_p4 = []\n",
        "writer_eplen = []\n",
        "for t_eps in range(last_teps, n_steps):\n",
        "    # Sample and compute update.\n",
        "    print('t_eps', t_eps)\n",
        "    states, actions, action_mask, rewards, done, unflatten_state = train_wrap.sample(verbose=verbose)\n",
        "    train_wrap.step(states, actions, action_mask, rewards, done, unflatten_state, verbose=verbose)\n",
        "\n",
        "    if ((t_eps + 1) % 20) == 0:\n",
        "        print(\"logging progress:\", t_eps + 1)\n",
        "\n",
        "        # Calculating discounted average reward for each agent.\n",
        "        disc_avg_reward = []\n",
        "        # pdb.set_trace()\n",
        "        for i in range(4):\n",
        "            total_sum = 0.\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                cumsum = 0.\n",
        "                for t in range(rewards.shape[2]):\n",
        "                    cumsum *= 0.99\n",
        "                    cumsum += rewards[i, j, t].cpu().item()\n",
        "                    if (done[i, j, t] == 0):\n",
        "                        total_sum += cumsum\n",
        "                        break\n",
        "\n",
        "            disc_avg_reward.append(total_sum/batch_size)\n",
        "\n",
        "        print('discounted sum of reward', disc_avg_reward, 'steps', t_eps)\n",
        "        # Log values to Tensorboard.\n",
        "\n",
        "        writer_tps.append(t_eps)\n",
        "        writer_p1.append(disc_avg_reward[0])\n",
        "        writer_p2.append(disc_avg_reward[1])\n",
        "        writer_p3.append(disc_avg_reward[2])\n",
        "        writer_p4.append(disc_avg_reward[3])\n",
        "        writer_eplen.append(rewards.shape[2])\n",
        "\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FbYdBUQ3o1Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}